{
    "docs": [
        {
            "location": "/", 
            "text": "SciPipe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject links: \nGitHub repo\n | \nIssue Tracker\n | \nMailing List\n | \nChat\n\n\nProject updates\n\n\n\n\nNEW blog post:\n \nProvenance reports in Scientific Workflows\n - going into details about how SciPipe is addressing provenance\n\n\nNEW blog post:\n \nFirst production workflow run with SciPipe\n\n\nNEW video:\n \nWatch a screencast on how to write a Hello World workflow in SciPipe [15:28]\n\n\n\n\nIntroduction\n\n\n\nSciPipe is a library for writing \nScientific\nWorkflows\n, sometimes\nalso called \"pipelines\", in the \nGo programming language\n.\n\n\nWhen you need to run many commandline programs that depend on each other in\ncomplex ways, SciPipe helps by making the process of running these programs\nflexible, robust and reproducible. SciPipe also lets you restart an interrupted\nrun without over-writing already produced output and produces an audit report\nof what was run, among many other things.\n\n\nSciPipe is built on the proven principles of \nFlow-Based Programming\n\n(FBP) to achieve maximum flexibility, productivity and agility when designing\nworkflows.  Compared to plain dataflow, FBP provides the benefits that\nprocesses are fully self-contained, so that a library of re-usable components\ncan be created, and plugged into new workflows ad-hoc.\n\n\nSimilar to other FBP systems, SciPipe workflows can be likened to a network of\nassembly lines in a factory, where items (files) are flowing through a network\nof conveyor belts, stopping at different independently running stations\n(processes) for processing, as depicted in the picture above.\n\n\nSciPipe was initially created for problems in bioinformatics and\ncheminformatics, but works equally well for any problem involving pipelines of\ncommandline applications.\n\n\nProject status:\n SciPipe is still alpha software and minor breaking API\nchanges still happens as we try to streamline the process of writing workflows.\nPlease follow the commit history closely for any API updates if you have code\nalready written in SciPipe (Let us know if you need any help in migrating code\nto the latest API).\n\n\nBenefits\n\n\nSome key benefits of SciPipe, that are not always found in similar systems:\n\n\n\n\nIntuitive behaviour:\n SciPipe operates by flowing data (files) through a\n  network of channels and processes, not unlike the conveyor belts and stations\n  in a factory.\n\n\nFlexible:\n Processes that wrap command-line programs or scripts, can be\n  combined with processes coded directly in Golang.\n\n\nCustom file naming:\n SciPipe gives you full control over how files are\n  named, making it easy to find your way among the output files of your\n  workflow.\n\n\nPortable:\n Workflows can be distributed either as Go code to be run with\n  \ngo run\n, or as stand-alone executable files that run on almost any UNIX-like\n  operating system.\n\n\nEasy to debug:\n As everything in SciPipe is just Go code, you can use some\n  of the available debugging tools, or just \nprintln()\n statements, to debug\n  your workflow.\n\n\nSupports streaming:\n Can stream outputs via UNIX FIFO files, to avoid temporary storage.\n\n\nEfficient and Parallel:\n Workflows are compiled into statically compiled\n  code that runs fast. SciPipe also leverages pipeline parallelism between\n  processes as well as task parallelism when there are multiple inputs to a\n  process, making efficient use of multiple CPU cores.\n\n\n\n\nKnown limitations\n\n\n\n\nThere are still a number of missing good-to-have features for workflow\n  design. See the \nissue tracker\n\n  for details.\n\n\nThere is not (yet) support for the \nCommon Workflow Language\n.\n\n\n\n\nHello World example\n\n\nLet's look at an example workflow to get a feel for what writing workflows in\nSciPipe looks like:\n\n\npackage\n \nmain\n\n\n\nimport\n \n(\n\n    \n// Import SciPipe, aliased to sp\n\n    \nsp\n \ngithub.com/scipipe/scipipe\n\n\n)\n\n\n\nfunc\n \nmain\n()\n \n{\n\n    \n// Init workflow and max concurrent tasks\n\n    \nwf\n \n:=\n \nsp\n.\nNewWorkflow\n(\nhello_world\n,\n \n4\n)\n\n\n    \n// Initialize processes, and file extensions\n\n    \nhello\n \n:=\n \nwf\n.\nNewProc\n(\nhello\n,\n \necho \nHello \n \n {o:out|.txt}\n)\n\n    \nworld\n \n:=\n \nwf\n.\nNewProc\n(\nworld\n,\n \necho $(cat {i:in}) World \n {o:out|.txt}\n)\n\n\n    \n// Define data flow\n\n    \nworld\n.\nIn\n(\nin\n).\nFrom\n(\nhello\n.\nOut\n(\nout\n))\n\n\n    \n// Run workflow\n\n    \nwf\n.\nRun\n()\n\n\n}\n\n\n\n\n\n\nRunning the example\n\n\nLet's put the code in a file named \nscipipe_helloworld.go\n and run it:\n\n\n$ go run hello_world.go\nAUDIT   \n2018\n/06/15 \n19\n:04:22 \n|\n workflow:hello_world             \n|\n Starting workflow \n(\nWriting log to log/scipipe-20180615-190422-hello_world.log\n)\n\nAUDIT   \n2018\n/06/15 \n19\n:04:22 \n|\n hello                            \n|\n Executing: \necho\n \nHello \n \n hello.out.txt.tmp/hello.out.txt\nAUDIT   \n2018\n/06/15 \n19\n:04:22 \n|\n hello                            \n|\n Finished:  \necho\n \nHello \n \n hello.out.txt.tmp/hello.out.txt\nAUDIT   \n2018\n/06/15 \n19\n:04:22 \n|\n world                            \n|\n Executing: \necho\n \n$(\ncat hello.out.txt\n)\n World \n hello.out.txt.world.out.txt.tmp/hello.out.txt.world.out.txt\nAUDIT   \n2018\n/06/15 \n19\n:04:22 \n|\n world                            \n|\n Finished:  \necho\n \n$(\ncat hello.out.txt\n)\n World \n hello.out.txt.world.out.txt.tmp/hello.out.txt.world.out.txt\nAUDIT   \n2018\n/06/15 \n19\n:04:22 \n|\n workflow:hello_world             \n|\n Finished workflow \n(\nLog written to log/scipipe-20180615-190422-hello_world.log\n)\n\n\n\n\n\n\nLet's check what file SciPipe has generated:\n\n\n$ ls -1 hello*\nhello.out.txt\nhello.out.txt.audit.json\nhello.out.txt.world.out.txt\nhello.out.txt.world.out.txt.audit.json\n\n\n\n\n\nAs you can see, it has created a file \nhello.out.txt\n, and \nhello.out.world.out.txt\n, and\nan accompanying \n.audit.json\n for each of these files.\n\n\nNow, let's check the output of the final resulting file:\n\n\n$ cat hello.out.txt.world.out.txt\nHello World\n\n\n\n\n\nNow we can rejoice that it contains the text \"Hello World\", exactly as a proper\nHello World example should :)\n\n\nNow, these were a little long and cumbersome filename, weren't they? SciPipe\ngives you very good control over how to name your files, if you don't want to\nrely on the automatic file naming. For example, we could set the first filename\nstatically, and then use the first name as a basis for the file name for the\nsecond process, like so:\n\n\npackage\n \nmain\n\n\n\nimport\n \n(\n\n    \n// Import the SciPipe package, aliased to \nsp\n\n    \nsp\n \ngithub.com/scipipe/scipipe\n\n\n)\n\n\n\nfunc\n \nmain\n()\n \n{\n\n    \n// Init workflow with a name, and max concurrent tasks\n\n    \nwf\n \n:=\n \nsp\n.\nNewWorkflow\n(\nhello_world\n,\n \n4\n)\n\n\n    \n// Initialize processes and set output file paths\n\n    \nhello\n \n:=\n \nwf\n.\nNewProc\n(\nhello\n,\n \necho \nHello \n \n {o:out}\n)\n\n    \nhello\n.\nSetOut\n(\nout\n,\n \nhello.txt\n)\n\n\n    \nworld\n \n:=\n \nwf\n.\nNewProc\n(\nworld\n,\n \necho $(cat {i:in}) World \n {o:out}\n)\n\n    \nworld\n.\nSetOut\n(\nout\n,\n \n{i:in|%.txt}_world.txt\n)\n\n\n    \n// Connect network\n\n    \nworld\n.\nIn\n(\nin\n).\nFrom\n(\nhello\n.\nOut\n(\nout\n))\n\n\n    \n// Run workflow\n\n    \nwf\n.\nRun\n()\n\n\n}\n\n\n\n\n\n\nIn the \n{i:in...\n part, we are re-using the file path from the file received on\nthe in-port named 'in', and then running a Bash-style trim-from-end command on\nit to remove the \n.txt\n extension.\n\n\nNow, if we run this, the file names get a little cleaner:\n\n\n$ ls -1 hello*\nhello.txt\nhello.txt.audit.json\nhello_world.go\nhello_world.txt\nhello_world.txt.audit.json\n\n\n\n\n\nThe audit logs\n\n\nFinally, we could have a look at one of those audit file created:\n\n\n$ cat hello_world.txt.audit.json\n\n{\n\n    \nID\n: \n99i5vxhtd41pmaewc8pr\n,\n    \nProcessName\n: \nworld\n,\n    \nCommand\n: \necho \n$(\ncat hello.txt\n)\n World \\u003e\\u003e hello_world.txt.tmp/hello_world.txt\n,\n    \nParams\n: \n{}\n,\n    \nTags\n: \n{}\n,\n    \nStartTime\n: \n2018-06-15T19:10:37.955602979+02:00\n,\n    \nFinishTime\n: \n2018-06-15T19:10:37.959410102+02:00\n,\n    \nExecTimeNS\n: \n3000000\n,\n    \nUpstream\n: \n{\n\n        \nhello.txt\n: \n{\n\n            \nID\n: \nw4oeiii9h5j7sckq7aqq\n,\n            \nProcessName\n: \nhello\n,\n            \nCommand\n: \necho \nHello \n \\u003e hello.txt.tmp/hello.txt\n,\n            \nParams\n: \n{}\n,\n            \nTags\n: \n{}\n,\n            \nStartTime\n: \n2018-06-15T19:10:37.950032676+02:00\n,\n            \nFinishTime\n: \n2018-06-15T19:10:37.95468214+02:00\n,\n            \nExecTimeNS\n: \n4000000\n,\n            \nUpstream\n: \n{}\n\n        \n}\n\n    \n}\n\n\n\n\n\n\nEach such audit-file contains a hierarchic JSON-representation of the full\nworkflow path that was executed in order to produce this file. On the first\nlevel is the command that directly produced the corresponding file, and then,\nindexed by their filenames, under \"Upstream\", there is a similar chunk\ndescribing how all of its input files were generated. This process will be\nrepeated in a recursive way for large workflows, so that, for each file\ngenerated by the workflow, there is always a full, hierarchic, history of all\nthe commands run - with their associated metadata - to produce that file.\n\n\nYou can find many more examples in the \nexamples folder\n in the GitHub repo.\n\n\nFor more information about how to write workflows using SciPipe, use the menu\nto the left, to browse the various topics!", 
            "title": "Introduction"
        }, 
        {
            "location": "/#scipipe", 
            "text": "Project links:  GitHub repo  |  Issue Tracker  |  Mailing List  |  Chat", 
            "title": "SciPipe"
        }, 
        {
            "location": "/#project-updates", 
            "text": "NEW blog post:   Provenance reports in Scientific Workflows  - going into details about how SciPipe is addressing provenance  NEW blog post:   First production workflow run with SciPipe  NEW video:   Watch a screencast on how to write a Hello World workflow in SciPipe [15:28]", 
            "title": "Project updates"
        }, 
        {
            "location": "/#introduction", 
            "text": "SciPipe is a library for writing  Scientific\nWorkflows , sometimes\nalso called \"pipelines\", in the  Go programming language .  When you need to run many commandline programs that depend on each other in\ncomplex ways, SciPipe helps by making the process of running these programs\nflexible, robust and reproducible. SciPipe also lets you restart an interrupted\nrun without over-writing already produced output and produces an audit report\nof what was run, among many other things.  SciPipe is built on the proven principles of  Flow-Based Programming \n(FBP) to achieve maximum flexibility, productivity and agility when designing\nworkflows.  Compared to plain dataflow, FBP provides the benefits that\nprocesses are fully self-contained, so that a library of re-usable components\ncan be created, and plugged into new workflows ad-hoc.  Similar to other FBP systems, SciPipe workflows can be likened to a network of\nassembly lines in a factory, where items (files) are flowing through a network\nof conveyor belts, stopping at different independently running stations\n(processes) for processing, as depicted in the picture above.  SciPipe was initially created for problems in bioinformatics and\ncheminformatics, but works equally well for any problem involving pipelines of\ncommandline applications.  Project status:  SciPipe is still alpha software and minor breaking API\nchanges still happens as we try to streamline the process of writing workflows.\nPlease follow the commit history closely for any API updates if you have code\nalready written in SciPipe (Let us know if you need any help in migrating code\nto the latest API).", 
            "title": "Introduction"
        }, 
        {
            "location": "/#benefits", 
            "text": "Some key benefits of SciPipe, that are not always found in similar systems:   Intuitive behaviour:  SciPipe operates by flowing data (files) through a\n  network of channels and processes, not unlike the conveyor belts and stations\n  in a factory.  Flexible:  Processes that wrap command-line programs or scripts, can be\n  combined with processes coded directly in Golang.  Custom file naming:  SciPipe gives you full control over how files are\n  named, making it easy to find your way among the output files of your\n  workflow.  Portable:  Workflows can be distributed either as Go code to be run with\n   go run , or as stand-alone executable files that run on almost any UNIX-like\n  operating system.  Easy to debug:  As everything in SciPipe is just Go code, you can use some\n  of the available debugging tools, or just  println()  statements, to debug\n  your workflow.  Supports streaming:  Can stream outputs via UNIX FIFO files, to avoid temporary storage.  Efficient and Parallel:  Workflows are compiled into statically compiled\n  code that runs fast. SciPipe also leverages pipeline parallelism between\n  processes as well as task parallelism when there are multiple inputs to a\n  process, making efficient use of multiple CPU cores.", 
            "title": "Benefits"
        }, 
        {
            "location": "/#known-limitations", 
            "text": "There are still a number of missing good-to-have features for workflow\n  design. See the  issue tracker \n  for details.  There is not (yet) support for the  Common Workflow Language .", 
            "title": "Known limitations"
        }, 
        {
            "location": "/#hello-world-example", 
            "text": "Let's look at an example workflow to get a feel for what writing workflows in\nSciPipe looks like:  package   main  import   ( \n     // Import SciPipe, aliased to sp \n     sp   github.com/scipipe/scipipe  )  func   main ()   { \n     // Init workflow and max concurrent tasks \n     wf   :=   sp . NewWorkflow ( hello_world ,   4 ) \n\n     // Initialize processes, and file extensions \n     hello   :=   wf . NewProc ( hello ,   echo  Hello     {o:out|.txt} ) \n     world   :=   wf . NewProc ( world ,   echo $(cat {i:in}) World   {o:out|.txt} ) \n\n     // Define data flow \n     world . In ( in ). From ( hello . Out ( out )) \n\n     // Run workflow \n     wf . Run ()  }", 
            "title": "Hello World example"
        }, 
        {
            "location": "/#running-the-example", 
            "text": "Let's put the code in a file named  scipipe_helloworld.go  and run it:  $ go run hello_world.go\nAUDIT    2018 /06/15  19 :04:22  |  workflow:hello_world              |  Starting workflow  ( Writing log to log/scipipe-20180615-190422-hello_world.log ) \nAUDIT    2018 /06/15  19 :04:22  |  hello                             |  Executing:  echo   Hello     hello.out.txt.tmp/hello.out.txt\nAUDIT    2018 /06/15  19 :04:22  |  hello                             |  Finished:   echo   Hello     hello.out.txt.tmp/hello.out.txt\nAUDIT    2018 /06/15  19 :04:22  |  world                             |  Executing:  echo   $( cat hello.out.txt )  World   hello.out.txt.world.out.txt.tmp/hello.out.txt.world.out.txt\nAUDIT    2018 /06/15  19 :04:22  |  world                             |  Finished:   echo   $( cat hello.out.txt )  World   hello.out.txt.world.out.txt.tmp/hello.out.txt.world.out.txt\nAUDIT    2018 /06/15  19 :04:22  |  workflow:hello_world              |  Finished workflow  ( Log written to log/scipipe-20180615-190422-hello_world.log )   Let's check what file SciPipe has generated:  $ ls -1 hello*\nhello.out.txt\nhello.out.txt.audit.json\nhello.out.txt.world.out.txt\nhello.out.txt.world.out.txt.audit.json  As you can see, it has created a file  hello.out.txt , and  hello.out.world.out.txt , and\nan accompanying  .audit.json  for each of these files.  Now, let's check the output of the final resulting file:  $ cat hello.out.txt.world.out.txt\nHello World  Now we can rejoice that it contains the text \"Hello World\", exactly as a proper\nHello World example should :)  Now, these were a little long and cumbersome filename, weren't they? SciPipe\ngives you very good control over how to name your files, if you don't want to\nrely on the automatic file naming. For example, we could set the first filename\nstatically, and then use the first name as a basis for the file name for the\nsecond process, like so:  package   main  import   ( \n     // Import the SciPipe package, aliased to  sp \n     sp   github.com/scipipe/scipipe  )  func   main ()   { \n     // Init workflow with a name, and max concurrent tasks \n     wf   :=   sp . NewWorkflow ( hello_world ,   4 ) \n\n     // Initialize processes and set output file paths \n     hello   :=   wf . NewProc ( hello ,   echo  Hello     {o:out} ) \n     hello . SetOut ( out ,   hello.txt ) \n\n     world   :=   wf . NewProc ( world ,   echo $(cat {i:in}) World   {o:out} ) \n     world . SetOut ( out ,   {i:in|%.txt}_world.txt ) \n\n     // Connect network \n     world . In ( in ). From ( hello . Out ( out )) \n\n     // Run workflow \n     wf . Run ()  }   In the  {i:in...  part, we are re-using the file path from the file received on\nthe in-port named 'in', and then running a Bash-style trim-from-end command on\nit to remove the  .txt  extension.  Now, if we run this, the file names get a little cleaner:  $ ls -1 hello*\nhello.txt\nhello.txt.audit.json\nhello_world.go\nhello_world.txt\nhello_world.txt.audit.json", 
            "title": "Running the example"
        }, 
        {
            "location": "/#the-audit-logs", 
            "text": "Finally, we could have a look at one of those audit file created:  $ cat hello_world.txt.audit.json { \n     ID :  99i5vxhtd41pmaewc8pr ,\n     ProcessName :  world ,\n     Command :  echo  $( cat hello.txt )  World \\u003e\\u003e hello_world.txt.tmp/hello_world.txt ,\n     Params :  {} ,\n     Tags :  {} ,\n     StartTime :  2018-06-15T19:10:37.955602979+02:00 ,\n     FinishTime :  2018-06-15T19:10:37.959410102+02:00 ,\n     ExecTimeNS :  3000000 ,\n     Upstream :  { \n         hello.txt :  { \n             ID :  w4oeiii9h5j7sckq7aqq ,\n             ProcessName :  hello ,\n             Command :  echo  Hello   \\u003e hello.txt.tmp/hello.txt ,\n             Params :  {} ,\n             Tags :  {} ,\n             StartTime :  2018-06-15T19:10:37.950032676+02:00 ,\n             FinishTime :  2018-06-15T19:10:37.95468214+02:00 ,\n             ExecTimeNS :  4000000 ,\n             Upstream :  {} \n         } \n     }   Each such audit-file contains a hierarchic JSON-representation of the full\nworkflow path that was executed in order to produce this file. On the first\nlevel is the command that directly produced the corresponding file, and then,\nindexed by their filenames, under \"Upstream\", there is a similar chunk\ndescribing how all of its input files were generated. This process will be\nrepeated in a recursive way for large workflows, so that, for each file\ngenerated by the workflow, there is always a full, hierarchic, history of all\nthe commands run - with their associated metadata - to produce that file.  You can find many more examples in the  examples folder  in the GitHub repo.  For more information about how to write workflows using SciPipe, use the menu\nto the left, to browse the various topics!", 
            "title": "The audit logs"
        }, 
        {
            "location": "/install/", 
            "text": "Installing SciPipe\n\n\nInstalling SciPipe means first installing the Go programming langauge, and then\nusing Go's \ngo get\n command to install the SciPipe library. After this, you will\nbe able to use Go's \ngo run\n command to run SciPipe workflows.\n\n\nInstall Go\n\n\nInstall Go by following the instructions \non this page\n,\nfor your operating system.\n\n\nInstall SciPipe\n\n\nThere are two main ways of installing SciPipe, one which is maximally easy, and one which\nis recommended if you want to make sure that your workflow will never break because of\nAPI changes in SciPipe, and that you always have a copy of the SciPipe source code available.\n\n\nEasiest: Using go get\n\n\nThe easiest way to intsall SciPipe is by using the \ngo get\n tool in the Go\ntool chain. To install scipipe with \ngo get\n, run the following command in\nyour terminal:\n\n\ngo get github.com/scipipe/scipipe/...\n\n\n\n\n\nN.B:\n Don't miss the \n...\n, as otherwise the \nscipipe\n helper tool will not be installed.\n\n\nFor maximum future proofing: Use a copy of SciPipe's source code in your own code\n\n\nIn order to make sure that your workflow will never break because of API\nchanges in SciPipe, and that you always have a copy of the SciPipe source\ncode available, we recommend to always include a copy of the SciPipe source\ncode in your workflow's source code repository. The SciPipe source code is\nonly around 1500 lines of code, with no external dependencies except Go and\nBash, so this should not increase the size of your repository too much.\n\n\nA simple way to do this, is to clone a copy of the SciPipe source code into a\nfolder structure that looks like this, under your main workflow code folder\n(where you store your own \n.go\n files):\n\n\nvendor/src/github.com/scipipe/scipipe\n\n\n\n\n\nTo create and clone the scipipe repo to this folder, you can use these\ncommands:\n\n\nmkdir -p vendor/src/github.com/scipipe\n\ncd\n vendor/src/github.com/scipipe\ngit clone https://github.com/scipipe/scipipe.git\n\n\n\n\n\nInitialize a new workflow file\n\n\nNow, you should be able to write code like in the example below, in files\nending with \n.go\n.\n\n\nThe easiest way to get started is to let the scipipe tool generate a starting point for you:\n\n\nscipipe new myfirstworkflow.go\n\n\n\n\n\n... which you can then edit to your liking.\n\n\nRun your workflow\n\n\nTo run a \n.go\n file, use \ngo run\n:\n\n\ngo run myfirstworkflow.go\n\n\n\n\n\nSome tips about editors\n\n\nIn order to be productive with SciPipe, you will also need a Go editor or IDE\nwith support for auto-completion, sometimes also called \"intellisense\".\n\n\nWe can warmly recommend to use one of these editors, sorted by level of endorsement:\n\n\n\n\nVisual Studio Code\n with the \nGo plugin\n - If you want a very powerful almost IDE-like editor\n\n\nThe \nvim-go\n plugin by \nFatih\n - if you are a Vim power-user, or need a terminal-only complement to VSCode.\n\n\nJetBrain's \nGoLand IDE\n, if you are ready to pay for maximum code intelligence in a professional IDE.\n\n\nLiteIDE\n - if you want a simple, robust and fast standalone Go-editor.\n\n\n\n\nThere are also popular Go-plugins for \nSublime text\n,\n\nAtom\n and \nIntelliJ IDEA\n,\nand an upcoming Go IDE from JetBrains, called", 
            "title": "Installing"
        }, 
        {
            "location": "/install/#installing-scipipe", 
            "text": "Installing SciPipe means first installing the Go programming langauge, and then\nusing Go's  go get  command to install the SciPipe library. After this, you will\nbe able to use Go's  go run  command to run SciPipe workflows.", 
            "title": "Installing SciPipe"
        }, 
        {
            "location": "/install/#install-go", 
            "text": "Install Go by following the instructions  on this page ,\nfor your operating system.", 
            "title": "Install Go"
        }, 
        {
            "location": "/install/#install-scipipe", 
            "text": "There are two main ways of installing SciPipe, one which is maximally easy, and one which\nis recommended if you want to make sure that your workflow will never break because of\nAPI changes in SciPipe, and that you always have a copy of the SciPipe source code available.", 
            "title": "Install SciPipe"
        }, 
        {
            "location": "/install/#easiest-using-go-get", 
            "text": "The easiest way to intsall SciPipe is by using the  go get  tool in the Go\ntool chain. To install scipipe with  go get , run the following command in\nyour terminal:  go get github.com/scipipe/scipipe/...  N.B:  Don't miss the  ... , as otherwise the  scipipe  helper tool will not be installed.", 
            "title": "Easiest: Using go get"
        }, 
        {
            "location": "/install/#for-maximum-future-proofing-use-a-copy-of-scipipes-source-code-in-your-own-code", 
            "text": "In order to make sure that your workflow will never break because of API\nchanges in SciPipe, and that you always have a copy of the SciPipe source\ncode available, we recommend to always include a copy of the SciPipe source\ncode in your workflow's source code repository. The SciPipe source code is\nonly around 1500 lines of code, with no external dependencies except Go and\nBash, so this should not increase the size of your repository too much.  A simple way to do this, is to clone a copy of the SciPipe source code into a\nfolder structure that looks like this, under your main workflow code folder\n(where you store your own  .go  files):  vendor/src/github.com/scipipe/scipipe  To create and clone the scipipe repo to this folder, you can use these\ncommands:  mkdir -p vendor/src/github.com/scipipe cd  vendor/src/github.com/scipipe\ngit clone https://github.com/scipipe/scipipe.git", 
            "title": "For maximum future proofing: Use a copy of SciPipe's source code in your own code"
        }, 
        {
            "location": "/install/#initialize-a-new-workflow-file", 
            "text": "Now, you should be able to write code like in the example below, in files\nending with  .go .  The easiest way to get started is to let the scipipe tool generate a starting point for you:  scipipe new myfirstworkflow.go  ... which you can then edit to your liking.", 
            "title": "Initialize a new workflow file"
        }, 
        {
            "location": "/install/#run-your-workflow", 
            "text": "To run a  .go  file, use  go run :  go run myfirstworkflow.go", 
            "title": "Run your workflow"
        }, 
        {
            "location": "/install/#some-tips-about-editors", 
            "text": "In order to be productive with SciPipe, you will also need a Go editor or IDE\nwith support for auto-completion, sometimes also called \"intellisense\".  We can warmly recommend to use one of these editors, sorted by level of endorsement:   Visual Studio Code  with the  Go plugin  - If you want a very powerful almost IDE-like editor  The  vim-go  plugin by  Fatih  - if you are a Vim power-user, or need a terminal-only complement to VSCode.  JetBrain's  GoLand IDE , if you are ready to pay for maximum code intelligence in a professional IDE.  LiteIDE  - if you want a simple, robust and fast standalone Go-editor.   There are also popular Go-plugins for  Sublime text , Atom  and  IntelliJ IDEA ,\nand an upcoming Go IDE from JetBrains, called", 
            "title": "Some tips about editors"
        }, 
        {
            "location": "/basic_concepts/", 
            "text": "Basic concepts\n\n\nIn SciPipe, we are discussing a few concepts all the time, so to make sure we\nare on the same page, we will below go through the basic ones briefly.\n\n\nProcesses\n\n\nThe probably most basic concept in SciPipe is the process.  A process is an\nasynchronously running component that is typically defined as a static,\n\"long-running\" part of the workflow, and the number of processes thus is\ntypically fixed for a workflow during its execution.\n\n\nOne can create customized types of processes, but for most basic workflows, the\n\nscipipe.Process\n\nwill be used, which is specialized for executing commandline applications. New\n\nProcess\n-es are typically created using the \nscipipe.NewProc(procName,\nshellPattern)\n command.\n\n\n\n\nSee \nGoDoc for Process\n\n\n\n\nTasks\n\n\nThe \"long-running\" processes mentioned above, will receive input files on its\nin-ports, and for each complete set of input files it receives, it will create\na new \ntask\n. Specifically, \nscipipe.Process\n will create\n\nscipipe.Task\n objects, and populate it with all data needed for one\nparticular shell command execution.  \nTask\n objects are executed via their\n\nExecute()\n\nmethod, or \nCustomExecute()\n, if custom Go code is supposed to be\nexecuted instead of a shell command.\n\n\nThe distinction between processes and tasks is important to understand, for\nexample when doing more advanced configuration of file naming strategies, since\nthe custom anonymous functions used to format paths are taking a \nTask\n as\ninput, even though these functions are saved on the process object.\n\n\nTo understand the difference between processes and tasks, it is helpful to\nremember that processes are long-running, and typically fixed during the course\nof a workflow, while tasks are transient objects, created temporarily as a\ncontainer for all data and code needed for each execution of a concrete shell\ncommand.\n\n\n\n\nSee \nGoDoc for Task\n\n\n\n\nPorts\n\n\nCentral to the way data dependencies are defined in SciPipe, is ports. Ports\nare fields on processes, which are connected to other ports via channels (see\nseparate section on this page).\n\n\nIn SciPipe, each port must have a unique name within its process (there can't\nbe an in-port and out-port named the same), and this name will be used in shell\ncommand patterns, when connecting dependencies / dataflow networks, and when\nconfiguring file naming strategies.\n\n\nIn \nProcess\n objects, in-ports are are accessed with\n\nmyProcess.In(\"my_port\")\n, and out-ports are similarly accessed with\n\nmyProcess.Out(\"my_other_port\")\n. They are of type\n\nInPort\n and\n\nOutPort\n respectively.\n\n\nSome pre-made components might have ports bound to custom field names though,\nsuch as \nmyFastaReader.InFastaFile\n, or \nmyZipComponent.OutZipFile\n.\n\n\nPort objects have some methods bound to them, most importantly the \nFrom()\n\nmethod (for in-ports. Out-ports have a corresponding \nTo()\n method), which\ntakes another port, and connects to it, by stitching a channel between the\nports.\n\n\nOn \nProcess\n objects, there is also a third port type, \nInParamPort\n (and the\naccompanying \nOutParamPort\n), which is used when it is needed to send a\nstream of parameter values (in string format) to be supplied to as arguments\nto shell commands.\n\n\n\n\nSee \nGoDoc for the InPort struct type\n\n\nSee \nGoDoc for the OutPort struct type\n\n\nSee \nGoDoc for the InParamPort struct type\n\n\nSee \nGoDoc for the OutParamPort struct type\n\n\n\n\nChannels\n\n\nPorts in SciPipe are connected via channels. Channels are \nplain Go channels\n\nand nothing more. Most of the time, one will not need to deal with the channels\ndirectly though, since the port objects (see separate section for ports) have\nall the logic to connect to other ports via channels, but it can be good to\nknow that they are there, in case you need to do something more advanced.\n\n\nWorkflow\n\n\nThe \nWorkflow\n\nis a special object in SciPipe, that just takes care of running a set of\ncomponents making up a workflow.\n\n\nThere is not much to say about the workflow component, other than that it is\ncreated with \nscipipe.NewWorkflow(workflowName, maxConcurrentTasks)\n, that all processes need to be added\nto it with \nwf.AddProc(proc)\n while the \"last\", or \"driving\" process needs to be specified with \nwf.SetDriver(driverProcess)\n, and that it should be run with\n\nwf.Run()\n. But this is already covered in the other examples and\ntutorials.\n\n\n\n\nSee \nGoDoc for Workflow\n\n\n\n\nShell command pattern\n\n\nThe \nProcess\n has the speciality that it can be configured using a special\nshell command pattern, supplied to the \nNewProc()\n\nfactory function. It is already explained in the section \"writing workflows\",\nbut in brief, it is a normal shell command, with placeholders for in-ports,\nout-ports and parameter ports, on the form \n{i:inportname}\n, \n{o:outportname}\n,\nand \n{p:paramportname}\n, respectively.\n\n\n\n\nSee \nGoDoc for NewProc()", 
            "title": "Basic Concepts"
        }, 
        {
            "location": "/basic_concepts/#basic-concepts", 
            "text": "In SciPipe, we are discussing a few concepts all the time, so to make sure we\nare on the same page, we will below go through the basic ones briefly.", 
            "title": "Basic concepts"
        }, 
        {
            "location": "/basic_concepts/#processes", 
            "text": "The probably most basic concept in SciPipe is the process.  A process is an\nasynchronously running component that is typically defined as a static,\n\"long-running\" part of the workflow, and the number of processes thus is\ntypically fixed for a workflow during its execution.  One can create customized types of processes, but for most basic workflows, the scipipe.Process \nwill be used, which is specialized for executing commandline applications. New Process -es are typically created using the  scipipe.NewProc(procName,\nshellPattern)  command.   See  GoDoc for Process", 
            "title": "Processes"
        }, 
        {
            "location": "/basic_concepts/#tasks", 
            "text": "The \"long-running\" processes mentioned above, will receive input files on its\nin-ports, and for each complete set of input files it receives, it will create\na new  task . Specifically,  scipipe.Process  will create scipipe.Task  objects, and populate it with all data needed for one\nparticular shell command execution.   Task  objects are executed via their Execute() \nmethod, or  CustomExecute() , if custom Go code is supposed to be\nexecuted instead of a shell command.  The distinction between processes and tasks is important to understand, for\nexample when doing more advanced configuration of file naming strategies, since\nthe custom anonymous functions used to format paths are taking a  Task  as\ninput, even though these functions are saved on the process object.  To understand the difference between processes and tasks, it is helpful to\nremember that processes are long-running, and typically fixed during the course\nof a workflow, while tasks are transient objects, created temporarily as a\ncontainer for all data and code needed for each execution of a concrete shell\ncommand.   See  GoDoc for Task", 
            "title": "Tasks"
        }, 
        {
            "location": "/basic_concepts/#ports", 
            "text": "Central to the way data dependencies are defined in SciPipe, is ports. Ports\nare fields on processes, which are connected to other ports via channels (see\nseparate section on this page).  In SciPipe, each port must have a unique name within its process (there can't\nbe an in-port and out-port named the same), and this name will be used in shell\ncommand patterns, when connecting dependencies / dataflow networks, and when\nconfiguring file naming strategies.  In  Process  objects, in-ports are are accessed with myProcess.In(\"my_port\") , and out-ports are similarly accessed with myProcess.Out(\"my_other_port\") . They are of type InPort  and OutPort  respectively.  Some pre-made components might have ports bound to custom field names though,\nsuch as  myFastaReader.InFastaFile , or  myZipComponent.OutZipFile .  Port objects have some methods bound to them, most importantly the  From() \nmethod (for in-ports. Out-ports have a corresponding  To()  method), which\ntakes another port, and connects to it, by stitching a channel between the\nports.  On  Process  objects, there is also a third port type,  InParamPort  (and the\naccompanying  OutParamPort ), which is used when it is needed to send a\nstream of parameter values (in string format) to be supplied to as arguments\nto shell commands.   See  GoDoc for the InPort struct type  See  GoDoc for the OutPort struct type  See  GoDoc for the InParamPort struct type  See  GoDoc for the OutParamPort struct type", 
            "title": "Ports"
        }, 
        {
            "location": "/basic_concepts/#channels", 
            "text": "Ports in SciPipe are connected via channels. Channels are  plain Go channels \nand nothing more. Most of the time, one will not need to deal with the channels\ndirectly though, since the port objects (see separate section for ports) have\nall the logic to connect to other ports via channels, but it can be good to\nknow that they are there, in case you need to do something more advanced.", 
            "title": "Channels"
        }, 
        {
            "location": "/basic_concepts/#workflow", 
            "text": "The  Workflow \nis a special object in SciPipe, that just takes care of running a set of\ncomponents making up a workflow.  There is not much to say about the workflow component, other than that it is\ncreated with  scipipe.NewWorkflow(workflowName, maxConcurrentTasks) , that all processes need to be added\nto it with  wf.AddProc(proc)  while the \"last\", or \"driving\" process needs to be specified with  wf.SetDriver(driverProcess) , and that it should be run with wf.Run() . But this is already covered in the other examples and\ntutorials.   See  GoDoc for Workflow", 
            "title": "Workflow"
        }, 
        {
            "location": "/basic_concepts/#shell-command-pattern", 
            "text": "The  Process  has the speciality that it can be configured using a special\nshell command pattern, supplied to the  NewProc() \nfactory function. It is already explained in the section \"writing workflows\",\nbut in brief, it is a normal shell command, with placeholders for in-ports,\nout-ports and parameter ports, on the form  {i:inportname} ,  {o:outportname} ,\nand  {p:paramportname} , respectively.   See  GoDoc for NewProc()", 
            "title": "Shell command pattern"
        }, 
        {
            "location": "/writing_workflows/", 
            "text": "Writing Workflows - An Overview\n\n\nIn order to give an overview of how to write workflows in SciPipe, let's look\nat the example workflow used on the front page again:\n\n\npackage\n \nmain\n\n\n\nimport\n \n(\n\n    \n// Import SciPipe into the main namespace (generally frowned upon but could\n\n    \n// be argued to be reasonable for short-lived workflow scripts like this)\n\n    \n.\n \ngithub.com/scipipe/scipipe\n\n\n)\n\n\n\nfunc\n \nmain\n()\n \n{\n\n    \n// Init workflow with a name, and a number for max concurrent tasks, so we\n\n    \n// don\nt overbook our CPU (it is recommended to set it to the number of CPU\n\n    \n// cores of your computer)\n\n    \nwf\n \n:=\n \nNewWorkflow\n(\nhello_world\n,\n \n4\n)\n\n\n    \n// Initialize processes and set output file paths\n\n    \nhello\n \n:=\n \nwf\n.\nNewProc\n(\nhello\n,\n \necho \nHello \n \n {o:out}\n)\n\n    \nhello\n.\nSetOut\n(\nout\n,\n \nhello.txt\n)\n\n\n    \nworld\n \n:=\n \nwf\n.\nNewProc\n(\nworld\n,\n \necho $(cat {i:in}) World \n {o:out}\n)\n\n    \nworld\n.\nSetOut\n(\nout\n,\n \n{i:in|%.txt}_world.txt\n)\n\n\n    \n// Connect network\n\n    \nworld\n.\nIn\n(\nin\n).\nFrom\n(\nhello\n.\nOut\n(\nout\n))\n\n\n    \n// Run workflow\n\n    \nwf\n.\nRun\n()\n\n\n}\n\n\n\n\n\n\nNow let's go through the code example in some detail, to see what we are\nactually doing.\n\n\nInitializing processes\n\n\n// Initialize processes from shell command patterns\n\n\nhello\n \n:=\n \nsp\n.\nNewProc\n(\nhello\n,\n \necho \nHello \n \n {o:out}\n)\n\n\nworld\n \n:=\n \nsp\n.\nNewProc\n(\nworld\n,\n \necho $(cat {i:in}) World \n {o:out}\n)\n\n\n\n\n\n\nHere we are initializing two new processes, both of them based on a shell\ncommand, using the \nscipipe.NewProc()\n function, which takes a processname, and\na shell command pattern as input.\n\n\nThe shell command pattern\n\n\nThe shell command patterns, in this case \necho 'Hello ' \n {o:out}\n and\n\necho $(cat {i:in}) World \n {o:out}\n, are basically normal bash\nshell commands, with the addition of \"placeholders\" for input and output\nfilenames.\n\n\nInput filename placeholders are on the form \n{i:INPORT-NAME}\n and the output\nfilename placeholders are similarly of the form \n{o:OUTPORT-NAME}\n.  These\nplaceholders will be replaced with actual filenames when the command is\nexecuted later. The reason that it a port-name is used to name them, is that\nfiles will be queued on the channel connecting to the port, and for each set of\nfiles on in-ports, a command will be created and executed whereafter new files\nwill be pulled in on the out-ports, and so on.\n\n\nFormatting output file paths\n\n\nNow we need to provide some way for scipipe to figure out a suitable file name\nfor each of the files propagating through the \"network\" of processes.  This can\nbe done using special convenience methods on the processes, starting with\n\nSetOut...\n. There are a few variants, of which two of them are shown here.\n\n\n// Configure output file path formatters for the processes created above\n\n\nhello\n.\nSetOut\n(\nout\n,\n \nhello.txt\n)\n\n\nworld\n.\nSetOut\n(\nout\n,\n \n{i:in|%.txt}_world.txt\n)\n\n\n\n\n\n\nSetOut\n takes a pattern similar to the shell command pattern, with\nplaceholders, used to define new (shell-based) processes. The available\nplaceholders that can be used are: \n{i:INPORTNAME}\n, \n{p:PARAMNAME}\n and\n\n{t:TAGNAME}\n. An example of a full pattern might be:\n\n{i:foo}.replace_with_{p:replacement}.txt\n, but can also be used for\nsimple, static paths, like in the example above.\n\n\nThe placeholders can also take certain extra commands, separated from the\nplaceholder name by pipe characters, and of which the one used above is\nprobably the most important one: \n%STRING\n. It will remove the specified\nstring from the \nend\n of the path, which is useful when we want to avoid\ngetting too long paths when re-using previous processes' paths. With the\nexample above, our input file named \nhello.txt\n will be converted into\n\nhello_world.txt\n by this path pattern.\n\n\nEven more control over file formatting\n\n\nWe can actually get even more control over how file names are produced than\nthis, by manually supplying each process with an anonymous function that\nreturns file paths given a \nscipipe.Task\n object, which will be produced for\neach command execution.\n\n\nIn order to implement the same path patterns as above, using this method, we\nwould write like this:\n\n\n// Configure output file path formatters for the processes created above\n\n\nhello\n.\nSetOutFunc\n(\nout\n,\n \nfunc\n(\nt\n \n*\nsp\n.\nTask\n)\n \nstring\n \n{\n\n    \nreturn\n \nhello.txt\n\n\n})\n\n\nworld\n.\nSetOutFunc\n(\nout\n,\n \nfunc\n(\nt\n \n*\nsp\n.\nTask\n)\n \nstring\n \n{\n\n    \nreturn\n \nstrings\n.\nReplace\n(\nt\n.\nInPath\n(\nin\n),\n \n.txt\n,\n \n_world.txt\n,\n \n-\n1\n)\n\n\n})\n\n\n\n\n\n\nAs you can see, this is a much more complicated way to format paths, but it can\nbe useful for example when needing to incorporate parameter values into file\nnames.\n\n\nA caveat about using variables in anonymous functions\n\n\nNote that when using anonymous functions, you have to be careful to not re-use\nthe same variable (even with different values) in multiple functions, due to\nthe \nsubtle ways in which closures work in Go\n.\n\n\nFor example, if you create multiple new processes with separate formatting\nfunctions in a loop, that uses a shared variable, like this:\n\n\nfor\n \n_\n,\n \nval\n \n:=\n \nrange\n \n[]\nstring\n{\nfoo\n,\n \nbar\n}\n \n{\n\n    \nproc\n \n:=\n \nscipipe\n.\nNewProc\n(\nval\n \n+\n \n_proc\n,\n \ncat {p:val} \n {o:out}\n)\n\n    \nproc\n.\nSetOutFunc\n(\nout\n,\n \nfunc\n(\nt\n \n*\nsp\n.\nTask\n)\n \nstring\n \n{\n\n        \nreturn\n \nval\n \n+\n \n.txt\n\n    \n})\n\n\n}\n\n\n\n\n\n\n... then, both functions will return \"bar.txt\", since both funcs were pointing to\nthe same variable (\"var\"), which had the value \"bar\" at the end of the loop.\n\n\nTo avoid this situation, you can do one of two things, of which the latter is\ngenerally recommended:\n\n\n\n\n\n\nCreate a new copy of the variable, inside the anonymous function:\n\n\ngo\nfor _, val := range []string{\"foo\", \"bar\"} {\n    proc := scipipe.NewProc(val + \"_proc\", \"cat {p:val} \n {o:out}\")\n    val := val // \n- Here we create a new copy of the variable\n    proc.SetOutFunc(\"out\", func(t *sp.Task) string {\n        return val + \".txt\"\n    })\n}\n\n\n\n\n\n\n... or, better, access the parameter value via the task which the path function receives:\n\n\ngo\nfor _, val := range []string{\"foo\", \"bar\"} {\n    proc := scipipe.NewProc(val + \"_proc\", \"cat {p:val} \n {o:out}\")\n    proc.SetOutFunc(\"out\", func(t *sp.Task) string {\n        return t.Param(\"val\") + \".txt\" // Access param via the task (`t`)\n    })\n}\n\n\n\n\n\n\nConnecting processes into a network\n\n\nFinally we need to define the data dependencies between our processes. We do\nthis by connecting the outports of one process to the inport of another\nprocess, using the \nFrom\n method available on each in-port object (Or the\n\nTo\n method on out-ports). We also need to connect the final out-port of the\npipeline to the workflow, so that the workflow can pull on this port\n(technically pulling on a Go channel), in order to drive the workflow.\n\n\n// Connect network\n\n\nworld\n.\nIn\n(\nin\n).\nFrom\n(\nhelloWriter\n.\nOut\n(\nout\n))\n\n\n\n\n\n\nRunning the pipeline\n\n\nSo, the final part probably explains itself, but the workflow component is a\nrelatively simple one that will start each component in a separate go-routine.\n\n\nFor technical reasons, one final process has to be run in the main go-routine\n(that where the program's \nmain()\n function runs), but generally you don't\nneed to think about this, as the workflow will then use an in-built\n\nsink\n process for this\npurpose. If you for any reason need to customize which process to use as the\n\"driver\" process, instead of the in-built sink. see the \nSetDriver\n section\n\nin the docs.\n\n\nwf\n.\nRun\n()\n\n\n\n\n\n\nSummary\n\n\nSo with this, we have done everything needed to set up a file-based batch workflow system.\n\n\nIn summary, what we did, was to:\n\n\n\n\nInitialize processes\n\n\nFor each out-port, define a file-naming strategy\n\n\nSpecify dependencies by connecting out- and in-ports\n\n\nRun the pipeline\n\n\n\n\nThis actually turns out to be a fixed set of components that always need to be\nincluded when writing workflows, so it might be good to keep them in mind and\nmemorize these steps, if needed.\n\n\nFor more examples, see the \nexamples folder\n\nin the GitHub repository.", 
            "title": "Writing Workflows"
        }, 
        {
            "location": "/writing_workflows/#writing-workflows-an-overview", 
            "text": "In order to give an overview of how to write workflows in SciPipe, let's look\nat the example workflow used on the front page again:  package   main  import   ( \n     // Import SciPipe into the main namespace (generally frowned upon but could \n     // be argued to be reasonable for short-lived workflow scripts like this) \n     .   github.com/scipipe/scipipe  )  func   main ()   { \n     // Init workflow with a name, and a number for max concurrent tasks, so we \n     // don t overbook our CPU (it is recommended to set it to the number of CPU \n     // cores of your computer) \n     wf   :=   NewWorkflow ( hello_world ,   4 ) \n\n     // Initialize processes and set output file paths \n     hello   :=   wf . NewProc ( hello ,   echo  Hello     {o:out} ) \n     hello . SetOut ( out ,   hello.txt ) \n\n     world   :=   wf . NewProc ( world ,   echo $(cat {i:in}) World   {o:out} ) \n     world . SetOut ( out ,   {i:in|%.txt}_world.txt ) \n\n     // Connect network \n     world . In ( in ). From ( hello . Out ( out )) \n\n     // Run workflow \n     wf . Run ()  }   Now let's go through the code example in some detail, to see what we are\nactually doing.", 
            "title": "Writing Workflows - An Overview"
        }, 
        {
            "location": "/writing_workflows/#initializing-processes", 
            "text": "// Initialize processes from shell command patterns  hello   :=   sp . NewProc ( hello ,   echo  Hello     {o:out} )  world   :=   sp . NewProc ( world ,   echo $(cat {i:in}) World   {o:out} )   Here we are initializing two new processes, both of them based on a shell\ncommand, using the  scipipe.NewProc()  function, which takes a processname, and\na shell command pattern as input.", 
            "title": "Initializing processes"
        }, 
        {
            "location": "/writing_workflows/#the-shell-command-pattern", 
            "text": "The shell command patterns, in this case  echo 'Hello '   {o:out}  and echo $(cat {i:in}) World   {o:out} , are basically normal bash\nshell commands, with the addition of \"placeholders\" for input and output\nfilenames.  Input filename placeholders are on the form  {i:INPORT-NAME}  and the output\nfilename placeholders are similarly of the form  {o:OUTPORT-NAME} .  These\nplaceholders will be replaced with actual filenames when the command is\nexecuted later. The reason that it a port-name is used to name them, is that\nfiles will be queued on the channel connecting to the port, and for each set of\nfiles on in-ports, a command will be created and executed whereafter new files\nwill be pulled in on the out-ports, and so on.", 
            "title": "The shell command pattern"
        }, 
        {
            "location": "/writing_workflows/#formatting-output-file-paths", 
            "text": "Now we need to provide some way for scipipe to figure out a suitable file name\nfor each of the files propagating through the \"network\" of processes.  This can\nbe done using special convenience methods on the processes, starting with SetOut... . There are a few variants, of which two of them are shown here.  // Configure output file path formatters for the processes created above  hello . SetOut ( out ,   hello.txt )  world . SetOut ( out ,   {i:in|%.txt}_world.txt )   SetOut  takes a pattern similar to the shell command pattern, with\nplaceholders, used to define new (shell-based) processes. The available\nplaceholders that can be used are:  {i:INPORTNAME} ,  {p:PARAMNAME}  and {t:TAGNAME} . An example of a full pattern might be: {i:foo}.replace_with_{p:replacement}.txt , but can also be used for\nsimple, static paths, like in the example above.  The placeholders can also take certain extra commands, separated from the\nplaceholder name by pipe characters, and of which the one used above is\nprobably the most important one:  %STRING . It will remove the specified\nstring from the  end  of the path, which is useful when we want to avoid\ngetting too long paths when re-using previous processes' paths. With the\nexample above, our input file named  hello.txt  will be converted into hello_world.txt  by this path pattern.", 
            "title": "Formatting output file paths"
        }, 
        {
            "location": "/writing_workflows/#even-more-control-over-file-formatting", 
            "text": "We can actually get even more control over how file names are produced than\nthis, by manually supplying each process with an anonymous function that\nreturns file paths given a  scipipe.Task  object, which will be produced for\neach command execution.  In order to implement the same path patterns as above, using this method, we\nwould write like this:  // Configure output file path formatters for the processes created above  hello . SetOutFunc ( out ,   func ( t   * sp . Task )   string   { \n     return   hello.txt  })  world . SetOutFunc ( out ,   func ( t   * sp . Task )   string   { \n     return   strings . Replace ( t . InPath ( in ),   .txt ,   _world.txt ,   - 1 )  })   As you can see, this is a much more complicated way to format paths, but it can\nbe useful for example when needing to incorporate parameter values into file\nnames.", 
            "title": "Even more control over file formatting"
        }, 
        {
            "location": "/writing_workflows/#a-caveat-about-using-variables-in-anonymous-functions", 
            "text": "Note that when using anonymous functions, you have to be careful to not re-use\nthe same variable (even with different values) in multiple functions, due to\nthe  subtle ways in which closures work in Go .  For example, if you create multiple new processes with separate formatting\nfunctions in a loop, that uses a shared variable, like this:  for   _ ,   val   :=   range   [] string { foo ,   bar }   { \n     proc   :=   scipipe . NewProc ( val   +   _proc ,   cat {p:val}   {o:out} ) \n     proc . SetOutFunc ( out ,   func ( t   * sp . Task )   string   { \n         return   val   +   .txt \n     })  }   ... then, both functions will return \"bar.txt\", since both funcs were pointing to\nthe same variable (\"var\"), which had the value \"bar\" at the end of the loop.  To avoid this situation, you can do one of two things, of which the latter is\ngenerally recommended:    Create a new copy of the variable, inside the anonymous function:  go\nfor _, val := range []string{\"foo\", \"bar\"} {\n    proc := scipipe.NewProc(val + \"_proc\", \"cat {p:val}   {o:out}\")\n    val := val //  - Here we create a new copy of the variable\n    proc.SetOutFunc(\"out\", func(t *sp.Task) string {\n        return val + \".txt\"\n    })\n}    ... or, better, access the parameter value via the task which the path function receives:  go\nfor _, val := range []string{\"foo\", \"bar\"} {\n    proc := scipipe.NewProc(val + \"_proc\", \"cat {p:val}   {o:out}\")\n    proc.SetOutFunc(\"out\", func(t *sp.Task) string {\n        return t.Param(\"val\") + \".txt\" // Access param via the task (`t`)\n    })\n}", 
            "title": "A caveat about using variables in anonymous functions"
        }, 
        {
            "location": "/writing_workflows/#connecting-processes-into-a-network", 
            "text": "Finally we need to define the data dependencies between our processes. We do\nthis by connecting the outports of one process to the inport of another\nprocess, using the  From  method available on each in-port object (Or the To  method on out-ports). We also need to connect the final out-port of the\npipeline to the workflow, so that the workflow can pull on this port\n(technically pulling on a Go channel), in order to drive the workflow.  // Connect network  world . In ( in ). From ( helloWriter . Out ( out ))", 
            "title": "Connecting processes into a network"
        }, 
        {
            "location": "/writing_workflows/#running-the-pipeline", 
            "text": "So, the final part probably explains itself, but the workflow component is a\nrelatively simple one that will start each component in a separate go-routine.  For technical reasons, one final process has to be run in the main go-routine\n(that where the program's  main()  function runs), but generally you don't\nneed to think about this, as the workflow will then use an in-built sink  process for this\npurpose. If you for any reason need to customize which process to use as the\n\"driver\" process, instead of the in-built sink. see the  SetDriver  section \nin the docs.  wf . Run ()", 
            "title": "Running the pipeline"
        }, 
        {
            "location": "/writing_workflows/#summary", 
            "text": "So with this, we have done everything needed to set up a file-based batch workflow system.  In summary, what we did, was to:   Initialize processes  For each out-port, define a file-naming strategy  Specify dependencies by connecting out- and in-ports  Run the pipeline   This actually turns out to be a fixed set of components that always need to be\nincluded when writing workflows, so it might be good to keep them in mind and\nmemorize these steps, if needed.  For more examples, see the  examples folder \nin the GitHub repository.", 
            "title": "Summary"
        }, 
        {
            "location": "/howtos/partial_workflows/", 
            "text": "SciPipe allows you to, on-demand, run only specific parts of a workflow. This\ncan be useful especially if you are doing modifications far up in an already\ndeveloped workflow, and want to run only up to a specific process, rather\nthan also running all downstream processes, which might be unnecessary heavy.\n\n\nThis can be done by using the\n\nworkflow.RunTo()\n\nmethod. By using this instead of the normal \nworkflow.Run()\n method, scipipe\nwill only run this process and all upstream processes of that one.\n\n\nSee also a\n\nsimple\nexample\n\nof where this is used.\n\n\nThere are a few other variants for specifying parts of workflows (and more\nmight be added in the future), such as specifying individual process names,\nor providing the process structs themselves. Please refer to the relevant\nparts of the\n\nworkflow\ndocumentation\n\nfor more about that.", 
            "title": "Running parts of workflows"
        }, 
        {
            "location": "/howtos/parameters/", 
            "text": "Parameters are arguments sent to commands as flags, or unnamed values, or\nsometimes just the occurance of flags.\n\n\nSciPipe does not provide one unified way to handle parameters, but instead\nsuggest a few different strategies, dependent on the usage pattern. This is\nbecause it turns out that there is a very large variety in how parameters can\nbe used with shell commands.\n\n\nTo keep SciPipe a small and flexible tool, we instead mostly leave the choice\nup to the workflow author to create a solution for each case, using a few helper\ntools provided with SciPipe, but also all the programming facilities built in to\nthe Go programming language.\n\n\nBelow we will discuss how to handle the most common uses for for parameters in\nSciPipe. For any more complicated use cases not covered here, please refer to\nthe \nmailing list\n or the\n\nchat\n, to ask your question.\n\n\nStatic parameters\n\n\nIf parameters in your shell command is always, the same, you can just add them\n\"manually\" to the shell command pattern used to create your process.\n\n\nFor example, if you always want to write the string \"hello\" to output files,\nyou could create your processes with this string added manually:\n\n\nhelloWriter\n \n:=\n \nscipipe\n.\nNewProc\n(\nhelloWriter\n,\n \necho hello \n {o:outfile}\n)\n\n\n\n\n\n\nSee also\n\n\n\n\nStatic parameters example\n\n\n\n\nReceive parameters dynamically\n\n\nReceiving parameters dynamically is a much more technically demandning solution\nthan using static parameters.\n\n\nThe idea is that by using placeholders for parameter values in a command, each\nparameter for a particular process, will automatically get a channel of type\nstring, on which it can receive values. When the process is ready to execute\nanother shell command, it receives one item on each parameter ports, in\naddition to receiving one file on each (file-)in-port, and merges the values\ninto the shell command, before executing it.\n\n\nAn example of this would be a little too complicated to cover briefly on this\npage, so please instead see the \ndynamic parameters example\n.\nIn the \nRun method of the Combinatorics task\n\nyou will find the code used to send values (all combinations of values in three\narrays of lenght 3, in this case).\n\n\nSee also\n\n\n\n\nDynamic parameters example\n\n\n\n\nHandle boolean flags\n\n\nTopic coming soon. Please add it as a support request in the \nissue tracker\n\nif you need this information fast, and we can prioritize writing it asap.\n\n\nHandling parameters in re-usable components\n\n\nTopic coming soon. Please add it as a support request in the \nissue tracker\n\nif you need this information fast, and we can prioritize writing it asap.\n\n\nRelevant examples\n\n\n\n\nStatic parameters\n\n\nReceive parameters dynamically", 
            "title": "Using parameters in commands and file names"
        }, 
        {
            "location": "/howtos/parameters/#static-parameters", 
            "text": "If parameters in your shell command is always, the same, you can just add them\n\"manually\" to the shell command pattern used to create your process.  For example, if you always want to write the string \"hello\" to output files,\nyou could create your processes with this string added manually:  helloWriter   :=   scipipe . NewProc ( helloWriter ,   echo hello   {o:outfile} )", 
            "title": "Static parameters"
        }, 
        {
            "location": "/howtos/parameters/#see-also", 
            "text": "Static parameters example", 
            "title": "See also"
        }, 
        {
            "location": "/howtos/parameters/#receive-parameters-dynamically", 
            "text": "Receiving parameters dynamically is a much more technically demandning solution\nthan using static parameters.  The idea is that by using placeholders for parameter values in a command, each\nparameter for a particular process, will automatically get a channel of type\nstring, on which it can receive values. When the process is ready to execute\nanother shell command, it receives one item on each parameter ports, in\naddition to receiving one file on each (file-)in-port, and merges the values\ninto the shell command, before executing it.  An example of this would be a little too complicated to cover briefly on this\npage, so please instead see the  dynamic parameters example .\nIn the  Run method of the Combinatorics task \nyou will find the code used to send values (all combinations of values in three\narrays of lenght 3, in this case).", 
            "title": "Receive parameters dynamically"
        }, 
        {
            "location": "/howtos/parameters/#see-also_1", 
            "text": "Dynamic parameters example", 
            "title": "See also"
        }, 
        {
            "location": "/howtos/parameters/#handle-boolean-flags", 
            "text": "Topic coming soon. Please add it as a support request in the  issue tracker \nif you need this information fast, and we can prioritize writing it asap.", 
            "title": "Handle boolean flags"
        }, 
        {
            "location": "/howtos/parameters/#handling-parameters-in-re-usable-components", 
            "text": "Topic coming soon. Please add it as a support request in the  issue tracker \nif you need this information fast, and we can prioritize writing it asap.", 
            "title": "Handling parameters in re-usable components"
        }, 
        {
            "location": "/howtos/parameters/#relevant-examples", 
            "text": "Static parameters  Receive parameters dynamically", 
            "title": "Relevant examples"
        }, 
        {
            "location": "/howtos/golang_components/", 
            "text": "Beware: Technical topic, best suited for power-users!\n\n\nIf you want to write a component with Go code, but would like to have it work\nseamlessly with other workflow processes in SciPipe, without reimplementing the\nwhole \nProcess\n\nfunctionality yourself, there is a way to do it: By using the \nCustomExecute\n\nfield of Process.\n\n\nIn short, it can be done like this:\n\n\n// Initiate task from a \nshell like\n pattern, though here we\n\n\n// just specify the out-port, and nothing else. We have to\n\n\n// specify the out-port (and any other ports we plan to use later),\n\n\n// so that they are correctly initialized.\n\n\nfooWriter\n \n:=\n \nsci\n.\nNewProc\n(\nfooer\n,\n \n{o:foo}\n)\n\n\n\n// Set the output formatter to a static string\n\n\nfooWriter\n.\nSetOut\n(\nfoo\n,\n \nfoo.txt\n)\n\n\n\n// Create the custom execute function, with pure Go code and\n\n\n// add it to the CustomExecute field of the fooWriter process\n\n\nfooWriter\n.\nCustomExecute\n \n=\n \nfunc\n(\ntask\n \n*\nsci\n.\nTask\n)\n \n{\n\n    \ntask\n.\nOutIP\n(\nfoo\n).\nWrite\n([]\nbyte\n(\nfoo\\n\n))\n\n\n}\n\n\n\n\n\n\nFor a more detailed example, see \nthis example\n\n(Have a look at the \nNewFooer()\n\nand \nNewFoo2Barer()\n\nfactory functions in particular!)", 
            "title": "Creating components in Go"
        }, 
        {
            "location": "/howtos/reusable_components/", 
            "text": "What are re-usable components\n\n\nWith re-usable components, we mean components that can be stored in a Go\npackage and imported and used later.\n\n\nIn order for components in such a library to be easy to use, the ports need\nto be static methods bound to the process struct, rather than just stored by\na string ID in a generic port map, like the \nIn()\n and \nOut()\n methods on\n\nProcess\n processes. This is so that the methods can show up in the\nauto-completion / intellisense function in code editors removing the need to\nlook up the name of the ports manually in the library code all the time.\n\n\nHow to create re-usable components in SciPipe\n\n\nProcess processes created with the \nscipipe.NewProc()\n command, can be turned\ninto such \"re-usable\" component by using a wrapping strategy, that is\ndemonstrated in an \nexample on GitHub\n.\n\n\nThe idea is to create a new struct type for the re-usable component, and\nthen, in the factory method for the process, create an \"inner\" process of\ntype Process, using \nNewProc()\n as in the normal case, embedding that in the\nouter struct and then adding statically defined accessor methods for each of\nthe ports in the inner process, with a similar name. So, if the inner process\nhas an outport named \"foo\", you would define an accessor method named\n\nmyproc.OutFoo()\n that returns this port from the inner process.\n\n\nLet's look at a code example of how this works, by creating a process that just\nwrites \"hi\" to a file:\n\n\ntype\n \nHiWriter\n \nstruct\n \n{\n\n    \n// Embedd a Process struct\n\n    \n*\nsci\n.\nProcess\n\n\n}\n\n\n\nfunc\n \nNewHiWriter\n()\n \n*\nHiWriter\n \n{\n\n    \n// Initialize a normal \nProcess\n to use as an \ninner\n process\n\n    \ninnerHiWriter\n \n:=\n \nsci\n.\nNewProc\n(\nhiwriter\n,\n \necho hi \n {o:hifile}\n)\n\n    \ninnerHiWriter\n.\nSetOut\n(\nhifile\n,\n \nhi.txt\n)\n\n\n    \n// Create a new HiWriter process with the inner process embedded into it\n\n    \nreturn\n \nHiWriter\n{\ninnerHiWriter\n}\n\n\n}\n\n\n\n// OutHiFile provides a static version of the \nhifile\n port in the inner\n\n\n// (embedded) process\n\n\nfunc\n \n(\np\n \n*\nHiWriter\n)\n \nOutHiFile\n()\n \n*\nsci\n.\nOutPort\n \n{\n\n    \n// Return the inner process\n port named \nhifile\n\n    \nreturn\n \np\n.\nOut\n(\nhifile\n)\n\n\n}\n\n\n\n\n\n\nSee also\n\n\n\n\nA full, working, workflow example using this trategy", 
            "title": "Creating re-usable components"
        }, 
        {
            "location": "/howtos/reusable_components/#what-are-re-usable-components", 
            "text": "With re-usable components, we mean components that can be stored in a Go\npackage and imported and used later.  In order for components in such a library to be easy to use, the ports need\nto be static methods bound to the process struct, rather than just stored by\na string ID in a generic port map, like the  In()  and  Out()  methods on Process  processes. This is so that the methods can show up in the\nauto-completion / intellisense function in code editors removing the need to\nlook up the name of the ports manually in the library code all the time.", 
            "title": "What are re-usable components"
        }, 
        {
            "location": "/howtos/reusable_components/#how-to-create-re-usable-components-in-scipipe", 
            "text": "Process processes created with the  scipipe.NewProc()  command, can be turned\ninto such \"re-usable\" component by using a wrapping strategy, that is\ndemonstrated in an  example on GitHub .  The idea is to create a new struct type for the re-usable component, and\nthen, in the factory method for the process, create an \"inner\" process of\ntype Process, using  NewProc()  as in the normal case, embedding that in the\nouter struct and then adding statically defined accessor methods for each of\nthe ports in the inner process, with a similar name. So, if the inner process\nhas an outport named \"foo\", you would define an accessor method named myproc.OutFoo()  that returns this port from the inner process.  Let's look at a code example of how this works, by creating a process that just\nwrites \"hi\" to a file:  type   HiWriter   struct   { \n     // Embedd a Process struct \n     * sci . Process  }  func   NewHiWriter ()   * HiWriter   { \n     // Initialize a normal  Process  to use as an  inner  process \n     innerHiWriter   :=   sci . NewProc ( hiwriter ,   echo hi   {o:hifile} ) \n     innerHiWriter . SetOut ( hifile ,   hi.txt ) \n\n     // Create a new HiWriter process with the inner process embedded into it \n     return   HiWriter { innerHiWriter }  }  // OutHiFile provides a static version of the  hifile  port in the inner  // (embedded) process  func   ( p   * HiWriter )   OutHiFile ()   * sci . OutPort   { \n     // Return the inner process  port named  hifile \n     return   p . Out ( hifile )  }", 
            "title": "How to create re-usable components in SciPipe"
        }, 
        {
            "location": "/howtos/reusable_components/#see-also", 
            "text": "A full, working, workflow example using this trategy", 
            "title": "See also"
        }, 
        {
            "location": "/howtos/globbing/", 
            "text": "This is a feature that is \nbeing worked on currently\n.\nPlease see \nthe issue\n to follow the status of it.", 
            "title": "Globbing files"
        }, 
        {
            "location": "/howtos/splitting/", 
            "text": "At the time of writing this, the recommended way to split files is to use the\n\nFileSplitter\n component\n\nin the \ncomponent library\n shipped with SciPipe.\n\n\nAlso see the \npage about Scatter/Gather\n.", 
            "title": "Splitting files"
        }, 
        {
            "location": "/howtos/joining/", 
            "text": "See the \nConcatenator component\n.\n\n\nAlso see the \npage about Scatter/Gather\n.", 
            "title": "Joining multiple files"
        }, 
        {
            "location": "/howtos/streaming/", 
            "text": "SciPipe can stream the output via UNIX \nnamed pipes (or \"FIFO files\")\n.\n\n\nStreaming can be turned on, on out-ports when creating processes with\n\nNewProc()\n, by using \n{os:outport_name}\n as placeholder, instead of the\nnormal \n{o:outport_name}\n (note the addisional \"s\")\n\n\nYou can see how this is used in \nthis example on GitHub\n.\n\n\nNote that when streaming, you will not get an output file for the output in\nquestion.\n\n\nNote also that you still have to provide a path formatting strategy (via some\nof the \nProcess.SetOut...()\n functions, or by manually adding one to\n\nProcess.PathFuncs\n. This is because a uniqe file name is needed in\norder to create any audit files, as well as to give a unique name for the named\npipe.\n\n\nSee also\n\n\n\n\nStreaming example on GitHub\n.", 
            "title": "Enable streaming between components"
        }, 
        {
            "location": "/howtos/streaming/#see-also", 
            "text": "Streaming example on GitHub .", 
            "title": "See also"
        }, 
        {
            "location": "/howtos/subworkflows/", 
            "text": "It is possible in SciPipe to wrap a whole workflow in a process, so that it can be used\nas any other process, in larger workflows.\n\n\nThis is demonstrated in \nthis example on GitHub\n.", 
            "title": "Creating sub-workflows"
        }, 
        {
            "location": "/howtos/scatter_gather/", 
            "text": "There is \nwork going on to add better scatter/gather support in SciPipe (Issue #20)\n.\nIn the meanwhile, have a look at \nthis example on GitHub\n\nwhich demonstrates one way of doing a scatter gather operation, using the Splitter component (\nsee line 24\n)\nand two concatenator components (\nsee lines 38-39\n)\nto do the scatter, and gather, operations respectively.", 
            "title": "Implement scatter/gather workflows"
        }, 
        {
            "location": "/howtos/hpc/", 
            "text": "This is \nbeing worked on right now (issue #38)\n.\n\n\nWhat you can do right now, is to use the \nPrepend\n field in processes, to add a\n\nsalloc\n command string (in the case of\nSLURM), or any analogous blocking command to other resource managers.\n\n\nSo, something like this (See on the third line how the salloc-line is added to the process):\n\n\nwf\n \n:=\n \nscipipe\n.\nNewWorkflow\n(\nHello_World_Workflow\n,\n \n4\n)\n\n\nmyProc\n \n:=\n \nwf\n.\nNewProc\n(\nhello_world\n,\n \necho Hello World; sleep 10;\n)\n\n\nmyProc\n.\nPrepend\n \n=\n \nsalloc -A projectABC123 -p core -t 1:00 -J HelloWorld\n\n\n\n\n\n\n(Beware: This is not a full code example, and won't compile without some more boilerplate, which you can find in the introductory examples)\n\n\nYou can find the updated GoDoc for the process struct \nhere\n.", 
            "title": "Interacting with an HPC resource manager"
        }, 
        {
            "location": "/howtos/constrain_resource_usage/", 
            "text": "It is important to carefully manage how much resources (CPU and memory) your\nworkflows are using, so that you don't overbook you compute node(s).\n\n\nIn SciPipe you can do that using two settings:\n\n\n\n\nMax concurrent tasks, which is set on the workflow level, when initiating a new workflow.\n\n\nCores per tasks, that can be set on processes after they are initialized.\n\n\n\n\nMax concurrent tasks is a required setting when initializing workflows, while\ncores per task can be left to the default, which is 1 core per task.\n\n\nYou might want to change this number if for example you have a software that\nuses more memory than the available memory on your computer divided by the max\nconcurrent tasks number you have set.\n\n\nFor example, if you have 8GB of free memory, and have set max concurrent tasks\non your workflow to 4, but you have a process whose commandline application\nuses not 2GB of memory, but 4GB, then you might want to set cores per tasks for\nthat process to 2, so that it gets the double amount of memory.\n\n\nIn practice, you set cores per task by setting the field \nCoresPerTask\n on the process struct, after it is initiated. \n\n\nExample\n\n\nfoo\n \n:=\n \nscipipe\n.\nNewProc\n(\nfoo\n,\n \necho foo \n {o:foofile}\n)\n\n\nfoo\n.\nCoresPerTask\n \n=\n \n2", 
            "title": "Constrain resource usage"
        }, 
        {
            "location": "/howtos/constrain_resource_usage/#example", 
            "text": "foo   :=   scipipe . NewProc ( foo ,   echo foo   {o:foofile} )  foo . CoresPerTask   =   2", 
            "title": "Example"
        }, 
        {
            "location": "/howtos/plot_workflow_graph/", 
            "text": "SciPipe 0.8.0\n\nintroduced a feature to plot a directed graph of workflows in SciPipe [1].\nThis can be done in two ways:\n\n\n\n\nJust producing a DOT text file, with the graph definition\n\n\nAlso converting this DOT file to PDF.\n\n\n\n\nNumber 1. above can be done without any external dependencies, while number 2\nrequires that graphviz, with the \ndot\n command is installed on the system (On\nUbuntu it can be installed with the command: \nsudo apt-get install graphviz\n).\n\n\nHow to plot graphs\n\n\nTo write a .dot file in SciPipe, include a line like follows, in your workflow\ndefinition, provided that you have initiated the variable \nwf\n with a workflow\nstruct:\n\n\nfunc\n \nmain\n()\n \n{\n\n    \nwf\n \n:=\n \nscipipe\n.\nNewWorkflow\n(\nmy workflow\n,\n \n4\n)\n\n    \n// Workflow code here\n\n    \nwf\n.\nPlotGraph\n(\nmy_workflow_graph.dot\n)\n \n// \n-- SEE THIS LINE!\n\n    \nwf\n.\nRun\n()\n\n\n}\n\n\n\n\n\n\nIf you want to also convert the dot file to PDF in one go, instead change the\nnext last line to:\n\n\n    \nwf\n.\nPlotGraphPDF\n(\nmy_workflow_graph.dot\n)\n\n\n\n\n\n\nHow to plot graphs conditionally based on a flag\n\n\nNow, you might not want to generate a new plot every time you run your workflow\n(although, perhaps you would? ... checking in a .dot version of your workflow\ncould in fact be a great way to keep a more readable version of your workflow\nat hand ... but anyhow), you could make the plotting optional, based on a flag.\nThis is something we've found ourselves doing quite often at pharmb.io. This\ncould be done as follows (more complete code example):\n\n\npackage\n \nmain\n\n\n\nimport\n \n(\n\n    \nflag\n\n    \ngithub.com/scipipe/scipipe\n\n\n)\n\n\n\nvar\n \n(\n\n    \nplotGraph\n \n=\n \nflag\n.\nBool\n(\nplotgraph\n,\n \nfalse\n,\n \nPlot a directed graph of the workflow to PDF\n)\n\n\n)\n\n\n\nfunc\n \nmain\n()\n \n{\n\n    \nflag\n.\nParse\n()\n\n\n    \nwf\n \n:=\n \nscipipe\n.\nNewWorkflow\n(\ntestwf\n,\n \n4\n)\n\n    \nwf\n.\nNewProc\n(\nfoo\n,\n \necho foo \n {o:out}\n)\n\n\n    \nif\n \n*\nplotGraph\n \n{\n\n        \nwf\n.\nPlotGraphPDF\n(\nwfgraph.dot\n)\n\n    \n}\n\n    \nwf\n.\nRun\n()\n\n\n}\n\n\n\n\n\n\nNow, the graph will only plotted if you run your workflow with the\n\n-plotgraph\n flag, e.g:\n\n\ngo run myworkflow.go -plotgraph\n\n\n\n\n\nLinks\n\n\n\n\nGoDoc for Workflow.PlotGraph()\n\n\nGoDoc for Workflow.PlotGraphPDF()\n\n\n\n\nFootnotes\n\n\n[1] these are often called \"DAG\" for \"Directed Acyclic Graph\", but\nSciPipe does not have a guarantee or requirement on acyclicness of the graph,\nthus just \"directed graph\".", 
            "title": "Plotting workflow graphs"
        }, 
        {
            "location": "/howtos/plot_workflow_graph/#how-to-plot-graphs", 
            "text": "To write a .dot file in SciPipe, include a line like follows, in your workflow\ndefinition, provided that you have initiated the variable  wf  with a workflow\nstruct:  func   main ()   { \n     wf   :=   scipipe . NewWorkflow ( my workflow ,   4 ) \n     // Workflow code here \n     wf . PlotGraph ( my_workflow_graph.dot )   //  -- SEE THIS LINE! \n     wf . Run ()  }   If you want to also convert the dot file to PDF in one go, instead change the\nnext last line to:       wf . PlotGraphPDF ( my_workflow_graph.dot )", 
            "title": "How to plot graphs"
        }, 
        {
            "location": "/howtos/plot_workflow_graph/#how-to-plot-graphs-conditionally-based-on-a-flag", 
            "text": "Now, you might not want to generate a new plot every time you run your workflow\n(although, perhaps you would? ... checking in a .dot version of your workflow\ncould in fact be a great way to keep a more readable version of your workflow\nat hand ... but anyhow), you could make the plotting optional, based on a flag.\nThis is something we've found ourselves doing quite often at pharmb.io. This\ncould be done as follows (more complete code example):  package   main  import   ( \n     flag \n     github.com/scipipe/scipipe  )  var   ( \n     plotGraph   =   flag . Bool ( plotgraph ,   false ,   Plot a directed graph of the workflow to PDF )  )  func   main ()   { \n     flag . Parse () \n\n     wf   :=   scipipe . NewWorkflow ( testwf ,   4 ) \n     wf . NewProc ( foo ,   echo foo   {o:out} ) \n\n     if   * plotGraph   { \n         wf . PlotGraphPDF ( wfgraph.dot ) \n     } \n     wf . Run ()  }   Now, the graph will only plotted if you run your workflow with the -plotgraph  flag, e.g:  go run myworkflow.go -plotgraph", 
            "title": "How to plot graphs conditionally based on a flag"
        }, 
        {
            "location": "/howtos/plot_workflow_graph/#links", 
            "text": "GoDoc for Workflow.PlotGraph()  GoDoc for Workflow.PlotGraphPDF()", 
            "title": "Links"
        }, 
        {
            "location": "/howtos/plot_workflow_graph/#footnotes", 
            "text": "[1] these are often called \"DAG\" for \"Directed Acyclic Graph\", but\nSciPipe does not have a guarantee or requirement on acyclicness of the graph,\nthus just \"directed graph\".", 
            "title": "Footnotes"
        }, 
        {
            "location": "/howtos/convert_audit_logs/", 
            "text": "SciPipe 0.8.0\n\nintroduced experimental support for converting audit logs (those\n\n.audit.json\n files produced to accompany all output files from SciPipe)\ninto other formats, such as HTML, TeX (for further conversion to PDF) or\neven executable Bash-scripts. Here's how to do it.\n\n\nConvert audit log to HTML\n\n\nGiven that you have an audit log file with the name \nmyfile.audit.json\n,\nthen execute:\n\n\nscipipe audit2html myfile.audit.json\n\n\n\n\n\nThis will produce an HTML file named \nmyfile.audit.html\n, which you can\nview in a web browser.\n\n\nConvert audit log to TeX\n\n\nGiven that you have an audit log file with the name \nmyfile.audit.json\n,\nthen execute:\n\n\nscipipe audit2tex myfile.audit.json\n\n\n\n\n\nThis will produce an HTML file named \nmyfile.audit.tex\n, which you can\neither edit manually, or convert directly to PDF using the \npdflatex\n\ncommand like so:\n\n\npdflatex myfile.audit.tex\n\n\n\n\n\nConverting to PDF requires that you have a TeX installation on your system.\nOn Ubuntu, you can install the base package or TeX live with \nsudo apt-get\ninstall texlive-base\n.\n\n\nConvert audit log to Bash\n\n\nGiven that you have an audit log file with the name \nmyfile.audit.json\n,\nthen execute:\n\n\nscipipe audit2bash myfile.audit.json\n\n\n\n\n\nThis will produce a Bash-file named \nmyfile.audit.sh\n, which you can\nexecute like so:\n\n\nsh myfile.audit.sh\n\n\n\n\n\n... in order to reproduce the file again from scratch, if it is removed,\ngiven that you have all the dependent files and tools installed on your\nsystem.", 
            "title": "Convert audit logs to other formats"
        }, 
        {
            "location": "/howtos/convert_audit_logs/#convert-audit-log-to-html", 
            "text": "Given that you have an audit log file with the name  myfile.audit.json ,\nthen execute:  scipipe audit2html myfile.audit.json  This will produce an HTML file named  myfile.audit.html , which you can\nview in a web browser.", 
            "title": "Convert audit log to HTML"
        }, 
        {
            "location": "/howtos/convert_audit_logs/#convert-audit-log-to-tex", 
            "text": "Given that you have an audit log file with the name  myfile.audit.json ,\nthen execute:  scipipe audit2tex myfile.audit.json  This will produce an HTML file named  myfile.audit.tex , which you can\neither edit manually, or convert directly to PDF using the  pdflatex \ncommand like so:  pdflatex myfile.audit.tex  Converting to PDF requires that you have a TeX installation on your system.\nOn Ubuntu, you can install the base package or TeX live with  sudo apt-get\ninstall texlive-base .", 
            "title": "Convert audit log to TeX"
        }, 
        {
            "location": "/howtos/convert_audit_logs/#convert-audit-log-to-bash", 
            "text": "Given that you have an audit log file with the name  myfile.audit.json ,\nthen execute:  scipipe audit2bash myfile.audit.json  This will produce a Bash-file named  myfile.audit.sh , which you can\nexecute like so:  sh myfile.audit.sh  ... in order to reproduce the file again from scratch, if it is removed,\ngiven that you have all the dependent files and tools installed on your\nsystem.", 
            "title": "Convert audit log to Bash"
        }, 
        {
            "location": "/examples/", 
            "text": "Examples\n\n\nWe plan to go through a few examples in more depth here soon, but in the\nmeanwhile, see the \nexamples folder\n\nin the main scipipe repository, for a bunch of examples spanning much of the\nfunctionality in SciPipe.", 
            "title": "Examples"
        }, 
        {
            "location": "/examples/#examples", 
            "text": "We plan to go through a few examples in more depth here soon, but in the\nmeanwhile, see the  examples folder \nin the main scipipe repository, for a bunch of examples spanning much of the\nfunctionality in SciPipe.", 
            "title": "Examples"
        }, 
        {
            "location": "/other_resources/", 
            "text": "Publications mentioning SciPipe\n\n\n\n\nSee \na poster on SciPipe\n, presented at the \ne-Science Academy in Lund, on Oct 12-13 2016\n.\n\n\nSee \nslides from a recent presentation of SciPipe for use in a Bioinformatics setting\n.\n\n\nThe architecture of SciPipe is based on an \nflow-based\n  programming\n like\n  pattern in pure Go presented in\n  \nthis\n and\n  \nthis\n\n  blog posts on Gopher Academy.", 
            "title": "Other Resources"
        }, 
        {
            "location": "/other_resources/#publications-mentioning-scipipe", 
            "text": "See  a poster on SciPipe , presented at the  e-Science Academy in Lund, on Oct 12-13 2016 .  See  slides from a recent presentation of SciPipe for use in a Bioinformatics setting .  The architecture of SciPipe is based on an  flow-based\n  programming  like\n  pattern in pure Go presented in\n   this  and\n   this \n  blog posts on Gopher Academy.", 
            "title": "Publications mentioning SciPipe"
        }, 
        {
            "location": "/acknowledgements/", 
            "text": "Acknowledgements\n\n\n\n\nSciPipe is very heavily dependent on the proven principles form \nFlow-Based\n  Programming (FBP)\n, as invented by \nJohn Paul Morrison\n.\n  From Flow-based programming, SciPipe uses the ideas of separate network\n  (workflow dependency graph) definition, named in- and out-ports,\n  sub-networks/sub-workflows and bounded buffers (already available in Go's\n  channels) to make writing workflows as easy as possible.\n\n\nThis library is has been much influenced/inspired also by the\n  \nGoFlow\n library by \nVladimir Sibirov\n.\n\n\nThanks to \nEgon Elbre\n for helpful input on the\n  design of the internals of the pipeline, and processes, which greatly\n  simplified the implementation.\n\n\nThis work is financed by faculty grants and other financing for the \nPharmaceutical Bioinformatics group\n of \nDept. of\n  Pharmaceutical Biosciences\n at \nUppsala University\n, and by \nSwedish Research Council\n\n  through the Swedish \nNational Bioinformatics Infrastructure Sweden\n.\n\n\nSupervisor for the project is \nOla Spjuth\n.", 
            "title": "Acknowledgements"
        }, 
        {
            "location": "/acknowledgements/#acknowledgements", 
            "text": "SciPipe is very heavily dependent on the proven principles form  Flow-Based\n  Programming (FBP) , as invented by  John Paul Morrison .\n  From Flow-based programming, SciPipe uses the ideas of separate network\n  (workflow dependency graph) definition, named in- and out-ports,\n  sub-networks/sub-workflows and bounded buffers (already available in Go's\n  channels) to make writing workflows as easy as possible.  This library is has been much influenced/inspired also by the\n   GoFlow  library by  Vladimir Sibirov .  Thanks to  Egon Elbre  for helpful input on the\n  design of the internals of the pipeline, and processes, which greatly\n  simplified the implementation.  This work is financed by faculty grants and other financing for the  Pharmaceutical Bioinformatics group  of  Dept. of\n  Pharmaceutical Biosciences  at  Uppsala University , and by  Swedish Research Council \n  through the Swedish  National Bioinformatics Infrastructure Sweden .  Supervisor for the project is  Ola Spjuth .", 
            "title": "Acknowledgements"
        }, 
        {
            "location": "/related_tools/", 
            "text": "Related tools\n\n\nFind below a few tools that are more or less similar to SciPipe that are worth worth checking out before\ndeciding on what tool fits you best (in approximate order of similarity to SciPipe):\n\n\n\n\nNextFlow\n\n\nLuigi\n/\nSciLuigi\n\n\nBPipe\n\n\nSnakeMake\n\n\nCuneiform", 
            "title": "Related Tools"
        }, 
        {
            "location": "/related_tools/#related-tools", 
            "text": "Find below a few tools that are more or less similar to SciPipe that are worth worth checking out before\ndeciding on what tool fits you best (in approximate order of similarity to SciPipe):   NextFlow  Luigi / SciLuigi  BPipe  SnakeMake  Cuneiform", 
            "title": "Related tools"
        }
    ]
}