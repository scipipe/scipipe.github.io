{
    "docs": [
        {
            "location": "/", 
            "text": "SciPipe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject links: \nGitHub repo\n | \nIssue Tracker\n | \nMailing List\n | \nChat\n\n\nProject updates\n\n\n\n\nNEW blog post:\n \nProvenance reports in Scientific Workflows\n - going into details about how SciPipe is addressing provenance\n\n\nNEW blog post:\n \nFirst production workflow run with SciPipe\n\n\nNEW video:\n \nWatch a screencast on how to write a Hello World workflow in SciPipe [15:28]\n\n\n\n\nIntroduction\n\n\n\nSciPipe is a library for writing \nScientific\nWorkflows\n, sometimes\nalso called \"pipelines\", in the \nGo programming language\n.\n\n\nWhen you need to run many commandline programs that depend on each other in\ncomplex ways, SciPipe helps by making the process of running these programs\nflexible, robust and reproducible. SciPipe also lets you restart an interrupted\nrun without over-writing already produced output and produces an audit report\nof what was run, among many other things.\n\n\nSciPipe is built on the proven principles of \nFlow-Based Programming\n\n(FBP) to achieve maximum flexibility, productivity and agility when designing\nworkflows.  Compared to plain dataflow, FBP provides the benefits that\nprocesses are fully self-contained, so that a library of re-usable components\ncan be created, and plugged into new workflows ad-hoc.\n\n\nSimilar to other FBP systems, SciPipe workflows can be likened to a network of\nassembly lines in a factory, where items (files) are flowing through a network\nof conveyor belts, stopping at different independently running stations\n(processes) for processing, as depicted in the picture above.\n\n\nSciPipe was initially created for problems in bioinformatics and\ncheminformatics, but works equally well for any problem involving pipelines of\ncommandline applications.\n\n\nProject status:\n SciPipe is still alpha software and minor breaking API\nchanges still happens as we try to streamline the process of writing workflows.\nPlease follow the commit history closely for any API updates if you have code\nalready written in SciPipe (Let us know if you need any help in migrating code\nto the latest API).\n\n\nBenefits\n\n\nSome key benefits of SciPipe, that are not always found in similar systems:\n\n\n\n\nIntuitive behaviour:\n SciPipe operates by flowing data (files) through a\n  network of channels and processes, not unlike the conveyor belts and stations\n  in a factory.\n\n\nFlexible:\n Processes that wrap command-line programs or scripts, can be\n  combined with processes coded directly in Golang.\n\n\nCustom file naming:\n SciPipe gives you full control over how files are\n  named, making it easy to find your way among the output files of your\n  workflow.\n\n\nPortable:\n Workflows can be distributed either as Go code to be run with\n  \ngo run\n, or as stand-alone executable files that run on almost any UNIX-like\n  operating system.\n\n\nEasy to debug:\n As everything in SciPipe is just Go code, you can use some\n  of the available debugging tools, or just \nprintln()\n statements, to debug\n  your workflow. \n\n\nSupports streaming:\n Can stream outputs via UNIX FIFO files, to avoid temporary storage.\n\n\nEfficient and Parallel:\n Workflows are compiled into statically compiled\n  code that runs fast. SciPipe also leverages pipeline parallelism between\n  processes as well as task parallelism when there are multiple inputs to a\n  process, making efficient use of multiple CPU cores.\n\n\n\n\nKnown limitations\n\n\n\n\nThere are still a number of missing good-to-have features for workflow\n  design. See the \nissue tracker\n\n  for details.\n\n\nThere is not (yet) support for the \nCommon Workflow Language\n.\n\n\n\n\nHello World example\n\n\nLet's look at an example workflow to get a feel for what writing workflows in\nSciPipe looks like:\n\n\npackage\n \nmain\n\n\n\nimport\n \n(\n\n    \n// Import SciPipe into the main namespace (generally frowned upon but could\n\n    \n// be argued to be reasonable for short-lived workflow scripts like this)\n\n    \n.\n \ngithub.com/scipipe/scipipe\n\n\n)\n\n\n\nfunc\n \nmain\n()\n \n{\n\n    \n// Init workflow with a name, and max concurrent tasks so we don\nt overbook\n\n    \n// our CPU\n\n    \nwf\n \n:=\n \nNewWorkflow\n(\nhello_world\n,\n \n4\n)\n\n\n    \n// Initialize processes and set output file paths\n\n    \nhello\n \n:=\n \nwf\n.\nNewProc\n(\nhello\n,\n \necho \nHello \n \n {o:out}\n)\n\n    \nhello\n.\nSetPathStatic\n(\nout\n,\n \nhello.txt\n)\n\n\n    \nworld\n \n:=\n \nwf\n.\nNewProc\n(\nworld\n,\n \necho $(cat {i:in}) World \n {o:out}\n)\n\n    \nworld\n.\nSetPathReplace\n(\nin\n,\n \nout\n,\n \n.txt\n,\n \n_world.txt\n)\n\n\n    \n// Connect network\n\n    \nworld\n.\nIn\n(\nin\n).\nConnect\n(\nhello\n.\nOut\n(\nout\n))\n\n\n    \n// Run workflow\n\n    \nwf\n.\nRun\n()\n\n\n}\n\n\n\n\n\n\nRunning the example\n\n\nLet's put the code in a file named \nscipipe_helloworld.go\n and run it:\n\n\n$ go run scipipe_helloworld.go \nAUDIT   \n2017\n/05/04 \n17\n:05:15 Task:hello         Executing command: \necho\n \nHello \n \n hello.txt.tmp\nAUDIT   \n2017\n/05/04 \n17\n:05:15 Task:world         Executing command: \necho\n \n$(\ncat hello.txt\n)\n World \n hello_world.txt.tmp\n\n\n\n\n\nLet's check what file SciPipe has generated:\n\n\n$ ls -1tr hello*\nhello.txt.audit.json\nhello.txt\nhello_world.txt\nhello_world.txt.audit.json\n\n\n\n\n\nAs you can see, it has created a file \nhello.txt\n, and \nhello_world.txt\n, and\nan accompanying \n.audit.json\n for each of these files.\n\n\nNow, let's check the output of the final resulting file:\n\n\n$ cat hello_world.txt\nHello World\n\n\n\n\n\nNow we can rejoice that it contains the text \"Hello World\", exactly as a proper\nHello World example should :)\n\n\nYou can find many more examples in the \nexamples folder\n in the GitHub repo.\n\n\nFor more information about how to write workflows using SciPipe, use the menu\nto the left, to browse the various topics!", 
            "title": "Introduction"
        }, 
        {
            "location": "/#scipipe", 
            "text": "Project links:  GitHub repo  |  Issue Tracker  |  Mailing List  |  Chat", 
            "title": "SciPipe"
        }, 
        {
            "location": "/#project-updates", 
            "text": "NEW blog post:   Provenance reports in Scientific Workflows  - going into details about how SciPipe is addressing provenance  NEW blog post:   First production workflow run with SciPipe  NEW video:   Watch a screencast on how to write a Hello World workflow in SciPipe [15:28]", 
            "title": "Project updates"
        }, 
        {
            "location": "/#introduction", 
            "text": "SciPipe is a library for writing  Scientific\nWorkflows , sometimes\nalso called \"pipelines\", in the  Go programming language .  When you need to run many commandline programs that depend on each other in\ncomplex ways, SciPipe helps by making the process of running these programs\nflexible, robust and reproducible. SciPipe also lets you restart an interrupted\nrun without over-writing already produced output and produces an audit report\nof what was run, among many other things.  SciPipe is built on the proven principles of  Flow-Based Programming \n(FBP) to achieve maximum flexibility, productivity and agility when designing\nworkflows.  Compared to plain dataflow, FBP provides the benefits that\nprocesses are fully self-contained, so that a library of re-usable components\ncan be created, and plugged into new workflows ad-hoc.  Similar to other FBP systems, SciPipe workflows can be likened to a network of\nassembly lines in a factory, where items (files) are flowing through a network\nof conveyor belts, stopping at different independently running stations\n(processes) for processing, as depicted in the picture above.  SciPipe was initially created for problems in bioinformatics and\ncheminformatics, but works equally well for any problem involving pipelines of\ncommandline applications.  Project status:  SciPipe is still alpha software and minor breaking API\nchanges still happens as we try to streamline the process of writing workflows.\nPlease follow the commit history closely for any API updates if you have code\nalready written in SciPipe (Let us know if you need any help in migrating code\nto the latest API).", 
            "title": "Introduction"
        }, 
        {
            "location": "/#benefits", 
            "text": "Some key benefits of SciPipe, that are not always found in similar systems:   Intuitive behaviour:  SciPipe operates by flowing data (files) through a\n  network of channels and processes, not unlike the conveyor belts and stations\n  in a factory.  Flexible:  Processes that wrap command-line programs or scripts, can be\n  combined with processes coded directly in Golang.  Custom file naming:  SciPipe gives you full control over how files are\n  named, making it easy to find your way among the output files of your\n  workflow.  Portable:  Workflows can be distributed either as Go code to be run with\n   go run , or as stand-alone executable files that run on almost any UNIX-like\n  operating system.  Easy to debug:  As everything in SciPipe is just Go code, you can use some\n  of the available debugging tools, or just  println()  statements, to debug\n  your workflow.   Supports streaming:  Can stream outputs via UNIX FIFO files, to avoid temporary storage.  Efficient and Parallel:  Workflows are compiled into statically compiled\n  code that runs fast. SciPipe also leverages pipeline parallelism between\n  processes as well as task parallelism when there are multiple inputs to a\n  process, making efficient use of multiple CPU cores.", 
            "title": "Benefits"
        }, 
        {
            "location": "/#known-limitations", 
            "text": "There are still a number of missing good-to-have features for workflow\n  design. See the  issue tracker \n  for details.  There is not (yet) support for the  Common Workflow Language .", 
            "title": "Known limitations"
        }, 
        {
            "location": "/#hello-world-example", 
            "text": "Let's look at an example workflow to get a feel for what writing workflows in\nSciPipe looks like:  package   main  import   ( \n     // Import SciPipe into the main namespace (generally frowned upon but could \n     // be argued to be reasonable for short-lived workflow scripts like this) \n     .   github.com/scipipe/scipipe  )  func   main ()   { \n     // Init workflow with a name, and max concurrent tasks so we don t overbook \n     // our CPU \n     wf   :=   NewWorkflow ( hello_world ,   4 ) \n\n     // Initialize processes and set output file paths \n     hello   :=   wf . NewProc ( hello ,   echo  Hello     {o:out} ) \n     hello . SetPathStatic ( out ,   hello.txt ) \n\n     world   :=   wf . NewProc ( world ,   echo $(cat {i:in}) World   {o:out} ) \n     world . SetPathReplace ( in ,   out ,   .txt ,   _world.txt ) \n\n     // Connect network \n     world . In ( in ). Connect ( hello . Out ( out )) \n\n     // Run workflow \n     wf . Run ()  }", 
            "title": "Hello World example"
        }, 
        {
            "location": "/#running-the-example", 
            "text": "Let's put the code in a file named  scipipe_helloworld.go  and run it:  $ go run scipipe_helloworld.go \nAUDIT    2017 /05/04  17 :05:15 Task:hello         Executing command:  echo   Hello     hello.txt.tmp\nAUDIT    2017 /05/04  17 :05:15 Task:world         Executing command:  echo   $( cat hello.txt )  World   hello_world.txt.tmp  Let's check what file SciPipe has generated:  $ ls -1tr hello*\nhello.txt.audit.json\nhello.txt\nhello_world.txt\nhello_world.txt.audit.json  As you can see, it has created a file  hello.txt , and  hello_world.txt , and\nan accompanying  .audit.json  for each of these files.  Now, let's check the output of the final resulting file:  $ cat hello_world.txt\nHello World  Now we can rejoice that it contains the text \"Hello World\", exactly as a proper\nHello World example should :)  You can find many more examples in the  examples folder  in the GitHub repo.  For more information about how to write workflows using SciPipe, use the menu\nto the left, to browse the various topics!", 
            "title": "Running the example"
        }, 
        {
            "location": "/install/", 
            "text": "Installing SciPipe\n\n\nInstalling SciPipe means first installing the Go programming langauge, and then\nusing Go's \ngo get\n command to install the SciPipe library. After this, you will\nbe able to use Go's \ngo run\n command to run SciPipe workflows.\n\n\nInstall Go\n\n\nInstall Go by following the instructions \non this page\n,\nfor your operating system.\n\n\nInstall SciPipe\n\n\nThen install SciPipe by running the following shell command:\n\n\ngo get github.com/scipipe/scipipe/...\n\n\n\n\n\nN.B:\n Don't miss the \n...\n, as otherwise the \nscipipe\n helper tool will not be installed.\n\n\nInitialize a new workflow file\n\n\nNow, you should be able to write code like in the example below, in files ending with \n.go\n.\n\n\nThe easiest way to get started is to let the scipipe tool generate a starting point for you:\n\n\nscipipe new myfirstworkflow.go\n\n\n\n\n\n... which you can then edit to your liking.\n\n\nRun your workflow\n\n\nTo run a \n.go\n file, use \ngo run\n:\n\n\ngo run myfirstworkflow.go\n\n\n\n\n\nSome tips about editors\n\n\nIn order to be productive with SciPipe, you will also need a Go editor or IDE\nwith support for auto-completion, sometimes also called \"intellisense\".\n\n\nWe can warmly recommend to use one of these editors, sorted by level of endorsement:\n\n\n\n\nVisual Studio Code\n with the \nGo plugin\n - If you want a very powerful almost IDE-like editor\n\n\nFatih's awesome \nvim-go\n plugin - if you are a Vim power-user\n\n\nLiteIDE\n - if you want a really simple, standalone Go-editor\n\n\n\n\nThere are also popular Go-plugins for \nSublime text\n,\n\nAtom\n and \nIntelliJ IDEA\n,\nand an upcoming Go IDE from JetBrains, called\n\nGogland\n, that might be worth checking out,\ndepending on your preferences.", 
            "title": "Installing"
        }, 
        {
            "location": "/install/#installing-scipipe", 
            "text": "Installing SciPipe means first installing the Go programming langauge, and then\nusing Go's  go get  command to install the SciPipe library. After this, you will\nbe able to use Go's  go run  command to run SciPipe workflows.", 
            "title": "Installing SciPipe"
        }, 
        {
            "location": "/install/#install-go", 
            "text": "Install Go by following the instructions  on this page ,\nfor your operating system.", 
            "title": "Install Go"
        }, 
        {
            "location": "/install/#install-scipipe", 
            "text": "Then install SciPipe by running the following shell command:  go get github.com/scipipe/scipipe/...  N.B:  Don't miss the  ... , as otherwise the  scipipe  helper tool will not be installed.", 
            "title": "Install SciPipe"
        }, 
        {
            "location": "/install/#initialize-a-new-workflow-file", 
            "text": "Now, you should be able to write code like in the example below, in files ending with  .go .  The easiest way to get started is to let the scipipe tool generate a starting point for you:  scipipe new myfirstworkflow.go  ... which you can then edit to your liking.", 
            "title": "Initialize a new workflow file"
        }, 
        {
            "location": "/install/#run-your-workflow", 
            "text": "To run a  .go  file, use  go run :  go run myfirstworkflow.go", 
            "title": "Run your workflow"
        }, 
        {
            "location": "/install/#some-tips-about-editors", 
            "text": "In order to be productive with SciPipe, you will also need a Go editor or IDE\nwith support for auto-completion, sometimes also called \"intellisense\".  We can warmly recommend to use one of these editors, sorted by level of endorsement:   Visual Studio Code  with the  Go plugin  - If you want a very powerful almost IDE-like editor  Fatih's awesome  vim-go  plugin - if you are a Vim power-user  LiteIDE  - if you want a really simple, standalone Go-editor   There are also popular Go-plugins for  Sublime text , Atom  and  IntelliJ IDEA ,\nand an upcoming Go IDE from JetBrains, called Gogland , that might be worth checking out,\ndepending on your preferences.", 
            "title": "Some tips about editors"
        }, 
        {
            "location": "/basic_concepts/", 
            "text": "Basic concepts\n\n\nIn SciPipe, we are discussing a few concepts all the time, so to make sure we\nare on the same page, we will below go through the basic ones briefly.\n\n\nProcesses\n\n\nThe probably most basic concept in SciPipe is the process.  A process is an\nasynchronously running component that is typically defined as a static,\n\"long-running\" part of the workflow, and the number of processes thus is\ntypically fixed for a workflow during its execution.\n\n\nOne can create customized types of processes, but for most basic workflows, the\n\nscipipe.Process\n\nwill be used, which is specialized for executing commandline applications. New\n\nProcess\n-es are typically created using the \nscipipe.NewProc(procName,\nshellPattern)\n command.\n\n\n\n\nSee \nGoDoc for Process\n\n\n\n\nTasks\n\n\nThe \"long-running\" processes mentioned above, will receive input files on its\nin-ports, and for each complete set of input files it receives, it will create\na new \ntask\n. Specifically, \nscipipe.Process\n will create\n\nscipipe.Task\n objects, and populate it with all data needed for one\nparticular shell command execution.  \nTask\n objects are executed via their\n\nExecute()\n\nmethod, or \nCustomExecute()\n, if custom Go code is supposed to be\nexecuted instead of a shell command.\n\n\nThe distinction between processes and tasks is important to understand, for\nexample when doing more advanced configuration of file naming strategies, since\nthe custom anonymous functions used to format paths are taking a \nTask\n as\ninput, even though these functions are saved on the process object.\n\n\nTo understand the difference between processes and tasks, it is helpful to\nremember that processes are long-running, and typically fixed during the course\nof a workflow, while tasks are transient objects, created temporarily as a\ncontainer for all data and code needed for each execution of a concrete shell\ncommand.\n\n\n\n\nSee \nGoDoc for Task\n\n\n\n\nPorts\n\n\nCentral to the way data dependencies are defined in SciPipe, is ports. Ports\nare fields on processes, which are connected to other ports via channels (see\nseparate section on this page).\n\n\nIn SciPipe, each port must have a unique name within its process (there can't\nbe an in-port and out-port named the same), and this name will be used in shell\ncommand patterns, when connecting dependencies / dataflow networks, and when\nconfiguring file naming strategies.\n\n\nIn \nProcess\n objects, in-ports are are accessed with\n\nmyProcess.In(\"my_port\")\n, and out-ports are similarly accessed with\n\nmyProcess.Out(\"my_other_port\")\n. They are of type\n\nInPort\n and\n\nOutPort\n respectively.\n\n\nSome pre-made components might have ports bound to custom field names though,\nsuch as \nmyFastaReader.InFastaFile\n, or \nmyZipComponent.OutZipFile\n.\n\n\nPort objects have some methods bound to them, most importantly the \nConnect()\n\nmethod, which takes another port, and connects to it, by stitching a channel\nbetween the ports.\n\n\nOn \nProcess\n objects, there is also a third port type, \nParamInPort\n (and the\naccompanying \nParamOutPort\n), which is used when it is needed to send a\nstream of parameter values (in string format) to be supplied to as arguments\nto shell commands.\n\n\n\n\nSee \nGoDoc for the InPort struct type\n\n\nSee \nGoDoc for the OutPort struct type\n\n\nSee \nGoDoc for the ParamInPort struct type\n\n\nSee \nGoDoc for the ParamOutPort struct type\n\n\n\n\nChannels\n\n\nPorts in SciPipe are connected via channels. Channels are \nplain Go channels\n\nand nothing more. Most of the time, one will not need to deal with the channels\ndirectly though, since the port objects (see separate section for ports) have\nall the logic to connect to other ports via channels, but it can be good to\nknow that they are there, in case you need to do something more advanced.\n\n\nWorkflow\n\n\nThe \nWorkflow\n\nis a special object in SciPipe, that just takes care of running a set of\ncomponents making up a workflow.\n\n\nThere is not much to say about the workflow component, other than that it is\ncreated with \nscipipe.NewWorkflow(workflowName, maxConcurrentTasks)\n, that all processes need to be added\nto it with \nwf.AddProc(proc)\n while the \"last\", or \"driving\" process needs to be specified with \nwf.SetDriver(driverProcess)\n, and that it should be run with\n\nwf.Run()\n. But this is already covered in the other examples and\ntutorials.\n\n\n\n\nSee \nGoDoc for Workflow\n\n\n\n\nShell command pattern\n\n\nThe \nProcess\n has the speciality that it can be configured using a special\nshell command pattern, supplied to the \nNewProc()\n\nfactory function. It is already explained in the section \"writing workflows\",\nbut in brief, it is a normal shell command, with placeholders for in-ports,\nout-ports and parameter ports, on the form \n{i:inportname}\n, \n{o:outportname}\n,\nand \n{p:paramportname}\n, respectively.\n\n\n\n\nSee \nGoDoc for NewProc()", 
            "title": "Basic Concepts"
        }, 
        {
            "location": "/basic_concepts/#basic-concepts", 
            "text": "In SciPipe, we are discussing a few concepts all the time, so to make sure we\nare on the same page, we will below go through the basic ones briefly.", 
            "title": "Basic concepts"
        }, 
        {
            "location": "/basic_concepts/#processes", 
            "text": "The probably most basic concept in SciPipe is the process.  A process is an\nasynchronously running component that is typically defined as a static,\n\"long-running\" part of the workflow, and the number of processes thus is\ntypically fixed for a workflow during its execution.  One can create customized types of processes, but for most basic workflows, the scipipe.Process \nwill be used, which is specialized for executing commandline applications. New Process -es are typically created using the  scipipe.NewProc(procName,\nshellPattern)  command.   See  GoDoc for Process", 
            "title": "Processes"
        }, 
        {
            "location": "/basic_concepts/#tasks", 
            "text": "The \"long-running\" processes mentioned above, will receive input files on its\nin-ports, and for each complete set of input files it receives, it will create\na new  task . Specifically,  scipipe.Process  will create scipipe.Task  objects, and populate it with all data needed for one\nparticular shell command execution.   Task  objects are executed via their Execute() \nmethod, or  CustomExecute() , if custom Go code is supposed to be\nexecuted instead of a shell command.  The distinction between processes and tasks is important to understand, for\nexample when doing more advanced configuration of file naming strategies, since\nthe custom anonymous functions used to format paths are taking a  Task  as\ninput, even though these functions are saved on the process object.  To understand the difference between processes and tasks, it is helpful to\nremember that processes are long-running, and typically fixed during the course\nof a workflow, while tasks are transient objects, created temporarily as a\ncontainer for all data and code needed for each execution of a concrete shell\ncommand.   See  GoDoc for Task", 
            "title": "Tasks"
        }, 
        {
            "location": "/basic_concepts/#ports", 
            "text": "Central to the way data dependencies are defined in SciPipe, is ports. Ports\nare fields on processes, which are connected to other ports via channels (see\nseparate section on this page).  In SciPipe, each port must have a unique name within its process (there can't\nbe an in-port and out-port named the same), and this name will be used in shell\ncommand patterns, when connecting dependencies / dataflow networks, and when\nconfiguring file naming strategies.  In  Process  objects, in-ports are are accessed with myProcess.In(\"my_port\") , and out-ports are similarly accessed with myProcess.Out(\"my_other_port\") . They are of type InPort  and OutPort  respectively.  Some pre-made components might have ports bound to custom field names though,\nsuch as  myFastaReader.InFastaFile , or  myZipComponent.OutZipFile .  Port objects have some methods bound to them, most importantly the  Connect() \nmethod, which takes another port, and connects to it, by stitching a channel\nbetween the ports.  On  Process  objects, there is also a third port type,  ParamInPort  (and the\naccompanying  ParamOutPort ), which is used when it is needed to send a\nstream of parameter values (in string format) to be supplied to as arguments\nto shell commands.   See  GoDoc for the InPort struct type  See  GoDoc for the OutPort struct type  See  GoDoc for the ParamInPort struct type  See  GoDoc for the ParamOutPort struct type", 
            "title": "Ports"
        }, 
        {
            "location": "/basic_concepts/#channels", 
            "text": "Ports in SciPipe are connected via channels. Channels are  plain Go channels \nand nothing more. Most of the time, one will not need to deal with the channels\ndirectly though, since the port objects (see separate section for ports) have\nall the logic to connect to other ports via channels, but it can be good to\nknow that they are there, in case you need to do something more advanced.", 
            "title": "Channels"
        }, 
        {
            "location": "/basic_concepts/#workflow", 
            "text": "The  Workflow \nis a special object in SciPipe, that just takes care of running a set of\ncomponents making up a workflow.  There is not much to say about the workflow component, other than that it is\ncreated with  scipipe.NewWorkflow(workflowName, maxConcurrentTasks) , that all processes need to be added\nto it with  wf.AddProc(proc)  while the \"last\", or \"driving\" process needs to be specified with  wf.SetDriver(driverProcess) , and that it should be run with wf.Run() . But this is already covered in the other examples and\ntutorials.   See  GoDoc for Workflow", 
            "title": "Workflow"
        }, 
        {
            "location": "/basic_concepts/#shell-command-pattern", 
            "text": "The  Process  has the speciality that it can be configured using a special\nshell command pattern, supplied to the  NewProc() \nfactory function. It is already explained in the section \"writing workflows\",\nbut in brief, it is a normal shell command, with placeholders for in-ports,\nout-ports and parameter ports, on the form  {i:inportname} ,  {o:outportname} ,\nand  {p:paramportname} , respectively.   See  GoDoc for NewProc()", 
            "title": "Shell command pattern"
        }, 
        {
            "location": "/writing_workflows/", 
            "text": "Writing Workflows - An Overview\n\n\nIn order to give an overview of how to write workflows in SciPipe, let's look\nat the example workflow used on the front page again:\n\n\npackage\n \nmain\n\n\n\nimport\n \n(\n\n    \n// Import SciPipe into the main namespace (generally frowned upon but could\n\n    \n// be argued to be reasonable for short-lived workflow scripts like this)\n\n    \n.\n \ngithub.com/scipipe/scipipe\n\n\n)\n\n\n\nfunc\n \nmain\n()\n \n{\n\n    \n// Init workflow with a name, and a number for max concurrent tasks, so we\n\n    \n// don\nt overbook our CPU (it is recommended to set it to the number of CPU\n\n    \n// cores of your computer)\n\n    \nwf\n \n:=\n \nNewWorkflow\n(\nhello_world\n,\n \n4\n)\n\n\n    \n// Initialize processes and set output file paths\n\n    \nhello\n \n:=\n \nwf\n.\nNewProc\n(\nhello\n,\n \necho \nHello \n \n {o:out}\n)\n\n    \nhello\n.\nSetPathStatic\n(\nout\n,\n \nhello.txt\n)\n\n\n    \nworld\n \n:=\n \nwf\n.\nNewProc\n(\nworld\n,\n \necho $(cat {i:in}) World \n {o:out}\n)\n\n    \nworld\n.\nSetPathReplace\n(\nin\n,\n \nout\n,\n \n.txt\n,\n \n_world.txt\n)\n\n\n    \n// Connect network\n\n    \nworld\n.\nIn\n(\nin\n).\nConnect\n(\nhello\n.\nOut\n(\nout\n))\n\n\n    \n// Run workflow\n\n    \nwf\n.\nRun\n()\n\n\n}\n\n\n\n\n\n\nNow let's go through the code example in some detail, to see what we are\nactually doing.\n\n\nInitializing processes\n\n\n// Initialize processes from shell command patterns\n\n\nhello\n \n:=\n \nsp\n.\nNewProc\n(\nhello\n,\n \necho \nHello \n \n {o:out}\n)\n\n\nworld\n \n:=\n \nsp\n.\nNewProc\n(\nworld\n,\n \necho $(cat {i:in}) World \n {o:out}\n)\n\n\n\n\n\n\nHere we are initializing two new processes, both of them based on a shell\ncommand, using the \nscipipe.NewProc()\n function, which takes a processname, and\na shell command pattern as input.\n\n\nThe shell command pattern\n\n\nThe shell command patterns, in this case \necho 'Hello ' \n {o:out}\n and\n\necho $(cat {i:in}) World \n {o:out}\n, are basically normal bash\nshell commands, with the addition of \"placeholders\" for input and output\nfilenames.\n\n\nInput filename placeholders are on the form \n{i:INPORT-NAME}\n and the output\nfilename placeholders are similarly of the form \n{o:OUTPORT-NAME}\n.  These\nplaceholders will be replaced with actual filenames when the command is\nexecuted later. The reason that it a port-name is used to name them, is that\nfiles will be queued on the channel connecting to the port, and for each set of\nfiles on in-ports, a command will be created and executed whereafter new files\nwill be pulled in on the out-ports, and so on.\n\n\nFormatting output file paths\n\n\nNow we need to provide some way for scipipe to figure out a suitable file name\nfor each of the files propagating through the \"network\" of processes.  This can\nbe done using special convenience methods on the processes, starting with\n\nSetPath...\n. There are a few variants, of which two of them are shown here.\n\n\n// Configure output file path formatters for the processes created above\n\n\nhello\n.\nSetPathStatic\n(\nout\n,\n \nhello.txt\n)\n\n\nworld\n.\nSetPathReplace\n(\nin\n,\n \nout\n,\n \n.txt\n,\n \n_world.txt\n)\n\n\n\n\n\n\nSetPathStatic\n just takes an out-port name and a static file name to use, and\nis suitable for processes which produce only one single output for a whole\nworkflow run.\n\n\nSetPathReplace\n is slightly more advanced: It takes an in-port name, and\nout-port name, and then a search-pattern in the input-filename, and a\nreplace-pattern for the output filename.  With the example above, our input\nfile named \nhello.txt\n will be converted into \nhello_world.txt\n by this path\npattern.\n\n\nEven more control over file formatting\n\n\nWe can actually get even more control over how file names are produced than\nthis, by manually supplying each process with an anonymous function that\nreturns file paths given a \nscipipe.Task\n object, which will be produced for\neach command execution.\n\n\nIn order to implement the same path patterns as above, using this method, we\nwould write like this:\n\n\n// Configure output file path formatters for the processes created above\n\n\nhello\n.\nSetPathCustom\n(\nout\n,\n \nfunc\n(\nt\n \n*\nsp\n.\nTask\n)\n \nstring\n \n{\n\n\nreturn\n \nhello.txt\n\n\n})\n\n\nworld\n.\nSetPathCustom\n(\nout\n,\n \nfunc\n(\nt\n \n*\nsp\n.\nTask\n)\n \nstring\n \n{\n\n\nreturn\n \nstrings\n.\nReplace\n(\nt\n.\nInPath\n(\nin\n),\n \n.txt\n,\n \n_world.txt\n,\n \n-\n1\n)\n\n\n})\n\n\n\n\n\n\nAs you can see, this is a much more complicated way to format paths, but it can\nbe useful for example when needing to incorporate parameter values into file\nnames.\n\n\nConnecting processes into a network\n\n\nFinally we need to define the data dependencies between our processes.  We do\nthis by connecting the outports of one process to the inport of another\nprocess, using the \nConnect\n method available on each port object. We also need\nto connect the final out-port of the pipeline to the workflow, so that the\nworkflow can pull on this port (technically pulling on a Go channel), in order\nto drive the workflow.\n\n\n// Connect network\n\n\nworld\n.\nIn\n(\nin\n).\nConnect\n(\nhelloWriter\n.\nOut\n(\nout\n))\n\n\n\n\n\n\nRunning the pipeline\n\n\nSo, the final part probably explains itself, but the workflow component is a\nrelatively simple one that will start each component in a separate go-routine.\n\n\nFor technical reasons, one final process has to be run in the main go-routine\n(that where the program's \nmain()\n function runs), but generally you don't\nneed to think about this, as the workflow will then use an in-built\n\nsink\n process for this\npurpose. If you for any reason need to customize which process to use as the\n\"driver\" process, instead of the in-built sink. see the \nSetDriver\n section\n\nin the docs.\n\n\nwf\n.\nRun\n()\n\n\n\n\n\n\nSummary\n\n\nSo with this, we have done everything needed to set up a file-based batch workflow system.\n\n\nIn summary, what we did, was to:\n\n\n\n\nInitialize processes\n\n\nFor each out-port, define a file-naming strategy\n\n\nSpecify dependencies by connecting out- and in-ports\n\n\nRun the pipeline\n\n\n\n\nThis actually turns out to be a fixed set of components that always need to be\nincluded when writing workflows, so it might be good to keep them in mind and\nmemorize these steps, if needed.\n\n\nFor more examples, see the \nexamples folder\n\nin the GitHub repository.", 
            "title": "Writing Workflows"
        }, 
        {
            "location": "/writing_workflows/#writing-workflows-an-overview", 
            "text": "In order to give an overview of how to write workflows in SciPipe, let's look\nat the example workflow used on the front page again:  package   main  import   ( \n     // Import SciPipe into the main namespace (generally frowned upon but could \n     // be argued to be reasonable for short-lived workflow scripts like this) \n     .   github.com/scipipe/scipipe  )  func   main ()   { \n     // Init workflow with a name, and a number for max concurrent tasks, so we \n     // don t overbook our CPU (it is recommended to set it to the number of CPU \n     // cores of your computer) \n     wf   :=   NewWorkflow ( hello_world ,   4 ) \n\n     // Initialize processes and set output file paths \n     hello   :=   wf . NewProc ( hello ,   echo  Hello     {o:out} ) \n     hello . SetPathStatic ( out ,   hello.txt ) \n\n     world   :=   wf . NewProc ( world ,   echo $(cat {i:in}) World   {o:out} ) \n     world . SetPathReplace ( in ,   out ,   .txt ,   _world.txt ) \n\n     // Connect network \n     world . In ( in ). Connect ( hello . Out ( out )) \n\n     // Run workflow \n     wf . Run ()  }   Now let's go through the code example in some detail, to see what we are\nactually doing.", 
            "title": "Writing Workflows - An Overview"
        }, 
        {
            "location": "/writing_workflows/#initializing-processes", 
            "text": "// Initialize processes from shell command patterns  hello   :=   sp . NewProc ( hello ,   echo  Hello     {o:out} )  world   :=   sp . NewProc ( world ,   echo $(cat {i:in}) World   {o:out} )   Here we are initializing two new processes, both of them based on a shell\ncommand, using the  scipipe.NewProc()  function, which takes a processname, and\na shell command pattern as input.", 
            "title": "Initializing processes"
        }, 
        {
            "location": "/writing_workflows/#the-shell-command-pattern", 
            "text": "The shell command patterns, in this case  echo 'Hello '   {o:out}  and echo $(cat {i:in}) World   {o:out} , are basically normal bash\nshell commands, with the addition of \"placeholders\" for input and output\nfilenames.  Input filename placeholders are on the form  {i:INPORT-NAME}  and the output\nfilename placeholders are similarly of the form  {o:OUTPORT-NAME} .  These\nplaceholders will be replaced with actual filenames when the command is\nexecuted later. The reason that it a port-name is used to name them, is that\nfiles will be queued on the channel connecting to the port, and for each set of\nfiles on in-ports, a command will be created and executed whereafter new files\nwill be pulled in on the out-ports, and so on.", 
            "title": "The shell command pattern"
        }, 
        {
            "location": "/writing_workflows/#formatting-output-file-paths", 
            "text": "Now we need to provide some way for scipipe to figure out a suitable file name\nfor each of the files propagating through the \"network\" of processes.  This can\nbe done using special convenience methods on the processes, starting with SetPath... . There are a few variants, of which two of them are shown here.  // Configure output file path formatters for the processes created above  hello . SetPathStatic ( out ,   hello.txt )  world . SetPathReplace ( in ,   out ,   .txt ,   _world.txt )   SetPathStatic  just takes an out-port name and a static file name to use, and\nis suitable for processes which produce only one single output for a whole\nworkflow run.  SetPathReplace  is slightly more advanced: It takes an in-port name, and\nout-port name, and then a search-pattern in the input-filename, and a\nreplace-pattern for the output filename.  With the example above, our input\nfile named  hello.txt  will be converted into  hello_world.txt  by this path\npattern.", 
            "title": "Formatting output file paths"
        }, 
        {
            "location": "/writing_workflows/#even-more-control-over-file-formatting", 
            "text": "We can actually get even more control over how file names are produced than\nthis, by manually supplying each process with an anonymous function that\nreturns file paths given a  scipipe.Task  object, which will be produced for\neach command execution.  In order to implement the same path patterns as above, using this method, we\nwould write like this:  // Configure output file path formatters for the processes created above  hello . SetPathCustom ( out ,   func ( t   * sp . Task )   string   {  return   hello.txt  })  world . SetPathCustom ( out ,   func ( t   * sp . Task )   string   {  return   strings . Replace ( t . InPath ( in ),   .txt ,   _world.txt ,   - 1 )  })   As you can see, this is a much more complicated way to format paths, but it can\nbe useful for example when needing to incorporate parameter values into file\nnames.", 
            "title": "Even more control over file formatting"
        }, 
        {
            "location": "/writing_workflows/#connecting-processes-into-a-network", 
            "text": "Finally we need to define the data dependencies between our processes.  We do\nthis by connecting the outports of one process to the inport of another\nprocess, using the  Connect  method available on each port object. We also need\nto connect the final out-port of the pipeline to the workflow, so that the\nworkflow can pull on this port (technically pulling on a Go channel), in order\nto drive the workflow.  // Connect network  world . In ( in ). Connect ( helloWriter . Out ( out ))", 
            "title": "Connecting processes into a network"
        }, 
        {
            "location": "/writing_workflows/#running-the-pipeline", 
            "text": "So, the final part probably explains itself, but the workflow component is a\nrelatively simple one that will start each component in a separate go-routine.  For technical reasons, one final process has to be run in the main go-routine\n(that where the program's  main()  function runs), but generally you don't\nneed to think about this, as the workflow will then use an in-built sink  process for this\npurpose. If you for any reason need to customize which process to use as the\n\"driver\" process, instead of the in-built sink. see the  SetDriver  section \nin the docs.  wf . Run ()", 
            "title": "Running the pipeline"
        }, 
        {
            "location": "/writing_workflows/#summary", 
            "text": "So with this, we have done everything needed to set up a file-based batch workflow system.  In summary, what we did, was to:   Initialize processes  For each out-port, define a file-naming strategy  Specify dependencies by connecting out- and in-ports  Run the pipeline   This actually turns out to be a fixed set of components that always need to be\nincluded when writing workflows, so it might be good to keep them in mind and\nmemorize these steps, if needed.  For more examples, see the  examples folder \nin the GitHub repository.", 
            "title": "Summary"
        }, 
        {
            "location": "/howtos/partial_workflows/", 
            "text": "SciPipe allows you to, on-demand, run only specific parts of a workflow. This\ncan be useful especially if you are doing modifications far up in an already\ndeveloped workflow, and want to run only up to a specific process, rather\nthan also running all downstream processes, which might be unnecessary heavy.\n\n\nThis can be done by using the\n\nworkflow.RunTo()\n\nmethod. By using this instead of the normal \nworkflow.Run()\n method, scipipe\nwill only run this process and all upstream processes of that one.\n\n\nSee also a\n\nsimple\nexample\n\nof where this is used.\n\n\nThere are a few other variants for specifying parts of workflows (and more\nmight be added in the future), such as specifying individual process names,\nor providing the process structs themselves. Please refer to the relevant\nparts of the\n\nworkflow\ndocumentation\n\nfor more about that.", 
            "title": "Running parts of workflows"
        }, 
        {
            "location": "/howtos/parameters/", 
            "text": "Parameters are arguments sent to commands as flags, or unnamed values, or\nsometimes just the occurance of flags.\n\n\nSciPipe does not provide one unified way to handle parameters, but instead\nsuggest a few different strategies, dependent on the usage pattern. This is\nbecause it turns out that there is a very large variety in how parameters can\nbe used with shell commands.\n\n\nTo keep SciPipe a small and flexible tool, we instead mostly leave the choice\nup to the workflow author to create a solution for each case, using a few helper\ntools provided with SciPipe, but also all the programming facilities built in to\nthe Go programming language.\n\n\nBelow we will discuss how to handle the most common uses for for parameters in\nSciPipe. For any more complicated use cases not covered here, please refer to\nthe \nmailing list\n or the\n\nchat\n, to ask your question.\n\n\nStatic parameters\n\n\nIf parameters in your shell command is always, the same, you can just add them\n\"manually\" to the shell command pattern used to create your process.\n\n\nFor example, if you always want to write the string \"hello\" to output files,\nyou could create your processes with this string added manually:\n\n\nhelloWriter\n \n:=\n \nscipipe\n.\nNewProc\n(\nhelloWriter\n,\n \necho hello \n {o:outfile}\n)\n\n\n\n\n\n\nIf you have a lot of various parameters, and want a little more flexible way\nto add their values to a command, you can use the \nExpandParams\n\nhelper function, to add the parameter values:\n\n\n// Create a shell command pattern\n\n\ncmd\n \n:=\n \necho {p:p1} {p:p2} {p:p3} \n {o:outfile}\n\n\n\n// Create a map from parameter names (p1, p2, p3) to parameter values\n\n\n// (one, two, three)\n\n\nparamVals\n \n:=\n \nmap\n[\nstring\n]\nstring\n{\np1\n:\n \none\n,\n \np2\n:\n \ntwo\n,\n \np3\n:\n \nthree\n}\n\n\n\n// Expand the parameters into the shell command pattern\n\n\ncmd\n \n=\n \nscipipe\n.\nExpandParams\n(\ncmd\n,\n \nparamVals\n)\n\n\n\n// Create a new process with the resulting command\n\n\nwrite123\n \n:=\n \nscipipe\n.\nNewProc\n(\nwrite123\n,\n \ncmd\n)\n\n\n\n\n\n\nSee also\n\n\n\n\nStatic parameters example\n\n\n\n\nReceive parameters dynamically\n\n\nReceiving parameters dynamically is a much more technically demandning solution\nthan using static parameters.\n\n\nThe idea is that by using placeholders for parameter values in a command, each\nparameter for a particular process, will automatically get a channel of type\nstring, on which it can receive values. When the process is ready to execute\nanother shell command, it receives one item on each parameter ports, in\naddition to receiving one file on each (file-)in-port, and merges the values\ninto the shell command, before executing it.\n\n\nAn example of this would be a little too complicated to cover briefly on this\npage, so please instead see the \ndynamic parameters example\n.\nIn the \nRun method of the Combinatorics task\n\nyou will find the code used to send values (all combinations of values in three\narrays of lenght 3, in this case).\n\n\nSee also\n\n\n\n\nDynamic parameters example\n\n\n\n\nHandle boolean flags\n\n\nTopic coming soon. Please add it as a support request in the \nissue tracker\n\nif you need this information fast, and we can prioritize writing it asap.\n\n\nHandling parameters in re-usable components\n\n\nTopic coming soon. Please add it as a support request in the \nissue tracker\n\nif you need this information fast, and we can prioritize writing it asap.\n\n\nRelevant examples\n\n\n\n\nStatic parameters\n\n\nReceive parameters dynamically", 
            "title": "Using parameters in commands and file names"
        }, 
        {
            "location": "/howtos/parameters/#static-parameters", 
            "text": "If parameters in your shell command is always, the same, you can just add them\n\"manually\" to the shell command pattern used to create your process.  For example, if you always want to write the string \"hello\" to output files,\nyou could create your processes with this string added manually:  helloWriter   :=   scipipe . NewProc ( helloWriter ,   echo hello   {o:outfile} )   If you have a lot of various parameters, and want a little more flexible way\nto add their values to a command, you can use the  ExpandParams \nhelper function, to add the parameter values:  // Create a shell command pattern  cmd   :=   echo {p:p1} {p:p2} {p:p3}   {o:outfile}  // Create a map from parameter names (p1, p2, p3) to parameter values  // (one, two, three)  paramVals   :=   map [ string ] string { p1 :   one ,   p2 :   two ,   p3 :   three }  // Expand the parameters into the shell command pattern  cmd   =   scipipe . ExpandParams ( cmd ,   paramVals )  // Create a new process with the resulting command  write123   :=   scipipe . NewProc ( write123 ,   cmd )", 
            "title": "Static parameters"
        }, 
        {
            "location": "/howtos/parameters/#see-also", 
            "text": "Static parameters example", 
            "title": "See also"
        }, 
        {
            "location": "/howtos/parameters/#receive-parameters-dynamically", 
            "text": "Receiving parameters dynamically is a much more technically demandning solution\nthan using static parameters.  The idea is that by using placeholders for parameter values in a command, each\nparameter for a particular process, will automatically get a channel of type\nstring, on which it can receive values. When the process is ready to execute\nanother shell command, it receives one item on each parameter ports, in\naddition to receiving one file on each (file-)in-port, and merges the values\ninto the shell command, before executing it.  An example of this would be a little too complicated to cover briefly on this\npage, so please instead see the  dynamic parameters example .\nIn the  Run method of the Combinatorics task \nyou will find the code used to send values (all combinations of values in three\narrays of lenght 3, in this case).", 
            "title": "Receive parameters dynamically"
        }, 
        {
            "location": "/howtos/parameters/#see-also_1", 
            "text": "Dynamic parameters example", 
            "title": "See also"
        }, 
        {
            "location": "/howtos/parameters/#handle-boolean-flags", 
            "text": "Topic coming soon. Please add it as a support request in the  issue tracker \nif you need this information fast, and we can prioritize writing it asap.", 
            "title": "Handle boolean flags"
        }, 
        {
            "location": "/howtos/parameters/#handling-parameters-in-re-usable-components", 
            "text": "Topic coming soon. Please add it as a support request in the  issue tracker \nif you need this information fast, and we can prioritize writing it asap.", 
            "title": "Handling parameters in re-usable components"
        }, 
        {
            "location": "/howtos/parameters/#relevant-examples", 
            "text": "Static parameters  Receive parameters dynamically", 
            "title": "Relevant examples"
        }, 
        {
            "location": "/howtos/golang_components/", 
            "text": "Beware: Technical topic, best suited for power-users!\n\n\nIf you want to write a component with Go code, but would like to have it work\nseamlessly with other workflow processes in SciPipe, without reimplementing the\nwhole \nProcess\n\nfunctionality yourself, there is a way to do it: By using the \nCustomExecute\n\nfield of Process.\n\n\nIn short, it can be done like this:\n\n\n// Initiate task from a \nshell like\n pattern, though here we\n\n\n// just specify the out-port, and nothing else. We have to\n\n\n// specify the out-port (and any other ports we plan to use later),\n\n\n// so that they are correctly initialized.\n\n\nfooWriter\n \n:=\n \nsci\n.\nNewProc\n(\nfooer\n,\n \n{o:foo}\n)\n\n\n\n// Set the output formatter to a static string\n\n\nfooWriter\n.\nSetPathStatic\n(\nfoo\n,\n \nfoo.txt\n)\n\n\n\n// Create the custom execute function, with pure Go code and\n\n\n// add it to the CustomExecute field of the fooWriter process\n\n\nfooWriter\n.\nCustomExecute\n \n=\n \nfunc\n(\ntask\n \n*\nsci\n.\nTask\n)\n \n{\n\n    \ntask\n.\nOutIP\n(\nfoo\n).\nWrite\n([]\nbyte\n(\nfoo\\n\n))\n\n\n}\n\n\n\n\n\n\nFor a more detailed example, see \nthis example\n\n(Have a look at the \nNewFooer()\n\nand \nNewFoo2Barer()\n\nfactory functions in particular!)", 
            "title": "Creating components in Go"
        }, 
        {
            "location": "/howtos/reusable_components/", 
            "text": "What are re-usable components\n\n\nWith re-usable components, we mean components that can be stored in a Go\npackage and imported and used later.\n\n\nIn order for components in such a library to be easy to use, the ports need\nto be static methods bound to the process struct, rather than just stored by\na string ID in a generic port map, like the \nIn()\n and \nOut()\n methods on\n\nProcess\n processes. This is so that the methods can show up in the\nauto-completion / intellisense function in code editors removing the need to\nlook up the name of the ports manually in the library code all the time.\n\n\nHow to create re-usable components in SciPipe\n\n\nProcess processes created with the \nscipipe.NewProc()\n command, can be turned\ninto such \"re-usable\" component by using a wrapping strategy, that is\ndemonstrated in an \nexample on GitHub\n.\n\n\nThe idea is to create a new struct type for the re-usable component, and\nthen, in the factory method for the process, create an \"inner\" process of\ntype Process, using \nNewProc()\n as in the normal case, embedding that in the\nouter struct and then adding statically defined accessor methods for each of\nthe ports in the inner process, with a similar name. So, if the inner process\nhas an outport named \"foo\", you would define an accessor method named\n\nmyproc.OutFoo()\n that returns this port from the inner process.\n\n\nLet's look at a code example of how this works, by creating a process that just\nwrites \"hi\" to a file:\n\n\ntype\n \nHiWriter\n \nstruct\n \n{\n\n    \n// Embedd a Process struct\n\n    \n*\nsci\n.\nProcess\n\n\n}\n\n\n\nfunc\n \nNewHiWriter\n()\n \n*\nHiWriter\n \n{\n\n    \n// Initialize a normal \nProcess\n to use as an \ninner\n process\n\n    \ninnerHiWriter\n \n:=\n \nsci\n.\nNewProc\n(\nhiwriter\n,\n \necho hi \n {o:hifile}\n)\n\n    \ninnerHiWriter\n.\nSetPathStatic\n(\nhifile\n,\n \nhi.txt\n)\n\n\n    \n// Create a new HiWriter process with the inner process embedded into it\n\n    \nreturn\n \nHiWriter\n{\ninnerHiWriter\n}\n\n\n}\n\n\n\n// OutHiFile provides a static version of the \nhifile\n port in the inner\n\n\n// (embedded) process\n\n\nfunc\n \n(\np\n \n*\nHiWriter\n)\n \nOutHiFile\n()\n \n*\nsci\n.\nOutPort\n \n{\n\n    \n// Return the inner process\n port named \nhifile\n\n    \nreturn\n \np\n.\nOut\n(\nhifile\n)\n\n\n}\n\n\n\n\n\n\nSee also\n\n\n\n\nA full, working, workflow example using this trategy", 
            "title": "Creating re-usable components"
        }, 
        {
            "location": "/howtos/reusable_components/#what-are-re-usable-components", 
            "text": "With re-usable components, we mean components that can be stored in a Go\npackage and imported and used later.  In order for components in such a library to be easy to use, the ports need\nto be static methods bound to the process struct, rather than just stored by\na string ID in a generic port map, like the  In()  and  Out()  methods on Process  processes. This is so that the methods can show up in the\nauto-completion / intellisense function in code editors removing the need to\nlook up the name of the ports manually in the library code all the time.", 
            "title": "What are re-usable components"
        }, 
        {
            "location": "/howtos/reusable_components/#how-to-create-re-usable-components-in-scipipe", 
            "text": "Process processes created with the  scipipe.NewProc()  command, can be turned\ninto such \"re-usable\" component by using a wrapping strategy, that is\ndemonstrated in an  example on GitHub .  The idea is to create a new struct type for the re-usable component, and\nthen, in the factory method for the process, create an \"inner\" process of\ntype Process, using  NewProc()  as in the normal case, embedding that in the\nouter struct and then adding statically defined accessor methods for each of\nthe ports in the inner process, with a similar name. So, if the inner process\nhas an outport named \"foo\", you would define an accessor method named myproc.OutFoo()  that returns this port from the inner process.  Let's look at a code example of how this works, by creating a process that just\nwrites \"hi\" to a file:  type   HiWriter   struct   { \n     // Embedd a Process struct \n     * sci . Process  }  func   NewHiWriter ()   * HiWriter   { \n     // Initialize a normal  Process  to use as an  inner  process \n     innerHiWriter   :=   sci . NewProc ( hiwriter ,   echo hi   {o:hifile} ) \n     innerHiWriter . SetPathStatic ( hifile ,   hi.txt ) \n\n     // Create a new HiWriter process with the inner process embedded into it \n     return   HiWriter { innerHiWriter }  }  // OutHiFile provides a static version of the  hifile  port in the inner  // (embedded) process  func   ( p   * HiWriter )   OutHiFile ()   * sci . OutPort   { \n     // Return the inner process  port named  hifile \n     return   p . Out ( hifile )  }", 
            "title": "How to create re-usable components in SciPipe"
        }, 
        {
            "location": "/howtos/reusable_components/#see-also", 
            "text": "A full, working, workflow example using this trategy", 
            "title": "See also"
        }, 
        {
            "location": "/howtos/globbing/", 
            "text": "This is a feature that is \nbeing worked on currently\n.\nPlease see \nthe issue\n to follow the status of it.", 
            "title": "Globbing files"
        }, 
        {
            "location": "/howtos/splitting/", 
            "text": "At the time of writing this, the recommended way to split files is to use the\n\nFileSplitter\n component\n\nin the \ncomponent library\n shipped with SciPipe.\n\n\nAlso see the \npage about Scatter/Gather\n.", 
            "title": "Splitting files"
        }, 
        {
            "location": "/howtos/joining/", 
            "text": "See the \nConcatenator component\n.\n\n\nAlso see the \npage about Scatter/Gather\n.", 
            "title": "Joining multiple files"
        }, 
        {
            "location": "/howtos/streaming/", 
            "text": "SciPipe can stream the output via UNIX \nnamed pipes (or \"FIFO files\")\n.\n\n\nStreaming can be turned on, on out-ports when creating processes with\n\nNewProc()\n, by using \n{os:outport_name}\n as placeholder, instead of the\nnormal \n{o:outport_name}\n (note the addisional \"s\")\n\n\nYou can see how this is used in \nthis example on GitHub\n.\n\n\nNote that when streaming, you will not get an output file for the output in\nquestion.\n\n\nNote also that you still have to provide a path formatting strategy (via some\nof the \nProcess.SetPath...()\n functions, or by manually adding one to\n\nProcess.PathFormatters\n. This is because a uniqe file name is needed in\norder to create any audit files, as well as to give a unique name for the named\npipe.\n\n\nSee also\n\n\n\n\nStreaming example on GitHub\n.", 
            "title": "Enable streaming between components"
        }, 
        {
            "location": "/howtos/streaming/#see-also", 
            "text": "Streaming example on GitHub .", 
            "title": "See also"
        }, 
        {
            "location": "/howtos/subworkflows/", 
            "text": "It is possible in SciPipe to wrap a whole workflow in a process, so that it can be used\nas any other process, in larger workflows.\n\n\nThis is demonstrated in \nthis example on GitHub\n.", 
            "title": "Creating sub-workflows"
        }, 
        {
            "location": "/howtos/scatter_gather/", 
            "text": "There is \nwork going on to add better scatter/gather support in SciPipe (Issue #20)\n.\nIn the meanwhile, have a look at \nthis example on GitHub\n\nwhich demonstrates one way of doing a scatter gather operation, using the Splitter component (\nsee line 24\n)\nand two concatenator components (\nsee lines 38-39\n)\nto do the scatter, and gather, operations respectively.", 
            "title": "Implement scatter/gather workflows"
        }, 
        {
            "location": "/howtos/hpc/", 
            "text": "This is \nbeing worked on right now (issue #38)\n.\n\n\nWhat you can do right now, is to use the \nPrepend\n field in processes, to add a\n\nsalloc\n command string (in the case of\nSLURM), or any analogous blocking command to other resource managers.\n\n\nSo, something like this (See on the third line how the salloc-line is added to the process):\n\n\nwf\n \n:=\n \nscipipe\n.\nNewWorkflow\n(\nHello_World_Workflow\n,\n \n4\n)\n\n\nmyProc\n \n:=\n \nwf\n.\nNewProc\n(\nhello_world\n,\n \necho Hello World; sleep 10;\n)\n\n\nmyProc\n.\nPrepend\n \n=\n \nsalloc -A projectABC123 -p core -t 1:00 -J HelloWorld\n\n\n\n\n\n\n(Beware: This is not a full code example, and won't compile without some more boilerplate, which you can find in the introductory examples)\n\n\nYou can find the updated GoDoc for the process struct \nhere\n.", 
            "title": "Interacting with an HPC resource manager"
        }, 
        {
            "location": "/howtos/constrain_resource_usage/", 
            "text": "It is important to carefully manage how much resources (CPU and memory) your\nworkflows are using, so that you don't overbook you compute node(s).\n\n\nIn SciPipe you can do that using two settings:\n\n\n\n\nMax concurrent tasks, which is set on the workflow level, when initiating a new workflow.\n\n\nCores per tasks, that can be set on processes after they are initialized.\n\n\n\n\nMax concurrent tasks is a required setting when initializing workflows, while\ncores per task can be left to the default, which is 1 core per task.\n\n\nYou might want to change this number if for example you have a software that\nuses more memory than the available memory on your computer divided by the max\nconcurrent tasks number you have set.\n\n\nFor example, if you have 8GB of free memory, and have set max concurrent tasks\non your workflow to 4, but you have a process whose commandline application\nuses not 2GB of memory, but 4GB, then you might want to set cores per tasks for\nthat process to 2, so that it gets the double amount of memory.\n\n\nIn practice, you set cores per task by setting the field \nCoresPerTask\n on the process struct, after it is initiated. \n\n\nExample\n\n\nfoo\n \n:=\n \nscipipe\n.\nNewProc\n(\nfoo\n,\n \necho foo \n {o:foofile}\n)\n\n\nfoo\n.\nCoresPerTask\n \n=\n \n2", 
            "title": "Constrain resource usage"
        }, 
        {
            "location": "/howtos/constrain_resource_usage/#example", 
            "text": "foo   :=   scipipe . NewProc ( foo ,   echo foo   {o:foofile} )  foo . CoresPerTask   =   2", 
            "title": "Example"
        }, 
        {
            "location": "/examples/", 
            "text": "Examples\n\n\nWe plan to go through a few examples in more depth here soon, but in the\nmeanwhile, see the \nexamples folder\n\nin the main scipipe repository, for a bunch of examples spanning much of the\nfunctionality in SciPipe.", 
            "title": "Examples"
        }, 
        {
            "location": "/examples/#examples", 
            "text": "We plan to go through a few examples in more depth here soon, but in the\nmeanwhile, see the  examples folder \nin the main scipipe repository, for a bunch of examples spanning much of the\nfunctionality in SciPipe.", 
            "title": "Examples"
        }, 
        {
            "location": "/other_resources/", 
            "text": "Publications mentioning SciPipe\n\n\n\n\nSee \na poster on SciPipe\n, presented at the \ne-Science Academy in Lund, on Oct 12-13 2016\n.\n\n\nSee \nslides from a recent presentation of SciPipe for use in a Bioinformatics setting\n.\n\n\nThe architecture of SciPipe is based on an \nflow-based\n  programming\n like\n  pattern in pure Go presented in\n  \nthis\n and\n  \nthis\n\n  blog posts on Gopher Academy.", 
            "title": "Other Resources"
        }, 
        {
            "location": "/other_resources/#publications-mentioning-scipipe", 
            "text": "See  a poster on SciPipe , presented at the  e-Science Academy in Lund, on Oct 12-13 2016 .  See  slides from a recent presentation of SciPipe for use in a Bioinformatics setting .  The architecture of SciPipe is based on an  flow-based\n  programming  like\n  pattern in pure Go presented in\n   this  and\n   this \n  blog posts on Gopher Academy.", 
            "title": "Publications mentioning SciPipe"
        }, 
        {
            "location": "/acknowledgements/", 
            "text": "Acknowledgements\n\n\n\n\nSciPipe is very heavily dependent on the proven principles form \nFlow-Based\n  Programming (FBP)\n, as invented by \nJohn Paul Morrison\n.\n  From Flow-based programming, SciPipe uses the ideas of separate network\n  (workflow dependency graph) definition, named in- and out-ports,\n  sub-networks/sub-workflows and bounded buffers (already available in Go's\n  channels) to make writing workflows as easy as possible.\n\n\nThis library is has been much influenced/inspired also by the\n  \nGoFlow\n library by \nVladimir Sibirov\n.\n\n\nThanks to \nEgon Elbre\n for helpful input on the\n  design of the internals of the pipeline, and processes, which greatly\n  simplified the implementation.\n\n\nThis work is financed by faculty grants and other financing for the \nPharmaceutical Bioinformatics group\n of \nDept. of\n  Pharmaceutical Biosciences\n at \nUppsala University\n, and by \nSwedish Research Council\n\n  through the Swedish \nNational Bioinformatics Infrastructure Sweden\n.\n\n\nSupervisor for the project is \nOla Spjuth\n.", 
            "title": "Acknowledgements"
        }, 
        {
            "location": "/acknowledgements/#acknowledgements", 
            "text": "SciPipe is very heavily dependent on the proven principles form  Flow-Based\n  Programming (FBP) , as invented by  John Paul Morrison .\n  From Flow-based programming, SciPipe uses the ideas of separate network\n  (workflow dependency graph) definition, named in- and out-ports,\n  sub-networks/sub-workflows and bounded buffers (already available in Go's\n  channels) to make writing workflows as easy as possible.  This library is has been much influenced/inspired also by the\n   GoFlow  library by  Vladimir Sibirov .  Thanks to  Egon Elbre  for helpful input on the\n  design of the internals of the pipeline, and processes, which greatly\n  simplified the implementation.  This work is financed by faculty grants and other financing for the  Pharmaceutical Bioinformatics group  of  Dept. of\n  Pharmaceutical Biosciences  at  Uppsala University , and by  Swedish Research Council \n  through the Swedish  National Bioinformatics Infrastructure Sweden .  Supervisor for the project is  Ola Spjuth .", 
            "title": "Acknowledgements"
        }, 
        {
            "location": "/related_tools/", 
            "text": "Related tools\n\n\nFind below a few tools that are more or less similar to SciPipe that are worth worth checking out before\ndeciding on what tool fits you best (in approximate order of similarity to SciPipe):\n\n\n\n\nNextFlow\n\n\nLuigi\n/\nSciLuigi\n\n\nBPipe\n\n\nSnakeMake\n\n\nCuneiform", 
            "title": "Related Tools"
        }, 
        {
            "location": "/related_tools/#related-tools", 
            "text": "Find below a few tools that are more or less similar to SciPipe that are worth worth checking out before\ndeciding on what tool fits you best (in approximate order of similarity to SciPipe):   NextFlow  Luigi / SciLuigi  BPipe  SnakeMake  Cuneiform", 
            "title": "Related tools"
        }
    ]
}