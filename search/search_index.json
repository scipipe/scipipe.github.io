{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Robust, flexible and resource-efficient pipelines using Go and the commandline Why SciPipe? Intuitive: SciPipe works by flowing data through a network of channels and processes Flexible: Wrapped command-line programs can be combined with processes in Go Convenient: Full control over how your files are named Efficient: Workflows are compiled to binary code that run fast Parallel: Pipeline paralellism between processes as well as task parallelism for multiple inputs, making efficient use of multiple CPU cores Supports streaming: Stream data between programs to avoid wasting disk space Easy to debug: Use available Go debugging tools or just println() Portable: Distribute workflows as Go code or as self-contained executable files Project links: GitHub repo | Issue Tracker | Chat Project updates Jan 2020: New screencast: \"Hello World\" scientific workflow in SciPipe May 2019: The SciPipe paper published open access in GigaScience: SciPipe: A workflow library for agile development of complex and dynamic bioinformatics pipelines Nov 2018: Scientific study using SciPipe: Predicting off-target binding profiles with confidence using Conformal Prediction Slides: Presentation on SciPipe and more at Go Stockholm Conference Blog post: Provenance reports in Scientific Workflows - going into details about how SciPipe is addressing provenance. Blog post: First production workflow run with SciPipe Introduction SciPipe is a library for writing Scientific Workflows , sometimes also called \"pipelines\", in the Go programming language . When you need to run many commandline programs that depend on each other in complex ways, SciPipe helps by making the process of running these programs flexible, robust and reproducible. SciPipe also lets you restart an interrupted run without over-writing already produced output and produces an audit report of what was run, among many other things. SciPipe is built on the proven principles of Flow-Based Programming (FBP) to achieve maximum flexibility, productivity and agility when designing workflows. Compared to plain dataflow, FBP provides the benefits that processes are fully self-contained, so that a library of re-usable components can be created, and plugged into new workflows ad-hoc. Similar to other FBP systems, SciPipe workflows can be likened to a network of assembly lines in a factory, where items (files) are flowing through a network of conveyor belts, stopping at different independently running stations (processes) for processing, as depicted in the picture above. SciPipe was initially created for problems in bioinformatics and cheminformatics, but works equally well for any problem involving pipelines of commandline applications. Project status: SciPipe pretty stable now, and only very minor API changes might still occur. We have successfully used SciPipe in a handful of both real and experimental projects, and it has had occasional use outside the research group as well. Known limitations There are still a number of missing good-to-have features for workflow design. See the issue tracker for details. There is not (yet) support for the Common Workflow Language . Installing For full installation instructions, see the intallation page . For quick getting started steps, you can do: Download and install Go Run the following command, to install the scipipe Go library (don't miss the trailing dots!), and create a Go module for your script: go install github.com/scipipe/scipipe/...@latest go mod init myfirstworkflow-module Hello World example Let's look at an example workflow to get a feel for what writing workflows in SciPipe looks like: package main import ( // Import SciPipe, aliased to sp sp \"github.com/scipipe/scipipe\" ) func main () { // Init workflow and max concurrent tasks wf := sp . NewWorkflow ( \"hello_world\" , 4 ) // Initialize processes, and file extensions hello := wf . NewProc ( \"hello\" , \"echo 'Hello ' > {o:out|.txt}\" ) world := wf . NewProc ( \"world\" , \"echo $(cat {i:in}) World > {o:out|.txt}\" ) // Define data flow world . In ( \"in\" ). From ( hello . Out ( \"out\" )) // Run workflow wf . Run () } To create a file with a similar simple example, you can run: scipipe new hello_world.go Running the example Let's put the code in a file named hello_world.go and run it. First you need to make sure that the dependencies (SciPipe in this case) is installed in your local Go module. This you can do with: go mod tidy Then you can go ahead and run the workflow: $ go run hello_world.go AUDIT 2018 /07/17 21 :42:26 | workflow:hello_world | Starting workflow ( Writing log to log/scipipe-20180717-214226-hello_world.log ) AUDIT 2018 /07/17 21 :42:26 | hello | Executing: echo 'Hello ' > hello.out.txt AUDIT 2018 /07/17 21 :42:26 | hello | Finished: echo 'Hello ' > hello.out.txt AUDIT 2018 /07/17 21 :42:26 | world | Executing: echo $( cat ../hello.out.txt ) World > hello.out.txt.world.out.txt AUDIT 2018 /07/17 21 :42:26 | world | Finished: echo $( cat ../hello.out.txt ) World > hello.out.txt.world.out.txt AUDIT 2018 /07/17 21 :42:26 | workflow:hello_world | Finished workflow ( Log written to log/scipipe-20180717-214226-hello_world.log ) Let's check what file SciPipe has generated: $ ls -1 hello* hello.out.txt hello.out.txt.audit.json hello.out.txt.world.out.txt hello.out.txt.world.out.txt.audit.json As you can see, it has created a file hello.out.txt , and hello.out.world.out.txt , and an accompanying .audit.json for each of these files. Now, let's check the output of the final resulting file: $ cat hello.out.txt.world.out.txt Hello World Now we can rejoice that it contains the text \"Hello World\", exactly as a proper Hello World example should :) Now, these were a little long and cumbersome filenames, weren't they? SciPipe gives you very good control over how to name your files, if you don't want to rely on the automatic file naming. For example, we could set the first filename to a static one, and then use the first name as a basis for the file name for the second process, like so: package main import ( // Import the SciPipe package, aliased to 'sp' sp \"github.com/scipipe/scipipe\" ) func main () { // Init workflow with a name, and max concurrent tasks wf := sp . NewWorkflow ( \"hello_world\" , 4 ) // Initialize processes and set output file paths hello := wf . NewProc ( \"hello\" , \"echo 'Hello ' > {o:out}\" ) hello . SetOut ( \"out\" , \"hello.txt\" ) world := wf . NewProc ( \"world\" , \"echo $(cat {i:in}) World >> {o:out}\" ) world . SetOut ( \"out\" , \"{i:in|%.txt}_world.txt\" ) // Connect network world . In ( \"in\" ). From ( hello . Out ( \"out\" )) // Run workflow wf . Run () } In the {i:in... part, we are re-using the file path from the file received on the in-port named 'in', and then running a Bash-style trim-from-end command on it to remove the .txt extension. Now, if we run this, the file names get a little cleaner: $ ls -1 hello* hello.txt hello.txt.audit.json hello_world.go hello_world.txt hello_world.txt.audit.json The audit logs Finally, we could have a look at one of those audit file created: $ cat hello_world.txt.audit.json { \"ID\" : \"99i5vxhtd41pmaewc8pr\" , \"ProcessName\" : \"world\" , \"Command\" : \"echo $( cat hello.txt ) World \\u003e\\u003e hello_world.txt.tmp/hello_world.txt\" , \"Params\" : {} , \"Tags\" : {} , \"StartTime\" : \"2018-06-15T19:10:37.955602979+02:00\" , \"FinishTime\" : \"2018-06-15T19:10:37.959410102+02:00\" , \"ExecTimeNS\" : 3000000 , \"Upstream\" : { \"hello.txt\" : { \"ID\" : \"w4oeiii9h5j7sckq7aqq\" , \"ProcessName\" : \"hello\" , \"Command\" : \"echo 'Hello ' \\u003e hello.txt.tmp/hello.txt\" , \"Params\" : {} , \"Tags\" : {} , \"StartTime\" : \"2018-06-15T19:10:37.950032676+02:00\" , \"FinishTime\" : \"2018-06-15T19:10:37.95468214+02:00\" , \"ExecTimeNS\" : 4000000 , \"Upstream\" : {} } } Each such audit-file contains a hierarchic JSON-representation of the full workflow path that was executed in order to produce this file. On the first level is the command that directly produced the corresponding file, and then, indexed by their filenames, under \"Upstream\", there is a similar chunk describing how all of its input files were generated. This process will be repeated in a recursive way for large workflows, so that, for each file generated by the workflow, there is always a full, hierarchic, history of all the commands run - with their associated metadata - to produce that file. You can find many more examples in the examples folder in the GitHub repo. For more information about how to write workflows using SciPipe, use the menu to the left, to browse the various topics! Citing SciPipe If you use SciPipe in academic or scholarly work, please cite the following paper as source: Lampa S, Dahl\u00f6 M, Alvarsson J, Spjuth O. SciPipe: A workflow library for agile development of complex and dynamic bioinformatics pipelines Gigascience . 8, 5 (2019). DOI: 10.1093/gigascience/giz044","title":"Introduction"},{"location":"#why-scipipe","text":"Intuitive: SciPipe works by flowing data through a network of channels and processes Flexible: Wrapped command-line programs can be combined with processes in Go Convenient: Full control over how your files are named Efficient: Workflows are compiled to binary code that run fast Parallel: Pipeline paralellism between processes as well as task parallelism for multiple inputs, making efficient use of multiple CPU cores Supports streaming: Stream data between programs to avoid wasting disk space Easy to debug: Use available Go debugging tools or just println() Portable: Distribute workflows as Go code or as self-contained executable files Project links: GitHub repo | Issue Tracker | Chat","title":"Why SciPipe?"},{"location":"#introduction","text":"SciPipe is a library for writing Scientific Workflows , sometimes also called \"pipelines\", in the Go programming language . When you need to run many commandline programs that depend on each other in complex ways, SciPipe helps by making the process of running these programs flexible, robust and reproducible. SciPipe also lets you restart an interrupted run without over-writing already produced output and produces an audit report of what was run, among many other things. SciPipe is built on the proven principles of Flow-Based Programming (FBP) to achieve maximum flexibility, productivity and agility when designing workflows. Compared to plain dataflow, FBP provides the benefits that processes are fully self-contained, so that a library of re-usable components can be created, and plugged into new workflows ad-hoc. Similar to other FBP systems, SciPipe workflows can be likened to a network of assembly lines in a factory, where items (files) are flowing through a network of conveyor belts, stopping at different independently running stations (processes) for processing, as depicted in the picture above. SciPipe was initially created for problems in bioinformatics and cheminformatics, but works equally well for any problem involving pipelines of commandline applications. Project status: SciPipe pretty stable now, and only very minor API changes might still occur. We have successfully used SciPipe in a handful of both real and experimental projects, and it has had occasional use outside the research group as well.","title":"Introduction"},{"location":"#known-limitations","text":"There are still a number of missing good-to-have features for workflow design. See the issue tracker for details. There is not (yet) support for the Common Workflow Language .","title":"Known limitations"},{"location":"#installing","text":"For full installation instructions, see the intallation page . For quick getting started steps, you can do: Download and install Go Run the following command, to install the scipipe Go library (don't miss the trailing dots!), and create a Go module for your script: go install github.com/scipipe/scipipe/...@latest go mod init myfirstworkflow-module","title":"Installing"},{"location":"#hello-world-example","text":"Let's look at an example workflow to get a feel for what writing workflows in SciPipe looks like: package main import ( // Import SciPipe, aliased to sp sp \"github.com/scipipe/scipipe\" ) func main () { // Init workflow and max concurrent tasks wf := sp . NewWorkflow ( \"hello_world\" , 4 ) // Initialize processes, and file extensions hello := wf . NewProc ( \"hello\" , \"echo 'Hello ' > {o:out|.txt}\" ) world := wf . NewProc ( \"world\" , \"echo $(cat {i:in}) World > {o:out|.txt}\" ) // Define data flow world . In ( \"in\" ). From ( hello . Out ( \"out\" )) // Run workflow wf . Run () } To create a file with a similar simple example, you can run: scipipe new hello_world.go","title":"Hello World example"},{"location":"#running-the-example","text":"Let's put the code in a file named hello_world.go and run it. First you need to make sure that the dependencies (SciPipe in this case) is installed in your local Go module. This you can do with: go mod tidy Then you can go ahead and run the workflow: $ go run hello_world.go AUDIT 2018 /07/17 21 :42:26 | workflow:hello_world | Starting workflow ( Writing log to log/scipipe-20180717-214226-hello_world.log ) AUDIT 2018 /07/17 21 :42:26 | hello | Executing: echo 'Hello ' > hello.out.txt AUDIT 2018 /07/17 21 :42:26 | hello | Finished: echo 'Hello ' > hello.out.txt AUDIT 2018 /07/17 21 :42:26 | world | Executing: echo $( cat ../hello.out.txt ) World > hello.out.txt.world.out.txt AUDIT 2018 /07/17 21 :42:26 | world | Finished: echo $( cat ../hello.out.txt ) World > hello.out.txt.world.out.txt AUDIT 2018 /07/17 21 :42:26 | workflow:hello_world | Finished workflow ( Log written to log/scipipe-20180717-214226-hello_world.log ) Let's check what file SciPipe has generated: $ ls -1 hello* hello.out.txt hello.out.txt.audit.json hello.out.txt.world.out.txt hello.out.txt.world.out.txt.audit.json As you can see, it has created a file hello.out.txt , and hello.out.world.out.txt , and an accompanying .audit.json for each of these files. Now, let's check the output of the final resulting file: $ cat hello.out.txt.world.out.txt Hello World Now we can rejoice that it contains the text \"Hello World\", exactly as a proper Hello World example should :) Now, these were a little long and cumbersome filenames, weren't they? SciPipe gives you very good control over how to name your files, if you don't want to rely on the automatic file naming. For example, we could set the first filename to a static one, and then use the first name as a basis for the file name for the second process, like so: package main import ( // Import the SciPipe package, aliased to 'sp' sp \"github.com/scipipe/scipipe\" ) func main () { // Init workflow with a name, and max concurrent tasks wf := sp . NewWorkflow ( \"hello_world\" , 4 ) // Initialize processes and set output file paths hello := wf . NewProc ( \"hello\" , \"echo 'Hello ' > {o:out}\" ) hello . SetOut ( \"out\" , \"hello.txt\" ) world := wf . NewProc ( \"world\" , \"echo $(cat {i:in}) World >> {o:out}\" ) world . SetOut ( \"out\" , \"{i:in|%.txt}_world.txt\" ) // Connect network world . In ( \"in\" ). From ( hello . Out ( \"out\" )) // Run workflow wf . Run () } In the {i:in... part, we are re-using the file path from the file received on the in-port named 'in', and then running a Bash-style trim-from-end command on it to remove the .txt extension. Now, if we run this, the file names get a little cleaner: $ ls -1 hello* hello.txt hello.txt.audit.json hello_world.go hello_world.txt hello_world.txt.audit.json","title":"Running the example"},{"location":"#the-audit-logs","text":"Finally, we could have a look at one of those audit file created: $ cat hello_world.txt.audit.json { \"ID\" : \"99i5vxhtd41pmaewc8pr\" , \"ProcessName\" : \"world\" , \"Command\" : \"echo $( cat hello.txt ) World \\u003e\\u003e hello_world.txt.tmp/hello_world.txt\" , \"Params\" : {} , \"Tags\" : {} , \"StartTime\" : \"2018-06-15T19:10:37.955602979+02:00\" , \"FinishTime\" : \"2018-06-15T19:10:37.959410102+02:00\" , \"ExecTimeNS\" : 3000000 , \"Upstream\" : { \"hello.txt\" : { \"ID\" : \"w4oeiii9h5j7sckq7aqq\" , \"ProcessName\" : \"hello\" , \"Command\" : \"echo 'Hello ' \\u003e hello.txt.tmp/hello.txt\" , \"Params\" : {} , \"Tags\" : {} , \"StartTime\" : \"2018-06-15T19:10:37.950032676+02:00\" , \"FinishTime\" : \"2018-06-15T19:10:37.95468214+02:00\" , \"ExecTimeNS\" : 4000000 , \"Upstream\" : {} } } Each such audit-file contains a hierarchic JSON-representation of the full workflow path that was executed in order to produce this file. On the first level is the command that directly produced the corresponding file, and then, indexed by their filenames, under \"Upstream\", there is a similar chunk describing how all of its input files were generated. This process will be repeated in a recursive way for large workflows, so that, for each file generated by the workflow, there is always a full, hierarchic, history of all the commands run - with their associated metadata - to produce that file. You can find many more examples in the examples folder in the GitHub repo. For more information about how to write workflows using SciPipe, use the menu to the left, to browse the various topics!","title":"The audit logs"},{"location":"#citing-scipipe","text":"If you use SciPipe in academic or scholarly work, please cite the following paper as source: Lampa S, Dahl\u00f6 M, Alvarsson J, Spjuth O. SciPipe: A workflow library for agile development of complex and dynamic bioinformatics pipelines Gigascience . 8, 5 (2019). DOI: 10.1093/gigascience/giz044","title":"Citing SciPipe"},{"location":"acknowledgements/","text":"Acknowledgements SciPipe is very heavily dependent on the proven principles form Flow-Based Programming (FBP) , as invented by John Paul Morrison . From Flow-based programming, SciPipe uses the ideas of separate network (workflow dependency graph) definition, named in- and out-ports, sub-networks/sub-workflows and bounded buffers (already available in Go's channels) to make writing workflows as easy as possible. This library is has been much influenced/inspired also by the GoFlow library by Vladimir Sibirov . Thanks to Egon Elbre for helpful input on the design of the internals of the pipeline, and processes, which greatly simplified the implementation. This work is financed by faculty grants and other financing for the Pharmaceutical Bioinformatics group of Dept. of Pharmaceutical Biosciences at Uppsala University , and by Swedish Research Council through the Swedish National Bioinformatics Infrastructure Sweden . Supervisor for the project is Ola Spjuth .","title":"Acknowledgements"},{"location":"acknowledgements/#acknowledgements","text":"SciPipe is very heavily dependent on the proven principles form Flow-Based Programming (FBP) , as invented by John Paul Morrison . From Flow-based programming, SciPipe uses the ideas of separate network (workflow dependency graph) definition, named in- and out-ports, sub-networks/sub-workflows and bounded buffers (already available in Go's channels) to make writing workflows as easy as possible. This library is has been much influenced/inspired also by the GoFlow library by Vladimir Sibirov . Thanks to Egon Elbre for helpful input on the design of the internals of the pipeline, and processes, which greatly simplified the implementation. This work is financed by faculty grants and other financing for the Pharmaceutical Bioinformatics group of Dept. of Pharmaceutical Biosciences at Uppsala University , and by Swedish Research Council through the Swedish National Bioinformatics Infrastructure Sweden . Supervisor for the project is Ola Spjuth .","title":"Acknowledgements"},{"location":"basic_concepts/","text":"Basic concepts In SciPipe, we are discussing a few concepts all the time, so to make sure we are on the same page, we will below go through the basic ones briefly. Processes The probably most basic concept in SciPipe is the process. A process is an asynchronously running component that is typically defined as a static, \"long-running\" part of the workflow, and the number of processes thus is typically fixed for a workflow during its execution. One can create customized types of processes, but for most basic workflows, the scipipe.Process will be used, which is specialized for executing commandline applications. New Process -es are typically created using the scipipe.NewProc(procName, shellPattern) command. See GoDoc for Process Tasks The \"long-running\" processes mentioned above, will receive input files on its in-ports, and for each complete set of input files it receives, it will create a new task . Specifically, scipipe.Process will create scipipe.Task objects, and populate it with all data needed for one particular shell command execution. Task objects are executed via their Execute() method, or CustomExecute() , if custom Go code is supposed to be executed instead of a shell command. The distinction between processes and tasks is important to understand, for example when doing more advanced configuration of file naming strategies, since the custom anonymous functions used to format paths are taking a Task as input, even though these functions are saved on the process object. To understand the difference between processes and tasks, it is helpful to remember that processes are long-running, and typically fixed during the course of a workflow, while tasks are transient objects, created temporarily as a container for all data and code needed for each execution of a concrete shell command. See GoDoc for Task Ports Central to the way data dependencies are defined in SciPipe, is ports. Ports are fields on processes, which are connected to other ports via channels (see separate section on this page). In SciPipe, each port must have a unique name within its process (there can't be an in-port and out-port named the same), and this name will be used in shell command patterns, when connecting dependencies / dataflow networks, and when configuring file naming strategies. In Process objects, in-ports are are accessed with myProcess.In(\"my_port\") , and out-ports are similarly accessed with myProcess.Out(\"my_other_port\") . They are of type InPort and OutPort respectively. Some pre-made components might have ports bound to custom field names though, such as myFastaReader.InFastaFile , or myZipComponent.OutZipFile . Port objects have some methods bound to them, most importantly the From() method (for in-ports. Out-ports have a corresponding To() method), which takes another port, and connects to it, by stitching a channel between the ports. On Process objects, there is also a third port type, InParamPort (and the accompanying OutParamPort ), which is used when it is needed to send a stream of parameter values (in string format) to be supplied to as arguments to shell commands. See GoDoc for the InPort struct type See GoDoc for the OutPort struct type See GoDoc for the InParamPort struct type See GoDoc for the OutParamPort struct type Channels Ports in SciPipe are connected via channels. Channels are plain Go channels and nothing more. Most of the time, one will not need to deal with the channels directly though, since the port objects (see separate section for ports) have all the logic to connect to other ports via channels, but it can be good to know that they are there, in case you need to do something more advanced. Workflow The Workflow is a special object in SciPipe, that just takes care of running a set of components making up a workflow. There is not much to say about the workflow component, other than that it is created with scipipe.NewWorkflow(workflowName, maxConcurrentTasks) , that all processes need to be added to it with wf.AddProc(proc) while the \"last\", or \"driving\" process needs to be specified with wf.SetDriver(driverProcess) , and that it should be run with wf.Run() . But this is already covered in the other examples and tutorials. See GoDoc for Workflow Shell command pattern The Process has the speciality that it can be configured using a special shell command pattern, supplied to the NewProc() factory function. It is already explained in the section \"writing workflows\", but in brief, it is a normal shell command, with placeholders for in-ports, out-ports and parameter ports, on the form {i:inportname} , {o:outportname} , and {p:paramportname} , respectively. See GoDoc for NewProc()","title":"Basic Concepts"},{"location":"basic_concepts/#basic-concepts","text":"In SciPipe, we are discussing a few concepts all the time, so to make sure we are on the same page, we will below go through the basic ones briefly.","title":"Basic concepts"},{"location":"basic_concepts/#processes","text":"The probably most basic concept in SciPipe is the process. A process is an asynchronously running component that is typically defined as a static, \"long-running\" part of the workflow, and the number of processes thus is typically fixed for a workflow during its execution. One can create customized types of processes, but for most basic workflows, the scipipe.Process will be used, which is specialized for executing commandline applications. New Process -es are typically created using the scipipe.NewProc(procName, shellPattern) command. See GoDoc for Process","title":"Processes"},{"location":"basic_concepts/#tasks","text":"The \"long-running\" processes mentioned above, will receive input files on its in-ports, and for each complete set of input files it receives, it will create a new task . Specifically, scipipe.Process will create scipipe.Task objects, and populate it with all data needed for one particular shell command execution. Task objects are executed via their Execute() method, or CustomExecute() , if custom Go code is supposed to be executed instead of a shell command. The distinction between processes and tasks is important to understand, for example when doing more advanced configuration of file naming strategies, since the custom anonymous functions used to format paths are taking a Task as input, even though these functions are saved on the process object. To understand the difference between processes and tasks, it is helpful to remember that processes are long-running, and typically fixed during the course of a workflow, while tasks are transient objects, created temporarily as a container for all data and code needed for each execution of a concrete shell command. See GoDoc for Task","title":"Tasks"},{"location":"basic_concepts/#ports","text":"Central to the way data dependencies are defined in SciPipe, is ports. Ports are fields on processes, which are connected to other ports via channels (see separate section on this page). In SciPipe, each port must have a unique name within its process (there can't be an in-port and out-port named the same), and this name will be used in shell command patterns, when connecting dependencies / dataflow networks, and when configuring file naming strategies. In Process objects, in-ports are are accessed with myProcess.In(\"my_port\") , and out-ports are similarly accessed with myProcess.Out(\"my_other_port\") . They are of type InPort and OutPort respectively. Some pre-made components might have ports bound to custom field names though, such as myFastaReader.InFastaFile , or myZipComponent.OutZipFile . Port objects have some methods bound to them, most importantly the From() method (for in-ports. Out-ports have a corresponding To() method), which takes another port, and connects to it, by stitching a channel between the ports. On Process objects, there is also a third port type, InParamPort (and the accompanying OutParamPort ), which is used when it is needed to send a stream of parameter values (in string format) to be supplied to as arguments to shell commands. See GoDoc for the InPort struct type See GoDoc for the OutPort struct type See GoDoc for the InParamPort struct type See GoDoc for the OutParamPort struct type","title":"Ports"},{"location":"basic_concepts/#channels","text":"Ports in SciPipe are connected via channels. Channels are plain Go channels and nothing more. Most of the time, one will not need to deal with the channels directly though, since the port objects (see separate section for ports) have all the logic to connect to other ports via channels, but it can be good to know that they are there, in case you need to do something more advanced.","title":"Channels"},{"location":"basic_concepts/#workflow","text":"The Workflow is a special object in SciPipe, that just takes care of running a set of components making up a workflow. There is not much to say about the workflow component, other than that it is created with scipipe.NewWorkflow(workflowName, maxConcurrentTasks) , that all processes need to be added to it with wf.AddProc(proc) while the \"last\", or \"driving\" process needs to be specified with wf.SetDriver(driverProcess) , and that it should be run with wf.Run() . But this is already covered in the other examples and tutorials. See GoDoc for Workflow","title":"Workflow"},{"location":"basic_concepts/#shell-command-pattern","text":"The Process has the speciality that it can be configured using a special shell command pattern, supplied to the NewProc() factory function. It is already explained in the section \"writing workflows\", but in brief, it is a normal shell command, with placeholders for in-ports, out-ports and parameter ports, on the form {i:inportname} , {o:outportname} , and {p:paramportname} , respectively. See GoDoc for NewProc()","title":"Shell command pattern"},{"location":"contributing/","text":"Contributing to SciPipe Working with forked repository Unlike other languages like C++/Python, Go requires modules being referenced, to be placed in a specific location for the import to work. Forking via GitHub provides a pull request workflow that is well documented but does not work well with Go import. Documented here is one approach utilizing go modules. First, fork and clone SciPipe the usual way via GitHub to a local file system, lets call it <cloned-scipipe-dir> To work on and test the changes made to scipipe in the cloned location, create a directory for your scipipe workflows, lets call it <workflow-dev-dir> . In that directory, you can create a simple main.go file with a package main so that you can run your code that exercises the changes you are making at <cloned-scipipe-dir> . Note that if you were to run your code in main.go as-is, it would pull down the code from the repository and cache them and you will not actually be able to test any local changes in scipipe. To do so, you need to create a file call go.mod . In <workflow-dev-dir> run the following: go mod init <some package name> Note: The actual name of the package is not critical. Next, define the modules required by your main.go: go mod edit - require = github . com / scipipe / scipipe @v0 .9.10 Note: A the time of this writing, v0.9.10 is the latest published version, this will change over time and you need to adapt. Next, replace any reference to the previous URL with reference to the <cloned-scipipe-dir> location: go mod edit - replace = github . com / scipipe / scipipe @v0 .9.10 =< cloned - scipipe - dir > Now when you do: go run main.go ... it will not be pulling the code from GitHub but references the code you have cloned locally. Do your development work and push to your forked repository and do a pull request for the author to review and optionally merge the contribution.","title":"Contributing"},{"location":"contributing/#contributing-to-scipipe","text":"","title":"Contributing to SciPipe"},{"location":"contributing/#working-with-forked-repository","text":"Unlike other languages like C++/Python, Go requires modules being referenced, to be placed in a specific location for the import to work. Forking via GitHub provides a pull request workflow that is well documented but does not work well with Go import. Documented here is one approach utilizing go modules. First, fork and clone SciPipe the usual way via GitHub to a local file system, lets call it <cloned-scipipe-dir> To work on and test the changes made to scipipe in the cloned location, create a directory for your scipipe workflows, lets call it <workflow-dev-dir> . In that directory, you can create a simple main.go file with a package main so that you can run your code that exercises the changes you are making at <cloned-scipipe-dir> . Note that if you were to run your code in main.go as-is, it would pull down the code from the repository and cache them and you will not actually be able to test any local changes in scipipe. To do so, you need to create a file call go.mod . In <workflow-dev-dir> run the following: go mod init <some package name> Note: The actual name of the package is not critical. Next, define the modules required by your main.go: go mod edit - require = github . com / scipipe / scipipe @v0 .9.10 Note: A the time of this writing, v0.9.10 is the latest published version, this will change over time and you need to adapt. Next, replace any reference to the previous URL with reference to the <cloned-scipipe-dir> location: go mod edit - replace = github . com / scipipe / scipipe @v0 .9.10 =< cloned - scipipe - dir > Now when you do: go run main.go ... it will not be pulling the code from GitHub but references the code you have cloned locally. Do your development work and push to your forked repository and do a pull request for the author to review and optionally merge the contribution.","title":"Working with forked repository"},{"location":"examples/","text":"Examples We plan to go through a few examples in more depth here soon, but in the meanwhile, see the examples folder in the main scipipe repository, for a bunch of examples spanning much of the functionality in SciPipe.","title":"Examples"},{"location":"examples/#examples","text":"We plan to go through a few examples in more depth here soon, but in the meanwhile, see the examples folder in the main scipipe repository, for a bunch of examples spanning much of the functionality in SciPipe.","title":"Examples"},{"location":"install/","text":"Installing SciPipe Installing SciPipe means first installing the Go programming langauge, and then using Go's go install command to install the SciPipe library. After this, you will be able to use Go's go run command to run SciPipe workflows. Install Go Install Go by following the instructions on this page , for your operating system. To make sure that everything is installed, run the go command in your terminal, and make sure that it outputs something. To be specific, you can try exetuging go version , which sould output something like the below: $ go version go version go1.17 linux/amd64 Install SciPipe There are two main ways of installing SciPipe, one which is super-easy, and one which is recommended if you want to make sure that your workflow will never break because of API changes in SciPipe, and that you always have a copy of the SciPipe source code available. Easy: Using go install The easiest way to intsall SciPipe is by using the go install tool in the Go tool chain. To install scipipe with go install , run the following command in your terminal: go install github.com/scipipe/scipipe/...@latest N.B: Don't miss the ... , as otherwise the scipipe helper tool will not be installed. Initialize a new workflow file Now, you should be able to write code like in the example below, in files ending with .go . The easiest way to get started is to let the scipipe tool generate a starting point for you: scipipe new myfirstworkflow.go ... which you can then edit to your liking. Create a Go module Before you can run the workflow, you need to also create a go module . To do this, you can run the following command, in the directory where you created your first workflow: go mod init <package-name> For <module-name> , you have to replace it with a name of the package. For a simple script, you can name whatever you want, but if you are thinking about publishing it online, e.g. on GitHub, you typically want to name it like the URL of the corresponding GitHub repo, e.g. github.com/<your-username>/<your-repository> . By doing this, two files will be created: go.mod go.sum Make sure to add them to your git repository, with: git add go.mod go.sum git commit -m \"Add Go module files\" Now, to make sure that scipipe is included as a dependency in the go.mod file, run the go mod tidy command: go mod tidy The go.mod file should now look something like: module mylittlemodule go 1.17 require github . com / scipipe / scipipe v0 .10.2 ... and the go.sum file might look something like: github.com/scipipe/scipipe v0.10.2 h1:crXD1gGh/LuBfWfT4CdXcRFtPjem5weyXN03BDfVOuU= github.com/scipipe/scipipe v0.10.2/go.mod h1:Nwof+Uimtam7GTpkU6cAf/EOnqvxcOVFytjnYU5I3vY= Optional extra step: Use a copy of SciPipe's source code in your own code In order to make sure that your workflow will never break because of API changes in SciPipe, and that you always have a copy of the SciPipe source code available, we recommend to always include a copy of the SciPipe source code in your workflow's source code repository. The SciPipe source code is only around 1500 lines of code, with no external dependencies except Go and Bash, so this should not increase the size of your repository too much. A simple way to do this, is to use Go's vendor tool, which stores a local copy of the source code of packages used, inside the local directory, in a sub-directory called \"vendor\". To do this, execute the following command: go mod vendor Then, to make sure the code is included in your git history, make sure to add it to git: git add vendor git commit -m \"Add vendored version of SciPipe\" Run your workflow Now youa re ready to run the workflow. To run a .go file, just use go run <script-file> , e.g: go run myfirstworkflow.go Some tips about editors In order to be productive with SciPipe, you will also need a Go editor or IDE with support for auto-completion, sometimes also called \"intellisense\". We can warmly recommend to use one of these editors, sorted by level of endorsement: Visual Studio Code with the Go plugin - If you want a very powerful almost IDE-like editor The vim-go plugin by Fatih - if you are a Vim power-user, or need a terminal-only complement to VSCode. JetBrain's GoLand IDE , if you are ready to pay for maximum code intelligence in a professional IDE. LiteIDE - if you want a simple, robust and fast standalone Go-editor. There are also popular Go-plugins for Sublime text , Atom and IntelliJ IDEA , and an upcoming Go IDE from JetBrains, called","title":"Installing"},{"location":"install/#installing-scipipe","text":"Installing SciPipe means first installing the Go programming langauge, and then using Go's go install command to install the SciPipe library. After this, you will be able to use Go's go run command to run SciPipe workflows.","title":"Installing SciPipe"},{"location":"install/#install-go","text":"Install Go by following the instructions on this page , for your operating system. To make sure that everything is installed, run the go command in your terminal, and make sure that it outputs something. To be specific, you can try exetuging go version , which sould output something like the below: $ go version go version go1.17 linux/amd64","title":"Install Go"},{"location":"install/#install-scipipe","text":"There are two main ways of installing SciPipe, one which is super-easy, and one which is recommended if you want to make sure that your workflow will never break because of API changes in SciPipe, and that you always have a copy of the SciPipe source code available.","title":"Install SciPipe"},{"location":"install/#easy-using-go-install","text":"The easiest way to intsall SciPipe is by using the go install tool in the Go tool chain. To install scipipe with go install , run the following command in your terminal: go install github.com/scipipe/scipipe/...@latest N.B: Don't miss the ... , as otherwise the scipipe helper tool will not be installed.","title":"Easy: Using go install"},{"location":"install/#initialize-a-new-workflow-file","text":"Now, you should be able to write code like in the example below, in files ending with .go . The easiest way to get started is to let the scipipe tool generate a starting point for you: scipipe new myfirstworkflow.go ... which you can then edit to your liking.","title":"Initialize a new workflow file"},{"location":"install/#create-a-go-module","text":"Before you can run the workflow, you need to also create a go module . To do this, you can run the following command, in the directory where you created your first workflow: go mod init <package-name> For <module-name> , you have to replace it with a name of the package. For a simple script, you can name whatever you want, but if you are thinking about publishing it online, e.g. on GitHub, you typically want to name it like the URL of the corresponding GitHub repo, e.g. github.com/<your-username>/<your-repository> . By doing this, two files will be created: go.mod go.sum Make sure to add them to your git repository, with: git add go.mod go.sum git commit -m \"Add Go module files\" Now, to make sure that scipipe is included as a dependency in the go.mod file, run the go mod tidy command: go mod tidy The go.mod file should now look something like: module mylittlemodule go 1.17 require github . com / scipipe / scipipe v0 .10.2 ... and the go.sum file might look something like: github.com/scipipe/scipipe v0.10.2 h1:crXD1gGh/LuBfWfT4CdXcRFtPjem5weyXN03BDfVOuU= github.com/scipipe/scipipe v0.10.2/go.mod h1:Nwof+Uimtam7GTpkU6cAf/EOnqvxcOVFytjnYU5I3vY=","title":"Create a Go module"},{"location":"install/#optional-extra-step-use-a-copy-of-scipipes-source-code-in-your-own-code","text":"In order to make sure that your workflow will never break because of API changes in SciPipe, and that you always have a copy of the SciPipe source code available, we recommend to always include a copy of the SciPipe source code in your workflow's source code repository. The SciPipe source code is only around 1500 lines of code, with no external dependencies except Go and Bash, so this should not increase the size of your repository too much. A simple way to do this, is to use Go's vendor tool, which stores a local copy of the source code of packages used, inside the local directory, in a sub-directory called \"vendor\". To do this, execute the following command: go mod vendor Then, to make sure the code is included in your git history, make sure to add it to git: git add vendor git commit -m \"Add vendored version of SciPipe\"","title":"Optional extra step: Use a copy of SciPipe's source code in your own code"},{"location":"install/#run-your-workflow","text":"Now youa re ready to run the workflow. To run a .go file, just use go run <script-file> , e.g: go run myfirstworkflow.go","title":"Run your workflow"},{"location":"install/#some-tips-about-editors","text":"In order to be productive with SciPipe, you will also need a Go editor or IDE with support for auto-completion, sometimes also called \"intellisense\". We can warmly recommend to use one of these editors, sorted by level of endorsement: Visual Studio Code with the Go plugin - If you want a very powerful almost IDE-like editor The vim-go plugin by Fatih - if you are a Vim power-user, or need a terminal-only complement to VSCode. JetBrain's GoLand IDE , if you are ready to pay for maximum code intelligence in a professional IDE. LiteIDE - if you want a simple, robust and fast standalone Go-editor. There are also popular Go-plugins for Sublime text , Atom and IntelliJ IDEA , and an upcoming Go IDE from JetBrains, called","title":"Some tips about editors"},{"location":"other_resources/","text":"Publications mentioning SciPipe NEW: Scientific study using SciPipe: Predicting off-target binding profiles with confidence using Conformal Prediction NEW: Slides: Presentation on SciPipe and more at Go Stockholm Conference Preprint paper on SciPipe: SciPipe - A workflow library for agile development of complex and dynamic bioinformatics pipelines Blog post: Provenance reports in Scientific workflows - going into details about how SciPipe is addressing provenance\">Provenance reports in Scientific Workflows Blog post: First production workflow run with SciPipe Poster: A poster on SciPipe , presented at the e-Science Academy in Lund, on Oct 12-13 2016 . See slides from a recent presentation of SciPipe for use in a Bioinformatics setting . The architecture of SciPipe is based on an flow-based programming like pattern in pure Go presented in this and this blog posts on Gopher Academy.","title":"Other Resources"},{"location":"other_resources/#publications-mentioning-scipipe","text":"NEW: Scientific study using SciPipe: Predicting off-target binding profiles with confidence using Conformal Prediction NEW: Slides: Presentation on SciPipe and more at Go Stockholm Conference Preprint paper on SciPipe: SciPipe - A workflow library for agile development of complex and dynamic bioinformatics pipelines Blog post: Provenance reports in Scientific workflows - going into details about how SciPipe is addressing provenance\">Provenance reports in Scientific Workflows Blog post: First production workflow run with SciPipe Poster: A poster on SciPipe , presented at the e-Science Academy in Lund, on Oct 12-13 2016 . See slides from a recent presentation of SciPipe for use in a Bioinformatics setting . The architecture of SciPipe is based on an flow-based programming like pattern in pure Go presented in this and this blog posts on Gopher Academy.","title":"Publications mentioning SciPipe"},{"location":"related_tools/","text":"Related tools Find below a few tools that are more or less similar to SciPipe that are worth worth checking out before deciding on what tool fits you best (in approximate order of similarity to SciPipe): NextFlow NiPype Luigi / SciLuigi BPipe SnakeMake Cuneiform","title":"Related Tools"},{"location":"related_tools/#related-tools","text":"Find below a few tools that are more or less similar to SciPipe that are worth worth checking out before deciding on what tool fits you best (in approximate order of similarity to SciPipe): NextFlow NiPype Luigi / SciLuigi BPipe SnakeMake Cuneiform","title":"Related tools"},{"location":"settings/","text":"Settings There are some global internal settings in SciPipe that can be changed via setting environment variables. The available options are listed below. Buffert size The buffert size is the number of information packets stored on each connection between an out-port and and in-port. This number, while by default being set to 128, is sometimes useful to increase, in order to work around some corner cases with stalling workflows. To do this, you can set the environment variable SCIPIPE_BUFSIZE , to a value other than 128, for example like this: export SCIPIPE_BUFSIZE = 1024 && go run myworkflow.go","title":"Settings"},{"location":"settings/#settings","text":"There are some global internal settings in SciPipe that can be changed via setting environment variables. The available options are listed below.","title":"Settings"},{"location":"settings/#buffert-size","text":"The buffert size is the number of information packets stored on each connection between an out-port and and in-port. This number, while by default being set to 128, is sometimes useful to increase, in order to work around some corner cases with stalling workflows. To do this, you can set the environment variable SCIPIPE_BUFSIZE , to a value other than 128, for example like this: export SCIPIPE_BUFSIZE = 1024 && go run myworkflow.go","title":"Buffert size"},{"location":"videos/","text":"Video tutorials covering SciPipe usage Screencast: \"Hello World\" scientific workflow in SciPipe","title":"Video tutorials"},{"location":"videos/#video-tutorials-covering-scipipe-usage","text":"Screencast: \"Hello World\" scientific workflow in SciPipe","title":"Video tutorials covering SciPipe usage"},{"location":"writing_workflows/","text":"Writing Workflows - An Overview In order to give an overview of how to write workflows in SciPipe, let's look at the example workflow used on the front page again: package main import ( // Import SciPipe \"github.com/scipipe/scipipe\" ) func main () { // Init workflow with a name, and a number for max concurrent tasks, so we // don't overbook our CPU (it is recommended to set it to the number of CPU // cores of your computer) wf := scipipe . NewWorkflow ( \"hello_world\" , 4 ) // Initialize processes and set output file paths hello := wf . NewProc ( \"hello\" , \"echo 'Hello ' > {o:out}\" ) hello . SetOut ( \"out\" , \"hello.txt\" ) world := wf . NewProc ( \"world\" , \"echo $(cat {i:in}) World >> {o:out}\" ) world . SetOut ( \"out\" , \"{i:in|%.txt}_world.txt\" ) // Connect network world . In ( \"in\" ). From ( hello . Out ( \"out\" )) // Run workflow wf . Run () } Now let's go through the code example in some detail, to see what we are actually doing. Initializing processes // Initialize processes from shell command patterns hello := wf . NewProc ( \"hello\" , \"echo 'Hello ' > {o:out}\" ) world := wf . NewProc ( \"world\" , \"echo $(cat {i:in}) World >> {o:out}\" ) Here we are initializing two new processes, both of them based on a shell command, using the wf.NewProc() function, which takes a processname, and a shell command pattern as input. The shell command pattern The shell command patterns, in this case echo 'Hello ' > {o:out} and echo $(cat {i:in}) World >> {o:out} , are basically normal bash shell commands, with the addition of \"placeholders\" for input and output filenames. Input filename placeholders are on the form {i:INPORT-NAME} and the output filename placeholders are similarly of the form {o:OUTPORT-NAME} . These placeholders will be replaced with actual filenames when the command is executed later. The reason that it a port-name is used to name them, is that files will be queued on the channel connecting to the port, and for each set of files on in-ports, a command will be created and executed whereafter new files will be pulled in on the out-ports, and so on. Formatting output file paths Now we need to provide some way for scipipe to figure out a suitable file name for each of the files propagating through the \"network\" of processes. This can be done using special convenience methods on the processes, starting with SetOut... . There are a few variants, of which two of them are shown here. // Configure output file path formatters for the processes created above hello . SetOut ( \"out\" , \"hello.txt\" ) world . SetOut ( \"out\" , \"{i:in|%.txt}_world.txt\" ) SetOut takes a pattern similar to the shell command pattern, with placeholders, used to define new (shell-based) processes. The available placeholders that can be used are: {i:INPORTNAME} , {p:PARAMNAME} and {t:TAGNAME} . An example of a full pattern might be: {i:foo}.replace_with_{p:replacement}.txt , but can also be used for simple, static paths, like in the example above. The placeholders can also take certain extra \"modifiers\", separated from the placeholder name by pipe characters, and of which the one used above is probably the most important one: %STRING . It will remove the specified string from the end of the path. This is useful when we want to avoid getting too long paths when re-using previous processes' paths. With the example above, our input file named hello.txt will be converted into hello_world.txt by this path pattern. Available path modifiers The currently available path modifiers are: basename - Removes all folders from the path, leaving only the filename. Example: {i:infile|basename} , if infile has the path data/file.txt , will convert it to just file.txt . dirname - Removes the ending file name part from the path, leaving only the folder path. Example: {i:infile|dirname}/newfile.txt , if infile has the path data/file.txt , will convert it to data/newfile.txt . %.<extension> - Removes the file extension <extension> . Example: {i:infile|%.txt} , if infile has the path file.txt , will convert it to just file . s/<search>/<replacement>/ - Will do a simple search and replace, from <search> to <replacement> . Example: {i:infile|s/file/my_file/} , if infile has the path file.txt , will convert it to my_file.txt . Even more control over file formatting We can actually get even more control over how file names are produced than this, by manually supplying each process with an anonymous function that returns file paths given a scipipe.Task object, which will be produced for each command execution. In order to implement the same path patterns as above, using this method, we would write like this: // Configure output file path formatters for the processes created above hello . SetOutFunc ( \"out\" , func ( t * scipipe . Task ) string { return \"hello.txt\" }) world . SetOutFunc ( \"out\" , func ( t * scipipe . Task ) string { return strings . Replace ( t . InPath ( \"in\" ), \".txt\" , \"_world.txt\" , - 1 ) }) As you can see, this is a much more complicated way to format paths, but it can be useful for example when needing to incorporate parameter values into file names. A caveat about using variables in anonymous functions Note that when using anonymous functions, you have to be careful to not re-use the same variable (even with different values) in multiple functions, due to the subtle ways in which closures work in Go . For example, if you create multiple new processes with separate formatting functions in a loop, that uses a shared variable, like this: for _ , val := range [] string { \"foo\" , \"bar\" } { proc := scipipe . NewProc ( val + \"_proc\" , \"cat {p:val} > {o:out}\" ) proc . SetOutFunc ( \"out\" , func ( t * scipipe . Task ) string { return val + \".txt\" }) } ... then, both functions will return \"bar.txt\", since both funcs were pointing to the same variable (\"var\"), which had the value \"bar\" at the end of the loop. To avoid this situation, you can do one of two things, of which the latter is generally recommended: 1. Create a new copy of the variable, inside the anonymous function: for _ , val := range [] string { \"foo\" , \"bar\" } { proc := scipipe . NewProc ( val + \"_proc\" , \"cat {p:val} > {o:out}\" ) val := val // <- Here we create a new copy of the variable proc . SetOutFunc ( \"out\" , func ( t * scipipe . Task ) string { return val + \".txt\" }) } 2. ... or, better, access the parameter value via the task which the path function receives: for _ , val := range [] string { \"foo\" , \"bar\" } { proc := scipipe . NewProc ( val + \"_proc\" , \"cat {p:val} > {o:out}\" ) proc . SetOutFunc ( \"out\" , func ( t * scipipe . Task ) string { return t . Param ( \"val\" ) + \".txt\" // Access param via the task (`t`) }) } Connecting processes into a network Finally we need to define the data dependencies between our processes. We do this by connecting the outports of one process to the inport of another process, using the From method available on each in-port object (Or the To method on out-ports). We also need to connect the final out-port of the pipeline to the workflow, so that the workflow can pull on this port (technically pulling on a Go channel), in order to drive the workflow. // Connect network world . In ( \"in\" ). From ( helloWriter . Out ( \"out\" )) Running the pipeline So, the final part probably explains itself, but the workflow component is a relatively simple one that will start each component in a separate go-routine. For technical reasons, one final process has to be run in the main go-routine (that where the program's main() function runs), but generally you don't need to think about this, as the workflow will then use an in-built sink process for this purpose. If you for any reason need to customize which process to use as the \"driver\" process, instead of the in-built sink. see the SetDriver section in the docs. wf . Run () Summary So with this, we have done everything needed to set up a file-based batch workflow system. In summary, what we did, was to: Initialize processes For each out-port, define a file-naming strategy Specify dependencies by connecting out- and in-ports Run the pipeline This actually turns out to be a fixed set of components that always need to be included when writing workflows, so it might be good to keep them in mind and memorize these steps, if needed. For more examples, see the examples folder in the GitHub repository.","title":"Writing Workflows"},{"location":"writing_workflows/#writing-workflows-an-overview","text":"In order to give an overview of how to write workflows in SciPipe, let's look at the example workflow used on the front page again: package main import ( // Import SciPipe \"github.com/scipipe/scipipe\" ) func main () { // Init workflow with a name, and a number for max concurrent tasks, so we // don't overbook our CPU (it is recommended to set it to the number of CPU // cores of your computer) wf := scipipe . NewWorkflow ( \"hello_world\" , 4 ) // Initialize processes and set output file paths hello := wf . NewProc ( \"hello\" , \"echo 'Hello ' > {o:out}\" ) hello . SetOut ( \"out\" , \"hello.txt\" ) world := wf . NewProc ( \"world\" , \"echo $(cat {i:in}) World >> {o:out}\" ) world . SetOut ( \"out\" , \"{i:in|%.txt}_world.txt\" ) // Connect network world . In ( \"in\" ). From ( hello . Out ( \"out\" )) // Run workflow wf . Run () } Now let's go through the code example in some detail, to see what we are actually doing.","title":"Writing Workflows - An Overview"},{"location":"writing_workflows/#initializing-processes","text":"// Initialize processes from shell command patterns hello := wf . NewProc ( \"hello\" , \"echo 'Hello ' > {o:out}\" ) world := wf . NewProc ( \"world\" , \"echo $(cat {i:in}) World >> {o:out}\" ) Here we are initializing two new processes, both of them based on a shell command, using the wf.NewProc() function, which takes a processname, and a shell command pattern as input.","title":"Initializing processes"},{"location":"writing_workflows/#the-shell-command-pattern","text":"The shell command patterns, in this case echo 'Hello ' > {o:out} and echo $(cat {i:in}) World >> {o:out} , are basically normal bash shell commands, with the addition of \"placeholders\" for input and output filenames. Input filename placeholders are on the form {i:INPORT-NAME} and the output filename placeholders are similarly of the form {o:OUTPORT-NAME} . These placeholders will be replaced with actual filenames when the command is executed later. The reason that it a port-name is used to name them, is that files will be queued on the channel connecting to the port, and for each set of files on in-ports, a command will be created and executed whereafter new files will be pulled in on the out-ports, and so on.","title":"The shell command pattern"},{"location":"writing_workflows/#formatting-output-file-paths","text":"Now we need to provide some way for scipipe to figure out a suitable file name for each of the files propagating through the \"network\" of processes. This can be done using special convenience methods on the processes, starting with SetOut... . There are a few variants, of which two of them are shown here. // Configure output file path formatters for the processes created above hello . SetOut ( \"out\" , \"hello.txt\" ) world . SetOut ( \"out\" , \"{i:in|%.txt}_world.txt\" ) SetOut takes a pattern similar to the shell command pattern, with placeholders, used to define new (shell-based) processes. The available placeholders that can be used are: {i:INPORTNAME} , {p:PARAMNAME} and {t:TAGNAME} . An example of a full pattern might be: {i:foo}.replace_with_{p:replacement}.txt , but can also be used for simple, static paths, like in the example above. The placeholders can also take certain extra \"modifiers\", separated from the placeholder name by pipe characters, and of which the one used above is probably the most important one: %STRING . It will remove the specified string from the end of the path. This is useful when we want to avoid getting too long paths when re-using previous processes' paths. With the example above, our input file named hello.txt will be converted into hello_world.txt by this path pattern.","title":"Formatting output file paths"},{"location":"writing_workflows/#available-path-modifiers","text":"The currently available path modifiers are: basename - Removes all folders from the path, leaving only the filename. Example: {i:infile|basename} , if infile has the path data/file.txt , will convert it to just file.txt . dirname - Removes the ending file name part from the path, leaving only the folder path. Example: {i:infile|dirname}/newfile.txt , if infile has the path data/file.txt , will convert it to data/newfile.txt . %.<extension> - Removes the file extension <extension> . Example: {i:infile|%.txt} , if infile has the path file.txt , will convert it to just file . s/<search>/<replacement>/ - Will do a simple search and replace, from <search> to <replacement> . Example: {i:infile|s/file/my_file/} , if infile has the path file.txt , will convert it to my_file.txt .","title":"Available path modifiers"},{"location":"writing_workflows/#even-more-control-over-file-formatting","text":"We can actually get even more control over how file names are produced than this, by manually supplying each process with an anonymous function that returns file paths given a scipipe.Task object, which will be produced for each command execution. In order to implement the same path patterns as above, using this method, we would write like this: // Configure output file path formatters for the processes created above hello . SetOutFunc ( \"out\" , func ( t * scipipe . Task ) string { return \"hello.txt\" }) world . SetOutFunc ( \"out\" , func ( t * scipipe . Task ) string { return strings . Replace ( t . InPath ( \"in\" ), \".txt\" , \"_world.txt\" , - 1 ) }) As you can see, this is a much more complicated way to format paths, but it can be useful for example when needing to incorporate parameter values into file names.","title":"Even more control over file formatting"},{"location":"writing_workflows/#a-caveat-about-using-variables-in-anonymous-functions","text":"Note that when using anonymous functions, you have to be careful to not re-use the same variable (even with different values) in multiple functions, due to the subtle ways in which closures work in Go . For example, if you create multiple new processes with separate formatting functions in a loop, that uses a shared variable, like this: for _ , val := range [] string { \"foo\" , \"bar\" } { proc := scipipe . NewProc ( val + \"_proc\" , \"cat {p:val} > {o:out}\" ) proc . SetOutFunc ( \"out\" , func ( t * scipipe . Task ) string { return val + \".txt\" }) } ... then, both functions will return \"bar.txt\", since both funcs were pointing to the same variable (\"var\"), which had the value \"bar\" at the end of the loop. To avoid this situation, you can do one of two things, of which the latter is generally recommended: 1. Create a new copy of the variable, inside the anonymous function: for _ , val := range [] string { \"foo\" , \"bar\" } { proc := scipipe . NewProc ( val + \"_proc\" , \"cat {p:val} > {o:out}\" ) val := val // <- Here we create a new copy of the variable proc . SetOutFunc ( \"out\" , func ( t * scipipe . Task ) string { return val + \".txt\" }) } 2. ... or, better, access the parameter value via the task which the path function receives: for _ , val := range [] string { \"foo\" , \"bar\" } { proc := scipipe . NewProc ( val + \"_proc\" , \"cat {p:val} > {o:out}\" ) proc . SetOutFunc ( \"out\" , func ( t * scipipe . Task ) string { return t . Param ( \"val\" ) + \".txt\" // Access param via the task (`t`) }) }","title":"A caveat about using variables in anonymous functions"},{"location":"writing_workflows/#connecting-processes-into-a-network","text":"Finally we need to define the data dependencies between our processes. We do this by connecting the outports of one process to the inport of another process, using the From method available on each in-port object (Or the To method on out-ports). We also need to connect the final out-port of the pipeline to the workflow, so that the workflow can pull on this port (technically pulling on a Go channel), in order to drive the workflow. // Connect network world . In ( \"in\" ). From ( helloWriter . Out ( \"out\" ))","title":"Connecting processes into a network"},{"location":"writing_workflows/#running-the-pipeline","text":"So, the final part probably explains itself, but the workflow component is a relatively simple one that will start each component in a separate go-routine. For technical reasons, one final process has to be run in the main go-routine (that where the program's main() function runs), but generally you don't need to think about this, as the workflow will then use an in-built sink process for this purpose. If you for any reason need to customize which process to use as the \"driver\" process, instead of the in-built sink. see the SetDriver section in the docs. wf . Run ()","title":"Running the pipeline"},{"location":"writing_workflows/#summary","text":"So with this, we have done everything needed to set up a file-based batch workflow system. In summary, what we did, was to: Initialize processes For each out-port, define a file-naming strategy Specify dependencies by connecting out- and in-ports Run the pipeline This actually turns out to be a fixed set of components that always need to be included when writing workflows, so it might be good to keep them in mind and memorize these steps, if needed. For more examples, see the examples folder in the GitHub repository.","title":"Summary"},{"location":"howtos/constrain_resource_usage/","text":"It is important to carefully manage how much resources (CPU and memory) your workflows are using, so that you don't overbook you compute node(s). In SciPipe you can do that using two settings: Max concurrent tasks, which is set on the workflow level, when initiating a new workflow. Cores per tasks, that can be set on processes after they are initialized. Max concurrent tasks is a required setting when initializing workflows, while cores per task can be left to the default, which is 1 core per task. You might want to change this number if for example you have a software that uses more memory than the available memory on your computer divided by the max concurrent tasks number you have set. For example, if you have 8GB of free memory, and have set max concurrent tasks on your workflow to 4, but you have a process whose commandline application uses not 2GB of memory, but 4GB, then you might want to set cores per tasks for that process to 2, so that it gets the double amount of memory. In practice, you set cores per task by setting the field CoresPerTask on the process struct, after it is initiated. Example foo := scipipe . NewProc ( \"foo\" , \"echo foo > {o:foofile}\" ) foo . CoresPerTask = 2","title":"Constrain resource usage"},{"location":"howtos/constrain_resource_usage/#example","text":"foo := scipipe . NewProc ( \"foo\" , \"echo foo > {o:foofile}\" ) foo . CoresPerTask = 2","title":"Example"},{"location":"howtos/convert_audit_logs/","text":"SciPipe 0.8.0 introduced experimental support for converting audit logs (those .audit.json files produced to accompany all output files from SciPipe) into other formats, such as HTML, TeX (for further conversion to PDF) or even executable Bash-scripts. Here's how to do it. Convert audit log to HTML Given that you have an audit log file with the name myfile.audit.json , then execute: scipipe audit2html myfile.audit.json This will produce an HTML file named myfile.audit.html , which you can view in a web browser. Convert audit log to TeX Given that you have an audit log file with the name myfile.audit.json , then execute: scipipe audit2tex myfile.audit.json This will produce an HTML file named myfile.audit.tex , which you can either edit manually, or convert directly to PDF using the pdflatex command like so: pdflatex myfile.audit.tex Converting to PDF requires that you have a TeX installation on your system. On Ubuntu, you can install the base package or TeX live with sudo apt-get install texlive-base . Convert audit log to Bash Given that you have an audit log file with the name myfile.audit.json , then execute: scipipe audit2bash myfile.audit.json This will produce a Bash-file named myfile.audit.sh , which you can execute like so: sh myfile.audit.sh ... in order to reproduce the file again from scratch, if it is removed, given that you have all the dependent files and tools installed on your system.","title":"Convert audit logs to other formats"},{"location":"howtos/convert_audit_logs/#convert-audit-log-to-html","text":"Given that you have an audit log file with the name myfile.audit.json , then execute: scipipe audit2html myfile.audit.json This will produce an HTML file named myfile.audit.html , which you can view in a web browser.","title":"Convert audit log to HTML"},{"location":"howtos/convert_audit_logs/#convert-audit-log-to-tex","text":"Given that you have an audit log file with the name myfile.audit.json , then execute: scipipe audit2tex myfile.audit.json This will produce an HTML file named myfile.audit.tex , which you can either edit manually, or convert directly to PDF using the pdflatex command like so: pdflatex myfile.audit.tex Converting to PDF requires that you have a TeX installation on your system. On Ubuntu, you can install the base package or TeX live with sudo apt-get install texlive-base .","title":"Convert audit log to TeX"},{"location":"howtos/convert_audit_logs/#convert-audit-log-to-bash","text":"Given that you have an audit log file with the name myfile.audit.json , then execute: scipipe audit2bash myfile.audit.json This will produce a Bash-file named myfile.audit.sh , which you can execute like so: sh myfile.audit.sh ... in order to reproduce the file again from scratch, if it is removed, given that you have all the dependent files and tools installed on your system.","title":"Convert audit log to Bash"},{"location":"howtos/file_combinations/","text":"Sometimes you need to create all the possible combinations of a set of files that you have as file streams. For example, say that you have two file streams: [a.txt b.txt] [1.txt 2.txt 3.txt] ... and you want to process all of the combinations of these two sets of files. So in other words, what you want is: [a.txt a.txt a.txt b.txt b.txt b.txt] [1.txt 2.txt 3.txt 1.txt 2.txt 3.txt] This is something you can accomplish with the FileCombinator component, available in SciPipe 0.9.1 and later. Example Given that you have a set of files: letterfile_a.txt letterfile_b.txt numberfile_1.txt numberfile_2.txt numberfile_3.txt ... and you want to create all combinations of the letter* files and the number* files, you can do it as follows: package main import ( \"github.com/scipipe/scipipe\" \"github.com/scipipe/scipipe/components\" ) func main () { wf := scipipe . NewWorkflow ( \"wf\" , 4 ) letterGlobber := components . NewFileGlobber ( wf , \"letter_globber\" , \"letterfile_*.txt\" ) numberGlobber := components . NewFileGlobber ( wf , \"number_globber\" , \"numberfile_*.txt\" ) fileCombiner := components . NewFileCombinator ( wf , \"file_combiner\" ) fileCombiner . In ( \"letters\" ). From ( letterGlobber . Out ()) fileCombiner . In ( \"numbers\" ). From ( numberGlobber . Out ()) catenator := wf . NewProc ( \"catenator\" , \"cat {i:letters} {i:numbers} > {o:combined}\" ) catenator . In ( \"letters\" ). From ( fileCombiner . Out ( \"letters\" )) catenator . In ( \"numbers\" ). From ( fileCombiner . Out ( \"numbers\" )) catenator . SetOut ( \"combined\" , \"{i:letters|basename|%.txt}.{i:numbers|basename|%.txt}.combined.txt\" ) wf . Run () } Note that when accessing an in-port on the FileCombinator with the In(PORTNAME) method, this port will be created automatically, together with a corresponding out-port which can be accessed with the same name, Out(PORTNAME) , as can be seen when we connect the fileCombinator to the catenator process further down in the code. The program above, if put in a .go file and run with go run file.go , will generate the following files (excluding the accompanying .audit.json files): letterfile_b.txt letterfile_a.txt numberfile_3.txt numberfile_2.txt numberfile_1.txt letterfile_a.numberfile_2.combined.txt letterfile_a.numberfile_1.combined.txt letterfile_a.numberfile_3.combined.txt letterfile_b.numberfile_2.combined.txt letterfile_b.numberfile_1.combined.txt letterfile_b.numberfile_3.combined.txt As you can see, all the combinations of the","title":"Creating combinations of files"},{"location":"howtos/file_combinations/#example","text":"Given that you have a set of files: letterfile_a.txt letterfile_b.txt numberfile_1.txt numberfile_2.txt numberfile_3.txt ... and you want to create all combinations of the letter* files and the number* files, you can do it as follows: package main import ( \"github.com/scipipe/scipipe\" \"github.com/scipipe/scipipe/components\" ) func main () { wf := scipipe . NewWorkflow ( \"wf\" , 4 ) letterGlobber := components . NewFileGlobber ( wf , \"letter_globber\" , \"letterfile_*.txt\" ) numberGlobber := components . NewFileGlobber ( wf , \"number_globber\" , \"numberfile_*.txt\" ) fileCombiner := components . NewFileCombinator ( wf , \"file_combiner\" ) fileCombiner . In ( \"letters\" ). From ( letterGlobber . Out ()) fileCombiner . In ( \"numbers\" ). From ( numberGlobber . Out ()) catenator := wf . NewProc ( \"catenator\" , \"cat {i:letters} {i:numbers} > {o:combined}\" ) catenator . In ( \"letters\" ). From ( fileCombiner . Out ( \"letters\" )) catenator . In ( \"numbers\" ). From ( fileCombiner . Out ( \"numbers\" )) catenator . SetOut ( \"combined\" , \"{i:letters|basename|%.txt}.{i:numbers|basename|%.txt}.combined.txt\" ) wf . Run () } Note that when accessing an in-port on the FileCombinator with the In(PORTNAME) method, this port will be created automatically, together with a corresponding out-port which can be accessed with the same name, Out(PORTNAME) , as can be seen when we connect the fileCombinator to the catenator process further down in the code. The program above, if put in a .go file and run with go run file.go , will generate the following files (excluding the accompanying .audit.json files): letterfile_b.txt letterfile_a.txt numberfile_3.txt numberfile_2.txt numberfile_1.txt letterfile_a.numberfile_2.combined.txt letterfile_a.numberfile_1.combined.txt letterfile_a.numberfile_3.combined.txt letterfile_b.numberfile_2.combined.txt letterfile_b.numberfile_1.combined.txt letterfile_b.numberfile_3.combined.txt As you can see, all the combinations of the","title":"Example"},{"location":"howtos/globbing/","text":"There is a component for that, FileGlobber (click link for GoDoc API documentation). A sketchy example, showing how the FileGlobber component can be used, is shown below: package main import ( \"github.com/scipipe/scipipe\" \"github.com/scipipe/scipipe/components\" ) func main () { wf := scipipe . NewWorkflow ( \"wf\" , 4 ) // Initiate a new globber component. Since it is not created from the // workflow object, it needs to take the workflow as its first argument, // in order to connect itself properly to it. globber := components . NewFileGlobber ( wf , \"globber\" , \"./somedirectory/*\" ) // Initiate a command that does some processing on all the globbed files. // Extend the command below to do some meaningful processing on all the // globbed files otherProc := wf . NewProc ( \"otherproc\" , \"cat {i:in} > {o:out}\" ) otherProc . In ( \"in\" ). From ( globber . Out ()) // Run the workflow wf . Run () } Then, given that you have a number of files in a subdirectory called somedirectory , these should now be captured by the globbing component, and sent to otherProc for processing.","title":"Globbing files"},{"location":"howtos/golang_components/","text":"Beware: Technical topic, best suited for power-users! If you want to write a component with Go code, but would like to have it work seamlessly with other workflow processes in SciPipe, without reimplementing the whole Process functionality yourself, there is a way to do it: By using the CustomExecute field of Process. In short, it can be done like this: // Initiate task from a \"shell like\" pattern, though here we // just specify the out-port, and nothing else. We have to // specify the out-port (and any other ports we plan to use later), // so that they are correctly initialized. fooWriter := sci . NewProc ( \"fooer\" , \"{o:foo}\" ) // Set the output formatter to a static string fooWriter . SetOut ( \"foo\" , \"foo.txt\" ) // Create the custom execute function, with pure Go code and // add it to the CustomExecute field of the fooWriter process fooWriter . CustomExecute = func ( task * sci . Task ) { task . OutIP ( \"foo\" ). Write ([] byte ( \"foo\\n\" )) } For a more detailed example, see this example (Have a look at the NewFooer() and NewFoo2Barer() factory functions in particular!)","title":"Creating components in Go"},{"location":"howtos/hpc/","text":"This is being worked on right now (issue #38) . What you can do right now, is to use the Prepend field in processes, to add a salloc command string (in the case of SLURM), or any analogous blocking command to other resource managers. So, something like this (See on the third line how the salloc-line is added to the process): wf := scipipe . NewWorkflow ( \"Hello_World_Workflow\" , 4 ) myProc := wf . NewProc ( \"hello_world\" , \"echo Hello World; sleep 10;\" ) myProc . Prepend = \"salloc -A projectABC123 -p core -t 1:00 -J HelloWorld\" (Beware: This is not a full code example, and won't compile without some more boilerplate, which you can find in the introductory examples) You can find the updated GoDoc for the process struct here .","title":"Interacting with an HPC resource manager"},{"location":"howtos/joining/","text":"If you want to join multiple files, you can do that by first using the StreamToSubstream component in combination with an in-port command pattern where the join pattern is used. A concrete usage of this can be seen in the DNA Cancer analysis workflow . In this example, we see that the bams inport is defined like so: {i:bams|join: } . This means that it will receive IPs on the bams inport, and join their filenames, with a space between each. On the next row, we connect this in-port to the OutSubStream of the StreamToSubstream component (in this example, the StreamToSubstream component was stored in a map, but that is specific to the example code, and not for using it in general). More info See the Concatenator component . Also see the page about Scatter/Gather .","title":"Joining multiple files"},{"location":"howtos/joining/#more-info","text":"See the Concatenator component . Also see the page about Scatter/Gather .","title":"More info"},{"location":"howtos/parameters/","text":"Parameters are arguments sent to commands as flags, or unnamed values, or sometimes just the occurance of flags. SciPipe does not provide one unified way to handle parameters, but instead suggest a few different strategies, dependent on the usage pattern. This is because it turns out that there is a very large variety in how parameters can be used with shell commands. To keep SciPipe a small and flexible tool, we instead mostly leave the choice up to the workflow author to create a solution for each case, using a few helper tools provided with SciPipe, but also all the programming facilities built in to the Go programming language. Below we will discuss how to handle the most common uses for for parameters in SciPipe. For any more complicated use cases not covered here, please refer to the mailing list or the chat , to ask your question. Static parameters If parameters in your shell command is always, the same, you can just add them \"manually\" to the shell command pattern used to create your process. For example, if you always want to write the string \"hello\" to output files, you could create your processes with this string added manually: helloWriter := scipipe . NewProc ( \"helloWriter\" , \"echo hello > {o:outfile}\" ) See also Static parameters example Receive parameters dynamically Receiving parameters dynamically is a much more technically demandning solution than using static parameters. The idea is that by using placeholders for parameter values in a command, each parameter for a particular process, will automatically get a channel of type string, on which it can receive values. When the process is ready to execute another shell command, it receives one item on each parameter ports, in addition to receiving one file on each (file-)in-port, and merges the values into the shell command, before executing it. An example of this would be a little too complicated to cover briefly on this page, so please instead see the dynamic parameters example . In the Run method of the Combinatorics task you will find the code used to send values (all combinations of values in three arrays of lenght 3, in this case). See also Dynamic parameters example Handle boolean flags Topic coming soon. Please add it as a support request in the issue tracker if you need this information fast, and we can prioritize writing it asap. Handling parameters in re-usable components Topic coming soon. Please add it as a support request in the issue tracker if you need this information fast, and we can prioritize writing it asap. Relevant examples Static parameters Receive parameters dynamically","title":"Using parameters in commands and file names"},{"location":"howtos/parameters/#static-parameters","text":"If parameters in your shell command is always, the same, you can just add them \"manually\" to the shell command pattern used to create your process. For example, if you always want to write the string \"hello\" to output files, you could create your processes with this string added manually: helloWriter := scipipe . NewProc ( \"helloWriter\" , \"echo hello > {o:outfile}\" )","title":"Static parameters"},{"location":"howtos/parameters/#see-also","text":"Static parameters example","title":"See also"},{"location":"howtos/parameters/#receive-parameters-dynamically","text":"Receiving parameters dynamically is a much more technically demandning solution than using static parameters. The idea is that by using placeholders for parameter values in a command, each parameter for a particular process, will automatically get a channel of type string, on which it can receive values. When the process is ready to execute another shell command, it receives one item on each parameter ports, in addition to receiving one file on each (file-)in-port, and merges the values into the shell command, before executing it. An example of this would be a little too complicated to cover briefly on this page, so please instead see the dynamic parameters example . In the Run method of the Combinatorics task you will find the code used to send values (all combinations of values in three arrays of lenght 3, in this case).","title":"Receive parameters dynamically"},{"location":"howtos/parameters/#see-also_1","text":"Dynamic parameters example","title":"See also"},{"location":"howtos/parameters/#handle-boolean-flags","text":"Topic coming soon. Please add it as a support request in the issue tracker if you need this information fast, and we can prioritize writing it asap.","title":"Handle boolean flags"},{"location":"howtos/parameters/#handling-parameters-in-re-usable-components","text":"Topic coming soon. Please add it as a support request in the issue tracker if you need this information fast, and we can prioritize writing it asap.","title":"Handling parameters in re-usable components"},{"location":"howtos/parameters/#relevant-examples","text":"Static parameters Receive parameters dynamically","title":"Relevant examples"},{"location":"howtos/partial_workflows/","text":"SciPipe allows you to, on-demand, run only specific parts of a workflow. This can be useful especially if you are doing modifications far up in an already developed workflow, and want to run only up to a specific process, rather than also running all downstream processes, which might be unnecessary heavy. This can be done by using the workflow.RunTo() method. By using this instead of the normal workflow.Run() method, scipipe will only run this process and all upstream processes of that one. See also a simple example of where this is used. There are a few other variants for specifying parts of workflows (and more might be added in the future), such as specifying individual process names, or providing the process structs themselves. Please refer to the relevant parts of the workflow documentation for more about that.","title":"Running parts of workflows"},{"location":"howtos/plot_workflow_graph/","text":"SciPipe 0.8.0 introduced a feature to plot a directed graph of workflows in SciPipe [1]. This can be done in two ways: Just producing a DOT text file, with the graph definition Also converting this DOT file to PDF. Number 1. above can be done without any external dependencies, while number 2 requires that graphviz, with the dot command is installed on the system (On Ubuntu it can be installed with the command: sudo apt-get install graphviz ). How to plot graphs To write a .dot file in SciPipe, include a line like follows, in your workflow definition, provided that you have initiated the variable wf with a workflow struct: func main () { wf := scipipe . NewWorkflow ( \"my workflow\" , 4 ) // Workflow code here wf . PlotGraph ( \"my_workflow_graph.dot\" ) // <-- SEE THIS LINE! wf . Run () } If you want to also convert the dot file to PDF in one go, instead change the next last line to: wf . PlotGraphPDF ( \"my_workflow_graph.dot\" ) How to plot graphs conditionally based on a flag Now, you might not want to generate a new plot every time you run your workflow (although, perhaps you would? ... checking in a .dot version of your workflow could in fact be a great way to keep a more readable version of your workflow at hand ... but anyhow), you could make the plotting optional, based on a flag. This is something we've found ourselves doing quite often at pharmb.io. This could be done as follows (more complete code example): package main import ( \"flag\" \"github.com/scipipe/scipipe\" ) var ( plotGraph = flag . Bool ( \"plotgraph\" , false , \"Plot a directed graph of the workflow to PDF\" ) ) func main () { flag . Parse () wf := scipipe . NewWorkflow ( \"testwf\" , 4 ) wf . NewProc ( \"foo\" , \"echo foo > {o:out}\" ) if * plotGraph { wf . PlotGraphPDF ( \"wfgraph.dot\" ) } wf . Run () } Now, the graph will only plotted if you run your workflow with the -plotgraph flag, e.g: go run myworkflow.go -plotgraph Links GoDoc for Workflow.PlotGraph() GoDoc for Workflow.PlotGraphPDF() Footnotes [1] these are often called \"DAG\" for \"Directed Acyclic Graph\", but SciPipe does not have a guarantee or requirement on acyclicness of the graph, thus just \"directed graph\".","title":"Plotting workflow graphs"},{"location":"howtos/plot_workflow_graph/#how-to-plot-graphs","text":"To write a .dot file in SciPipe, include a line like follows, in your workflow definition, provided that you have initiated the variable wf with a workflow struct: func main () { wf := scipipe . NewWorkflow ( \"my workflow\" , 4 ) // Workflow code here wf . PlotGraph ( \"my_workflow_graph.dot\" ) // <-- SEE THIS LINE! wf . Run () } If you want to also convert the dot file to PDF in one go, instead change the next last line to: wf . PlotGraphPDF ( \"my_workflow_graph.dot\" )","title":"How to plot graphs"},{"location":"howtos/plot_workflow_graph/#how-to-plot-graphs-conditionally-based-on-a-flag","text":"Now, you might not want to generate a new plot every time you run your workflow (although, perhaps you would? ... checking in a .dot version of your workflow could in fact be a great way to keep a more readable version of your workflow at hand ... but anyhow), you could make the plotting optional, based on a flag. This is something we've found ourselves doing quite often at pharmb.io. This could be done as follows (more complete code example): package main import ( \"flag\" \"github.com/scipipe/scipipe\" ) var ( plotGraph = flag . Bool ( \"plotgraph\" , false , \"Plot a directed graph of the workflow to PDF\" ) ) func main () { flag . Parse () wf := scipipe . NewWorkflow ( \"testwf\" , 4 ) wf . NewProc ( \"foo\" , \"echo foo > {o:out}\" ) if * plotGraph { wf . PlotGraphPDF ( \"wfgraph.dot\" ) } wf . Run () } Now, the graph will only plotted if you run your workflow with the -plotgraph flag, e.g: go run myworkflow.go -plotgraph","title":"How to plot graphs conditionally based on a flag"},{"location":"howtos/plot_workflow_graph/#links","text":"GoDoc for Workflow.PlotGraph() GoDoc for Workflow.PlotGraphPDF()","title":"Links"},{"location":"howtos/plot_workflow_graph/#footnotes","text":"[1] these are often called \"DAG\" for \"Directed Acyclic Graph\", but SciPipe does not have a guarantee or requirement on acyclicness of the graph, thus just \"directed graph\".","title":"Footnotes"},{"location":"howtos/reusable_components/","text":"What are re-usable components With re-usable components, we mean components that can be stored in a Go package and imported and used later. In order for components in such a library to be easy to use, the ports need to be static methods bound to the process struct, rather than just stored by a string ID in a generic port map, like the In() and Out() methods on Process processes. This is so that the methods can show up in the auto-completion / intellisense function in code editors removing the need to look up the name of the ports manually in the library code all the time. How to create re-usable components in SciPipe Process processes created with the scipipe.NewProc() command, can be turned into such \"re-usable\" component by using a wrapping strategy, that is demonstrated in an example on GitHub . The idea is to create a new struct type for the re-usable component, and then, in the factory method for the process, create an \"inner\" process of type Process, using NewProc() as in the normal case, embedding that in the outer struct and then adding statically defined accessor methods for each of the ports in the inner process, with a similar name. So, if the inner process has an outport named \"foo\", you would define an accessor method named myproc.OutFoo() that returns this port from the inner process. Let's look at a code example of how this works, by creating a process that just writes \"hi\" to a file: type HiWriter struct { // Embedd a Process struct * sci . Process } func NewHiWriter () * HiWriter { // Initialize a normal \"Process\" to use as an \"inner\" process innerHiWriter := sci . NewProc ( \"hiwriter\" , \"echo hi > {o:hifile}\" ) innerHiWriter . SetOut ( \"hifile\" , \"hi.txt\" ) // Create a new HiWriter process with the inner process embedded into it return & HiWriter { innerHiWriter } } // OutHiFile provides a static version of the \"hifile\" port in the inner // (embedded) process func ( p * HiWriter ) OutHiFile () * sci . OutPort { // Return the inner process' port named \"hifile\" return p . Out ( \"hifile\" ) } See also A full, working, workflow example using this trategy","title":"Creating re-usable components"},{"location":"howtos/reusable_components/#what-are-re-usable-components","text":"With re-usable components, we mean components that can be stored in a Go package and imported and used later. In order for components in such a library to be easy to use, the ports need to be static methods bound to the process struct, rather than just stored by a string ID in a generic port map, like the In() and Out() methods on Process processes. This is so that the methods can show up in the auto-completion / intellisense function in code editors removing the need to look up the name of the ports manually in the library code all the time.","title":"What are re-usable components"},{"location":"howtos/reusable_components/#how-to-create-re-usable-components-in-scipipe","text":"Process processes created with the scipipe.NewProc() command, can be turned into such \"re-usable\" component by using a wrapping strategy, that is demonstrated in an example on GitHub . The idea is to create a new struct type for the re-usable component, and then, in the factory method for the process, create an \"inner\" process of type Process, using NewProc() as in the normal case, embedding that in the outer struct and then adding statically defined accessor methods for each of the ports in the inner process, with a similar name. So, if the inner process has an outport named \"foo\", you would define an accessor method named myproc.OutFoo() that returns this port from the inner process. Let's look at a code example of how this works, by creating a process that just writes \"hi\" to a file: type HiWriter struct { // Embedd a Process struct * sci . Process } func NewHiWriter () * HiWriter { // Initialize a normal \"Process\" to use as an \"inner\" process innerHiWriter := sci . NewProc ( \"hiwriter\" , \"echo hi > {o:hifile}\" ) innerHiWriter . SetOut ( \"hifile\" , \"hi.txt\" ) // Create a new HiWriter process with the inner process embedded into it return & HiWriter { innerHiWriter } } // OutHiFile provides a static version of the \"hifile\" port in the inner // (embedded) process func ( p * HiWriter ) OutHiFile () * sci . OutPort { // Return the inner process' port named \"hifile\" return p . Out ( \"hifile\" ) }","title":"How to create re-usable components in SciPipe"},{"location":"howtos/reusable_components/#see-also","text":"A full, working, workflow example using this trategy","title":"See also"},{"location":"howtos/scatter_gather/","text":"There is work going on to add better scatter/gather support in SciPipe (Issue #20) . In the meanwhile, have a look at this example on GitHub which demonstrates one way of doing a scatter gather operation, using the Splitter component ( see line 24 ) and two concatenator components ( see lines 38-39 ) to do the scatter, and gather, operations respectively.","title":"Implement scatter/gather workflows"},{"location":"howtos/splitting/","text":"At the time of writing this, the recommended way to split files is to use the FileSplitter component in the component library shipped with SciPipe. Also see the page about Scatter/Gather .","title":"Splitting files"},{"location":"howtos/streaming/","text":"SciPipe can stream the output via UNIX named pipes (or \"FIFO files\") . Streaming can be turned on, on out-ports when creating processes with NewProc() , by using {os:outport_name} as placeholder, instead of the normal {o:outport_name} (note the addisional \"s\") You can see how this is used in this example on GitHub . Note that when streaming, you will not get an output file for the output in question. Note also that you still have to provide a path formatting strategy (via some of the Process.SetOut...() functions, or by manually adding one to Process.PathFuncs . This is because a uniqe file name is needed in order to create any audit files, as well as to give a unique name for the named pipe. See also Streaming example on GitHub .","title":"Enable streaming between components"},{"location":"howtos/streaming/#see-also","text":"Streaming example on GitHub .","title":"See also"},{"location":"howtos/subworkflows/","text":"It is possible in SciPipe to wrap a whole workflow in a process, so that it can be used as any other process, in larger workflows. This is demonstrated in this example on GitHub .","title":"Creating sub-workflows"}]}