{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Robust, flexible and resource-efficient pipelines using Go and the commandline</p>"},{"location":"#why-scipipe","title":"Why SciPipe?","text":"<ul> <li>Intuitive: SciPipe works by flowing data through a network of channels   and processes</li> <li>Flexible: Wrapped command-line programs can be combined with processes in   Go</li> <li>Convenient: Full control over how your files are named</li> <li>Efficient: Workflows are compiled to binary code that run fast</li> <li>Parallel: Pipeline paralellism between processes as well as task   parallelism for multiple inputs, making efficient use of multiple CPU cores</li> <li>Supports streaming: Stream data between programs to avoid wasting disk space</li> <li>Easy to debug: Use available Go debugging tools or just <code>println()</code></li> <li>Portable: Distribute workflows as Go code or as self-contained executable   files</li> </ul> <p>Project links: GitHub repo | Issue Tracker | Chat</p> <p> </p> Project updates Jan 2020: New screencast: \"Hello World\" scientific workflow in SciPipe May 2019: The SciPipe paper published open access in GigaScience: SciPipe: A workflow library for agile development of complex and dynamic bioinformatics pipelines Nov 2018: Scientific study using SciPipe: Predicting off-target binding profiles with confidence using Conformal Prediction Slides: Presentation on SciPipe and more at Go Stockholm Conference Blog post: Provenance reports in Scientific Workflows - going into details about how SciPipe is addressing provenance. Blog post: First production workflow run with SciPipe"},{"location":"#introduction","title":"Introduction","text":"<p>SciPipe is a library for writing Scientific Workflows, sometimes also called \"pipelines\", in the Go programming language.</p> <p>When you need to run many commandline programs that depend on each other in complex ways, SciPipe helps by making the process of running these programs flexible, robust and reproducible. SciPipe also lets you restart an interrupted run without over-writing already produced output and produces an audit report of what was run, among many other things.</p> <p>SciPipe is built on the proven principles of Flow-Based Programming (FBP) to achieve maximum flexibility, productivity and agility when designing workflows.  Compared to plain dataflow, FBP provides the benefits that processes are fully self-contained, so that a library of re-usable components can be created, and plugged into new workflows ad-hoc.</p> <p>Similar to other FBP systems, SciPipe workflows can be likened to a network of assembly lines in a factory, where items (files) are flowing through a network of conveyor belts, stopping at different independently running stations (processes) for processing, as depicted in the picture above.</p> <p>SciPipe was initially created for problems in bioinformatics and cheminformatics, but works equally well for any problem involving pipelines of commandline applications.</p> <p>Project status: SciPipe pretty stable now, and only very minor API changes might still occur. We have successfully used SciPipe in a handful of both real and experimental projects, and it has had occasional use outside the research group as well.</p>"},{"location":"#known-limitations","title":"Known limitations","text":"<ul> <li>There are still a number of missing good-to-have features for workflow   design. See the issue tracker   for details.</li> <li>There is not (yet) support for the Common Workflow Language.</li> </ul>"},{"location":"#installing","title":"Installing","text":"<p>For full installation instructions, see the intallation page. For quick getting started steps, you can do:</p> <ol> <li>Download and install Go</li> <li>Run the following command, to install the scipipe Go library (don't miss the    trailing dots!), and create a Go module for your script:</li> </ol> <pre><code>go install github.com/scipipe/scipipe/...@latest\ngo mod init myfirstworkflow-module\n</code></pre>"},{"location":"#hello-world-example","title":"Hello World example","text":"<p>Let's look at an example workflow to get a feel for what writing workflows in SciPipe looks like:</p> <pre><code>package main\n\nimport (\n// Import SciPipe, aliased to sp\nsp \"github.com/scipipe/scipipe\"\n)\n\nfunc main() {\n// Init workflow and max concurrent tasks\nwf := sp.NewWorkflow(\"hello_world\", 4)\n\n// Initialize processes, and file extensions\nhello := wf.NewProc(\"hello\", \"echo 'Hello ' &gt; {o:out|.txt}\")\nworld := wf.NewProc(\"world\", \"echo $(cat {i:in}) World &gt; {o:out|.txt}\")\n\n// Define data flow\nworld.In(\"in\").From(hello.Out(\"out\"))\n\n// Run workflow\nwf.Run()\n}\n</code></pre> <p>To create a file with a similar simple example, you can run:</p> <pre><code>scipipe new hello_world.go\n</code></pre>"},{"location":"#running-the-example","title":"Running the example","text":"<p>Let's put the code in a file named <code>hello_world.go</code> and run it.</p> <p>First you need to make sure that the dependencies (SciPipe in this case) is installed in your local Go module. This you can do with:</p> <pre><code>go mod tidy\n</code></pre> <p>Then you can go ahead and run the workflow:</p> <pre><code>$ go run hello_world.go\nAUDIT   2018/07/17 21:42:26 | workflow:hello_world             | Starting workflow (Writing log to log/scipipe-20180717-214226-hello_world.log)\nAUDIT   2018/07/17 21:42:26 | hello                            | Executing: echo 'Hello ' &gt; hello.out.txt\nAUDIT   2018/07/17 21:42:26 | hello                            | Finished: echo 'Hello ' &gt; hello.out.txt\nAUDIT   2018/07/17 21:42:26 | world                            | Executing: echo $(cat ../hello.out.txt) World &gt; hello.out.txt.world.out.txt\nAUDIT   2018/07/17 21:42:26 | world                            | Finished: echo $(cat ../hello.out.txt) World &gt; hello.out.txt.world.out.txt\nAUDIT   2018/07/17 21:42:26 | workflow:hello_world             | Finished workflow (Log written to log/scipipe-20180717-214226-hello_world.log)\n</code></pre> <p>Let's check what file SciPipe has generated:</p> <pre><code>$ ls -1 hello*\nhello.out.txt\nhello.out.txt.audit.json\nhello.out.txt.world.out.txt\nhello.out.txt.world.out.txt.audit.json\n</code></pre> <p>As you can see, it has created a file <code>hello.out.txt</code>, and <code>hello.out.world.out.txt</code>, and an accompanying <code>.audit.json</code> for each of these files.</p> <p>Now, let's check the output of the final resulting file:</p> <pre><code>$ cat hello.out.txt.world.out.txt\nHello World\n</code></pre> <p>Now we can rejoice that it contains the text \"Hello World\", exactly as a proper Hello World example should :)</p> <p>Now, these were a little long and cumbersome filenames, weren't they? SciPipe gives you very good control over how to name your files, if you don't want to rely on the automatic file naming. For example, we could set the first filename to a static one, and then use the first name as a basis for the file name for the second process, like so:</p> <pre><code>package main\n\nimport (\n// Import the SciPipe package, aliased to 'sp'\nsp \"github.com/scipipe/scipipe\"\n)\n\nfunc main() {\n// Init workflow with a name, and max concurrent tasks\nwf := sp.NewWorkflow(\"hello_world\", 4)\n\n// Initialize processes and set output file paths\nhello := wf.NewProc(\"hello\", \"echo 'Hello ' &gt; {o:out}\")\nhello.SetOut(\"out\", \"hello.txt\")\n\nworld := wf.NewProc(\"world\", \"echo $(cat {i:in}) World &gt;&gt; {o:out}\")\nworld.SetOut(\"out\", \"{i:in|%.txt}_world.txt\")\n\n// Connect network\nworld.In(\"in\").From(hello.Out(\"out\"))\n\n// Run workflow\nwf.Run()\n}\n</code></pre> <p>In the <code>{i:in...</code> part, we are re-using the file path from the file received on the in-port named 'in', and then running a Bash-style trim-from-end command on it to remove the <code>.txt</code> extension.</p> <p>Now, if we run this, the file names get a little cleaner:</p> <pre><code>$ ls -1 hello*\nhello.txt\nhello.txt.audit.json\nhello_world.go\nhello_world.txt\nhello_world.txt.audit.json\n</code></pre>"},{"location":"#the-audit-logs","title":"The audit logs","text":"<p>Finally, we could have a look at one of those audit file created:</p> <pre><code>$ cat hello_world.txt.audit.json\n{\n\"ID\": \"99i5vxhtd41pmaewc8pr\",\n    \"ProcessName\": \"world\",\n    \"Command\": \"echo $(cat hello.txt) World \\u003e\\u003e hello_world.txt.tmp/hello_world.txt\",\n    \"Params\": {},\n    \"Tags\": {},\n    \"StartTime\": \"2018-06-15T19:10:37.955602979+02:00\",\n    \"FinishTime\": \"2018-06-15T19:10:37.959410102+02:00\",\n    \"ExecTimeNS\": 3000000,\n    \"Upstream\": {\n\"hello.txt\": {\n\"ID\": \"w4oeiii9h5j7sckq7aqq\",\n            \"ProcessName\": \"hello\",\n            \"Command\": \"echo 'Hello ' \\u003e hello.txt.tmp/hello.txt\",\n            \"Params\": {},\n            \"Tags\": {},\n            \"StartTime\": \"2018-06-15T19:10:37.950032676+02:00\",\n            \"FinishTime\": \"2018-06-15T19:10:37.95468214+02:00\",\n            \"ExecTimeNS\": 4000000,\n            \"Upstream\": {}\n}\n}\n</code></pre> <p>Each such audit-file contains a hierarchic JSON-representation of the full workflow path that was executed in order to produce this file. On the first level is the command that directly produced the corresponding file, and then, indexed by their filenames, under \"Upstream\", there is a similar chunk describing how all of its input files were generated. This process will be repeated in a recursive way for large workflows, so that, for each file generated by the workflow, there is always a full, hierarchic, history of all the commands run - with their associated metadata - to produce that file.</p> <p>You can find many more examples in the examples folder in the GitHub repo.</p> <p>For more information about how to write workflows using SciPipe, use the menu to the left, to browse the various topics!</p>"},{"location":"#citing-scipipe","title":"Citing SciPipe","text":"<p>If you use SciPipe in academic or scholarly work, please cite the following paper as source:</p> <p>Lampa S, Dahl\u00f6 M, Alvarsson J, Spjuth O. SciPipe: A workflow library for agile development of complex and dynamic bioinformatics pipelines Gigascience. 8, 5 (2019). DOI: 10.1093/gigascience/giz044</p>"},{"location":"acknowledgements/","title":"Acknowledgements","text":"<ul> <li>SciPipe is very heavily dependent on the proven principles form Flow-Based   Programming (FBP), as invented by John Paul Morrison.   From Flow-based programming, SciPipe uses the ideas of separate network   (workflow dependency graph) definition, named in- and out-ports,   sub-networks/sub-workflows and bounded buffers (already available in Go's   channels) to make writing workflows as easy as possible.</li> <li>This library is has been much influenced/inspired also by the   GoFlow library by Vladimir Sibirov.</li> <li>Thanks to Egon Elbre for helpful input on the   design of the internals of the pipeline, and processes, which greatly   simplified the implementation.</li> <li>This work is financed by faculty grants and other financing for the Pharmaceutical Bioinformatics group of Dept. of   Pharmaceutical Biosciences at Uppsala University, and by Swedish Research Council   through the Swedish National Bioinformatics Infrastructure Sweden.</li> <li>Supervisor for the project is Ola Spjuth.</li> </ul>"},{"location":"basic_concepts/","title":"Basic concepts","text":"<p>In SciPipe, we are discussing a few concepts all the time, so to make sure we are on the same page, we will below go through the basic ones briefly.</p>"},{"location":"basic_concepts/#processes","title":"Processes","text":"<p>The probably most basic concept in SciPipe is the process.  A process is an asynchronously running component that is typically defined as a static, \"long-running\" part of the workflow, and the number of processes thus is typically fixed for a workflow during its execution.</p> <p>One can create customized types of processes, but for most basic workflows, the <code>scipipe.Process</code> will be used, which is specialized for executing commandline applications. New <code>Process</code>-es are typically created using the <code>scipipe.NewProc(procName, shellPattern)</code> command.</p> <ul> <li>See GoDoc for Process</li> </ul>"},{"location":"basic_concepts/#tasks","title":"Tasks","text":"<p>The \"long-running\" processes mentioned above, will receive input files on its in-ports, and for each complete set of input files it receives, it will create a new task. Specifically, <code>scipipe.Process</code> will create <code>scipipe.Task</code> objects, and populate it with all data needed for one particular shell command execution.  <code>Task</code> objects are executed via their <code>Execute()</code> method, or <code>CustomExecute()</code>, if custom Go code is supposed to be executed instead of a shell command.</p> <p>The distinction between processes and tasks is important to understand, for example when doing more advanced configuration of file naming strategies, since the custom anonymous functions used to format paths are taking a <code>Task</code> as input, even though these functions are saved on the process object.</p> <p>To understand the difference between processes and tasks, it is helpful to remember that processes are long-running, and typically fixed during the course of a workflow, while tasks are transient objects, created temporarily as a container for all data and code needed for each execution of a concrete shell command.</p> <ul> <li>See GoDoc for Task</li> </ul>"},{"location":"basic_concepts/#ports","title":"Ports","text":"<p>Central to the way data dependencies are defined in SciPipe, is ports. Ports are fields on processes, which are connected to other ports via channels (see separate section on this page).</p> <p>In SciPipe, each port must have a unique name within its process (there can't be an in-port and out-port named the same), and this name will be used in shell command patterns, when connecting dependencies / dataflow networks, and when configuring file naming strategies.</p> <p>In <code>Process</code> objects, in-ports are are accessed with <code>myProcess.In(\"my_port\")</code>, and out-ports are similarly accessed with <code>myProcess.Out(\"my_other_port\")</code>. They are of type <code>InPort</code> and <code>OutPort</code> respectively.</p> <p>Some pre-made components might have ports bound to custom field names though, such as <code>myFastaReader.InFastaFile</code>, or <code>myZipComponent.OutZipFile</code>.</p> <p>Port objects have some methods bound to them, most importantly the <code>From()</code> method (for in-ports. Out-ports have a corresponding <code>To()</code> method), which takes another port, and connects to it, by stitching a channel between the ports.</p> <p>On <code>Process</code> objects, there is also a third port type, <code>InParamPort</code> (and the accompanying <code>OutParamPort</code>), which is used when it is needed to send a stream of parameter values (in string format) to be supplied to as arguments to shell commands.</p> <ul> <li>See GoDoc for the InPort struct type</li> <li>See GoDoc for the OutPort struct type</li> <li>See GoDoc for the InParamPort struct type</li> <li>See GoDoc for the OutParamPort struct type</li> </ul>"},{"location":"basic_concepts/#channels","title":"Channels","text":"<p>Ports in SciPipe are connected via channels. Channels are plain Go channels and nothing more. Most of the time, one will not need to deal with the channels directly though, since the port objects (see separate section for ports) have all the logic to connect to other ports via channels, but it can be good to know that they are there, in case you need to do something more advanced.</p>"},{"location":"basic_concepts/#workflow","title":"Workflow","text":"<p>The <code>Workflow</code> is a special object in SciPipe, that just takes care of running a set of components making up a workflow.</p> <p>There is not much to say about the workflow component, other than that it is created with <code>scipipe.NewWorkflow(workflowName, maxConcurrentTasks)</code>, that all processes need to be added to it with <code>wf.AddProc(proc)</code> while the \"last\", or \"driving\" process needs to be specified with <code>wf.SetDriver(driverProcess)</code>, and that it should be run with <code>wf.Run()</code>. But this is already covered in the other examples and tutorials.</p> <ul> <li>See GoDoc for Workflow</li> </ul>"},{"location":"basic_concepts/#shell-command-pattern","title":"Shell command pattern","text":"<p>The <code>Process</code> has the speciality that it can be configured using a special shell command pattern, supplied to the <code>NewProc()</code> factory function. It is already explained in the section \"writing workflows\", but in brief, it is a normal shell command, with placeholders for in-ports, out-ports and parameter ports, on the form <code>{i:inportname}</code>, <code>{o:outportname}</code>, and <code>{p:paramportname}</code>, respectively.</p> <ul> <li>See GoDoc for NewProc()</li> </ul>"},{"location":"contributing/","title":"Contributing to SciPipe","text":""},{"location":"contributing/#working-with-forked-repository","title":"Working with forked repository","text":"<p>Unlike other languages like C++/Python, Go requires modules being referenced, to be placed in a specific location for the import to work. Forking via GitHub provides a pull request workflow that is well documented but does not work well with Go import. Documented here is one approach utilizing go modules.</p> <p>First, fork and clone SciPipe the usual way via GitHub to a local file system, lets call it <code>&lt;cloned-scipipe-dir&gt;</code></p> <p>To work on and test the changes made to scipipe in the cloned location, create a directory for your scipipe workflows, lets call it <code>&lt;workflow-dev-dir&gt;</code>. In that directory, you can create a simple <code>main.go</code> file with a <code>package main</code> so that you can run your code that exercises the changes you are making at <code>&lt;cloned-scipipe-dir&gt;</code>.</p> <p>Note that if you were to run your code in <code>main.go</code> as-is, it would pull down the code from the repository and cache them and you will not actually be able to test any local changes in scipipe. To do so, you need to create a file call <code>go.mod</code>.</p> <p>In <code>&lt;workflow-dev-dir&gt;</code> run the following:</p> <pre><code>go mod init &lt;some package name&gt;\n</code></pre> <p>Note: The actual name of the package is not critical.</p> <p>Next, define the modules required by your main.go:</p> <pre><code>go mod edit -require=github.com/scipipe/scipipe@v0.9.10\n</code></pre> <p>Note: A the time of this writing, v0.9.10 is the latest published version, this will change over time and you need to adapt.</p> <p>Next, replace any reference to the previous URL with reference to the <code>&lt;cloned-scipipe-dir&gt;</code> location:</p> <pre><code>go mod edit -replace=github.com/scipipe/scipipe@v0.9.10=&lt;cloned-scipipe-dir&gt;\n</code></pre> <p>Now when you do:</p> <pre><code>go run main.go\n</code></pre> <p>... it will not be pulling the code from GitHub but references the code you have cloned locally.</p> <p>Do your development work and push to your forked repository and do a pull request for the author to review and optionally merge the contribution.</p>"},{"location":"examples/","title":"Examples","text":"<p>We plan to go through a few examples in more depth here soon, but in the meanwhile, see the examples folder in the main scipipe repository, for a bunch of examples spanning much of the functionality in SciPipe.</p>"},{"location":"install/","title":"Installing SciPipe","text":"<p>Installing SciPipe means first installing the Go programming langauge, and then using Go's <code>go install</code> command to install the SciPipe library. After this, you will be able to use Go's <code>go run</code> command to run SciPipe workflows.</p>"},{"location":"install/#install-go","title":"Install Go","text":"<p>Install Go by following the instructions on this page, for your operating system.</p> <p>To make sure that everything is installed, run the <code>go</code> command in your terminal, and make sure that it outputs something.</p> <p>To be specific, you can try exetuging <code>go version</code>, which sould output something like the below:</p> <pre><code>$ go version\ngo version go1.17 linux/amd64\n</code></pre>"},{"location":"install/#install-scipipe","title":"Install SciPipe","text":"<p>There are two main ways of installing SciPipe, one which is super-easy, and one which is recommended if you want to make sure that your workflow will never break because of API changes in SciPipe, and that you always have a copy of the SciPipe source code available.</p>"},{"location":"install/#easy-using-go-install","title":"Easy: Using go install","text":"<p>The easiest way to intsall SciPipe is by using the <code>go install</code> tool in the Go tool chain. To install scipipe with <code>go install</code>, run the following command in your terminal:</p> <pre><code>go install github.com/scipipe/scipipe/...@latest\n</code></pre> <p>N.B: Don't miss the <code>...</code>, as otherwise the <code>scipipe</code> helper tool will not be installed.</p>"},{"location":"install/#initialize-a-new-workflow-file","title":"Initialize a new workflow file","text":"<p>Now, you should be able to write code like in the example below, in files ending with <code>.go</code>.</p> <p>The easiest way to get started is to let the scipipe tool generate a starting point for you:</p> <pre><code>scipipe new myfirstworkflow.go\n</code></pre> <p>... which you can then edit to your liking.</p>"},{"location":"install/#create-a-go-module","title":"Create a Go module","text":"<p>Before you can run the workflow, you need to also create a go module.</p> <p>To do this, you can run the following command, in the directory where you created your first workflow:</p> <pre><code>go mod init &lt;package-name&gt;\n</code></pre> <p>For <code>&lt;module-name&gt;</code>, you have to replace it with a name of the package. For a simple script, you can name whatever you want, but if you are thinking about publishing it online, e.g. on GitHub, you typically want to name it like the URL of the corresponding GitHub repo, e.g. <code>github.com/&lt;your-username&gt;/&lt;your-repository&gt;</code>.</p> <p>By doing this, two files will be created:</p> <pre><code>go.mod\ngo.sum\n</code></pre> <p>Make sure to add them to your git repository, with:</p> <pre><code>git add go.mod go.sum\ngit commit -m \"Add Go module files\"\n</code></pre> <p>Now, to make sure that scipipe is included as a dependency in the go.mod file, run the <code>go mod tidy</code> command:</p> <pre><code>go mod tidy\n</code></pre> <p>The <code>go.mod</code> file should now look something like:</p> <pre><code>module mylittlemodule\n\ngo 1.17\n\nrequire github.com/scipipe/scipipe v0.10.2\n</code></pre> <p>... and the go.sum file might look something like:</p> <pre><code>github.com/scipipe/scipipe v0.10.2 h1:crXD1gGh/LuBfWfT4CdXcRFtPjem5weyXN03BDfVOuU=\ngithub.com/scipipe/scipipe v0.10.2/go.mod h1:Nwof+Uimtam7GTpkU6cAf/EOnqvxcOVFytjnYU5I3vY=\n</code></pre>"},{"location":"install/#optional-extra-step-use-a-copy-of-scipipes-source-code-in-your-own-code","title":"Optional extra step: Use a copy of SciPipe's source code in your own code","text":"<p>In order to make sure that your workflow will never break because of API changes in SciPipe, and that you always have a copy of the SciPipe source code available, we recommend to always include a copy of the SciPipe source code in your workflow's source code repository. The SciPipe source code is only around 1500 lines of code, with no external dependencies except Go and Bash, so this should not increase the size of your repository too much.</p> <p>A simple way to do this, is to use Go's <code>vendor</code> tool, which stores a local copy of the source code of packages used, inside the local directory, in a sub-directory called \"vendor\".</p> <p>To do this, execute the following command:</p> <pre><code>go mod vendor\n</code></pre> <p>Then, to make sure the code is included in your git history, make sure to add it to git:</p> <pre><code>git add vendor\ngit commit -m \"Add vendored version of SciPipe\"\n</code></pre>"},{"location":"install/#run-your-workflow","title":"Run your workflow","text":"<p>Now youa re ready to run the workflow. To run a <code>.go</code> file, just use <code>go run &lt;script-file&gt;</code>, e.g:</p> <pre><code>go run myfirstworkflow.go\n</code></pre>"},{"location":"install/#some-tips-about-editors","title":"Some tips about editors","text":"<p>In order to be productive with SciPipe, you will also need a Go editor or IDE with support for auto-completion, sometimes also called \"intellisense\".</p> <p>We can warmly recommend to use one of these editors, sorted by level of endorsement:</p> <ol> <li>Visual Studio Code with the Go plugin - If you want a very powerful almost IDE-like editor</li> <li>The vim-go plugin by Fatih - if you are a Vim power-user, or need a terminal-only complement to VSCode.</li> <li>JetBrain's GoLand IDE, if you are ready to pay for maximum code intelligence in a professional IDE.</li> <li>LiteIDE - if you want a simple, robust and fast standalone Go-editor.</li> </ol> <p>There are also popular Go-plugins for Sublime text, Atom and IntelliJ IDEA, and an upcoming Go IDE from JetBrains, called</p>"},{"location":"other_resources/","title":"Other Resources","text":""},{"location":"other_resources/#publications-mentioning-scipipe","title":"Publications mentioning SciPipe","text":"<ul> <li>NEW: Scientific study using SciPipe:Predicting off-target binding profiles with confidence using Conformal Prediction</li> <li>NEW: Slides: Presentation on SciPipe and more at Go Stockholm Conference</li> <li>Preprint paper on SciPipe:SciPipe - A workflow library for agile development of complex and dynamic bioinformatics pipelines</li> <li>Blog post: Provenance reports in Scientific workflows - going into details about how SciPipe is addressing provenance\"&gt;Provenance reports in Scientific Workflows</li> <li>Blog post: First production workflow run with SciPipe</li> <li>Poster: A poster on SciPipe, presented at the e-Science Academy in Lund, on Oct 12-13 2016.</li> <li>See slides from a recent presentation of SciPipe for use in a Bioinformatics setting.</li> <li>The architecture of SciPipe is based on an flow-based   programming like   pattern in pure Go presented in   this and   this   blog posts on Gopher Academy.</li> </ul>"},{"location":"related_tools/","title":"Related tools","text":"<p>Find below a few tools that are more or less similar to SciPipe that are worth worth checking out before deciding on what tool fits you best (in approximate order of similarity to SciPipe):</p> <ul> <li>NextFlow</li> <li>NiPype</li> <li>Luigi/SciLuigi</li> <li>BPipe</li> <li>SnakeMake</li> <li>Cuneiform</li> </ul>"},{"location":"videos/","title":"Video tutorials","text":""},{"location":"videos/#video-tutorials-covering-scipipe-usage","title":"Video tutorials covering SciPipe usage","text":"<ul> <li>Screencast: \"Hello World\" scientific workflow in SciPipe</li> </ul>"},{"location":"writing_workflows/","title":"Writing Workflows - An Overview","text":"<p>In order to give an overview of how to write workflows in SciPipe, let's look at the example workflow used on the front page again:</p> <pre><code>package main\n\nimport (\n// Import SciPipe\n\"github.com/scipipe/scipipe\"\n)\n\nfunc main() {\n// Init workflow with a name, and a number for max concurrent tasks, so we\n// don't overbook our CPU (it is recommended to set it to the number of CPU\n// cores of your computer)\nwf := scipipe.NewWorkflow(\"hello_world\", 4)\n\n// Initialize processes and set output file paths\nhello := wf.NewProc(\"hello\", \"echo 'Hello ' &gt; {o:out}\")\nhello.SetOut(\"out\", \"hello.txt\")\n\nworld := wf.NewProc(\"world\", \"echo $(cat {i:in}) World &gt;&gt; {o:out}\")\nworld.SetOut(\"out\", \"{i:in|%.txt}_world.txt\")\n\n// Connect network\nworld.In(\"in\").From(hello.Out(\"out\"))\n\n// Run workflow\nwf.Run()\n}\n</code></pre> <p>Now let's go through the code example in some detail, to see what we are actually doing.</p>"},{"location":"writing_workflows/#initializing-processes","title":"Initializing processes","text":"<pre><code>// Initialize processes from shell command patterns\nhello := wf.NewProc(\"hello\", \"echo 'Hello ' &gt; {o:out}\")\nworld := wf.NewProc(\"world\", \"echo $(cat {i:in}) World &gt;&gt; {o:out}\")\n</code></pre> <p>Here we are initializing two new processes, both of them based on a shell command, using the <code>wf.NewProc()</code> function, which takes a processname, and a shell command pattern as input.</p>"},{"location":"writing_workflows/#the-shell-command-pattern","title":"The shell command pattern","text":"<p>The shell command patterns, in this case <code>echo 'Hello ' &gt; {o:out}</code> and <code>echo $(cat {i:in}) World &gt;&gt; {o:out}</code>, are basically normal bash shell commands, with the addition of \"placeholders\" for input and output filenames.</p> <p>Input filename placeholders are on the form <code>{i:INPORT-NAME}</code> and the output filename placeholders are similarly of the form <code>{o:OUTPORT-NAME}</code>.  These placeholders will be replaced with actual filenames when the command is executed later. The reason that it a port-name is used to name them, is that files will be queued on the channel connecting to the port, and for each set of files on in-ports, a command will be created and executed whereafter new files will be pulled in on the out-ports, and so on.</p>"},{"location":"writing_workflows/#formatting-output-file-paths","title":"Formatting output file paths","text":"<p>Now we need to provide some way for scipipe to figure out a suitable file name for each of the files propagating through the \"network\" of processes.  This can be done using special convenience methods on the processes, starting with <code>SetOut...</code>. There are a few variants, of which two of them are shown here.</p> <pre><code>// Configure output file path formatters for the processes created above\nhello.SetOut(\"out\", \"hello.txt\")\nworld.SetOut(\"out\", \"{i:in|%.txt}_world.txt\")\n</code></pre> <p><code>SetOut</code> takes a pattern similar to the shell command pattern, with placeholders, used to define new (shell-based) processes. The available placeholders that can be used are: <code>{i:INPORTNAME}</code>, <code>{p:PARAMNAME}</code> and <code>{t:TAGNAME}</code>. An example of a full pattern might be: <code>{i:foo}.replace_with_{p:replacement}.txt</code>, but can also be used for simple, static paths, like in the example above.</p> <p>The placeholders can also take certain extra \"modifiers\", separated from the placeholder name by pipe characters, and of which the one used above is probably the most important one: <code>%STRING</code>. It will remove the specified string from the end of the path. This is useful when we want to avoid getting too long paths when re-using previous processes' paths. With the example above, our input file named <code>hello.txt</code> will be converted into <code>hello_world.txt</code> by this path pattern.</p>"},{"location":"writing_workflows/#available-path-modifiers","title":"Available path modifiers","text":"<p>The currently available path modifiers are:</p> <ul> <li><code>basename</code> - Removes all folders from the path, leaving only the filename.</li> <li>Example: <code>{i:infile|basename}</code>, if <code>infile</code> has the path <code>data/file.txt</code>, will convert it to just <code>file.txt</code>.</li> <li><code>dirname</code> - Removes the ending file name part from the path, leaving only the folder path.</li> <li>Example: <code>{i:infile|dirname}/newfile.txt</code>, if <code>infile</code> has the path <code>data/file.txt</code>, will convert it to <code>data/newfile.txt</code>.</li> <li><code>%.&lt;extension&gt;</code> - Removes the file extension <code>&lt;extension&gt;</code>.</li> <li>Example: <code>{i:infile|%.txt}</code>, if <code>infile</code> has the path <code>file.txt</code>, will convert it to just <code>file</code>.</li> <li><code>s/&lt;search&gt;/&lt;replacement&gt;/</code> - Will do a simple search and replace, from <code>&lt;search&gt;</code> to <code>&lt;replacement&gt;</code>.</li> <li>Example: <code>{i:infile|s/file/my_file/}</code>, if <code>infile</code> has the path <code>file.txt</code>, will convert it to <code>my_file.txt</code>.</li> </ul>"},{"location":"writing_workflows/#even-more-control-over-file-formatting","title":"Even more control over file formatting","text":"<p>We can actually get even more control over how file names are produced than this, by manually supplying each process with an anonymous function that returns file paths given a <code>scipipe.Task</code> object, which will be produced for each command execution.</p> <p>In order to implement the same path patterns as above, using this method, we would write like this:</p> <pre><code>// Configure output file path formatters for the processes created above\nhello.SetOutFunc(\"out\", func(t *scipipe.Task) string {\nreturn \"hello.txt\"\n})\nworld.SetOutFunc(\"out\", func(t *scipipe.Task) string {\nreturn strings.Replace(t.InPath(\"in\"), \".txt\", \"_world.txt\", -1)\n})\n</code></pre> <p>As you can see, this is a much more complicated way to format paths, but it can be useful for example when needing to incorporate parameter values into file names.</p>"},{"location":"writing_workflows/#a-caveat-about-using-variables-in-anonymous-functions","title":"A caveat about using variables in anonymous functions","text":"<p>Note that when using anonymous functions, you have to be careful to not re-use the same variable (even with different values) in multiple functions, due to the subtle ways in which closures work in Go.</p> <p>For example, if you create multiple new processes with separate formatting functions in a loop, that uses a shared variable, like this:</p> <pre><code>for _, val := range []string{\"foo\", \"bar\"} {\nproc := scipipe.NewProc(val + \"_proc\", \"cat {p:val} &gt; {o:out}\")\nproc.SetOutFunc(\"out\", func(t *scipipe.Task) string {\nreturn val + \".txt\"\n})\n}\n</code></pre> <p>... then, both functions will return \"bar.txt\", since both funcs were pointing to the same variable (\"var\"), which had the value \"bar\" at the end of the loop.</p> <p>To avoid this situation, you can do one of two things, of which the latter is generally recommended:</p> <p>1. Create a new copy of the variable, inside the anonymous function:</p> <pre><code>for _, val := range []string{\"foo\", \"bar\"} {\nproc := scipipe.NewProc(val + \"_proc\", \"cat {p:val} &gt; {o:out}\")\nval := val // &lt;- Here we create a new copy of the variable\nproc.SetOutFunc(\"out\", func(t *scipipe.Task) string {\nreturn val + \".txt\"\n})\n}\n</code></pre> <p>2. ... or, better, access the parameter value via the task which the path function receives:</p> <pre><code>for _, val := range []string{\"foo\", \"bar\"} {\nproc := scipipe.NewProc(val + \"_proc\", \"cat {p:val} &gt; {o:out}\")\nproc.SetOutFunc(\"out\", func(t *scipipe.Task) string {\nreturn t.Param(\"val\") + \".txt\" // Access param via the task (`t`)\n})\n}\n</code></pre>"},{"location":"writing_workflows/#connecting-processes-into-a-network","title":"Connecting processes into a network","text":"<p>Finally we need to define the data dependencies between our processes. We do this by connecting the outports of one process to the inport of another process, using the <code>From</code> method available on each in-port object (Or the <code>To</code> method on out-ports). We also need to connect the final out-port of the pipeline to the workflow, so that the workflow can pull on this port (technically pulling on a Go channel), in order to drive the workflow.</p> <pre><code>// Connect network\nworld.In(\"in\").From(helloWriter.Out(\"out\"))\n</code></pre>"},{"location":"writing_workflows/#running-the-pipeline","title":"Running the pipeline","text":"<p>So, the final part probably explains itself, but the workflow component is a relatively simple one that will start each component in a separate go-routine.</p> <p>For technical reasons, one final process has to be run in the main go-routine (that where the program's <code>main()</code> function runs), but generally you don't need to think about this, as the workflow will then use an in-built sink process for this purpose. If you for any reason need to customize which process to use as the \"driver\" process, instead of the in-built sink. see the <code>SetDriver</code> section in the docs.</p> <pre><code>wf.Run()\n</code></pre>"},{"location":"writing_workflows/#summary","title":"Summary","text":"<p>So with this, we have done everything needed to set up a file-based batch workflow system.</p> <p>In summary, what we did, was to:</p> <ol> <li>Initialize processes</li> <li>For each out-port, define a file-naming strategy</li> <li>Specify dependencies by connecting out- and in-ports</li> <li>Run the pipeline</li> </ol> <p>This actually turns out to be a fixed set of components that always need to be included when writing workflows, so it might be good to keep them in mind and memorize these steps, if needed.</p> <p>For more examples, see the examples folder in the GitHub repository.</p>"},{"location":"howtos/constrain_resource_usage/","title":"Constrain resource usage","text":"<p>It is important to carefully manage how much resources (CPU and memory) your workflows are using, so that you don't overbook you compute node(s).</p> <p>In SciPipe you can do that using two settings:</p> <ul> <li>Max concurrent tasks, which is set on the workflow level, when initiating a new workflow.</li> <li>Cores per tasks, that can be set on processes after they are initialized.</li> </ul> <p>Max concurrent tasks is a required setting when initializing workflows, while cores per task can be left to the default, which is 1 core per task.</p> <p>You might want to change this number if for example you have a software that uses more memory than the available memory on your computer divided by the max concurrent tasks number you have set.</p> <p>For example, if you have 8GB of free memory, and have set max concurrent tasks on your workflow to 4, but you have a process whose commandline application uses not 2GB of memory, but 4GB, then you might want to set cores per tasks for that process to 2, so that it gets the double amount of memory.</p> <p>In practice, you set cores per task by setting the field <code>CoresPerTask</code> on the process struct, after it is initiated. </p>"},{"location":"howtos/constrain_resource_usage/#example","title":"Example","text":"<pre><code>foo := scipipe.NewProc(\"foo\", \"echo foo &gt; {o:foofile}\")\nfoo.CoresPerTask = 2\n</code></pre>"},{"location":"howtos/convert_audit_logs/","title":"Convert audit logs to other formats","text":"<p>SciPipe 0.8.0 introduced experimental support for converting audit logs (those <code>.audit.json</code> files produced to accompany all output files from SciPipe) into other formats, such as HTML, TeX (for further conversion to PDF) or even executable Bash-scripts. Here's how to do it.</p>"},{"location":"howtos/convert_audit_logs/#convert-audit-log-to-html","title":"Convert audit log to HTML","text":"<p>Given that you have an audit log file with the name <code>myfile.audit.json</code>, then execute:</p> <pre><code>scipipe audit2html myfile.audit.json\n</code></pre> <p>This will produce an HTML file named <code>myfile.audit.html</code>, which you can view in a web browser.</p>"},{"location":"howtos/convert_audit_logs/#convert-audit-log-to-tex","title":"Convert audit log to TeX","text":"<p>Given that you have an audit log file with the name <code>myfile.audit.json</code>, then execute:</p> <pre><code>scipipe audit2tex myfile.audit.json\n</code></pre> <p>This will produce an HTML file named <code>myfile.audit.tex</code>, which you can either edit manually, or convert directly to PDF using the <code>pdflatex</code> command like so:</p> <pre><code>pdflatex myfile.audit.tex\n</code></pre> <p>Converting to PDF requires that you have a TeX installation on your system. On Ubuntu, you can install the base package or TeX live with <code>sudo apt-get install texlive-base</code>.</p>"},{"location":"howtos/convert_audit_logs/#convert-audit-log-to-bash","title":"Convert audit log to Bash","text":"<p>Given that you have an audit log file with the name <code>myfile.audit.json</code>, then execute:</p> <pre><code>scipipe audit2bash myfile.audit.json\n</code></pre> <p>This will produce a Bash-file named <code>myfile.audit.sh</code>, which you can execute like so:</p> <pre><code>sh myfile.audit.sh\n</code></pre> <p>... in order to reproduce the file again from scratch, if it is removed, given that you have all the dependent files and tools installed on your system.</p>"},{"location":"howtos/file_combinations/","title":"Creating combinations of files","text":"<p>Sometimes you need to create all the possible combinations of a set of files that you have as file streams. </p> <p>For example, say that you have two file streams:</p> <pre><code>[a.txt b.txt]\n[1.txt 2.txt 3.txt]\n</code></pre> <p>... and you want to process all of the combinations of these two sets of files. So in other words, what you want is:</p> <pre><code>[a.txt a.txt a.txt b.txt b.txt b.txt]\n[1.txt 2.txt 3.txt 1.txt 2.txt 3.txt]\n</code></pre> <p>This is something you can accomplish with the FileCombinator component, available in SciPipe 0.9.1 and later.</p>"},{"location":"howtos/file_combinations/#example","title":"Example","text":"<p>Given that you have a set of files:</p> <pre><code>letterfile_a.txt\nletterfile_b.txt\nnumberfile_1.txt\nnumberfile_2.txt\nnumberfile_3.txt\n</code></pre> <p>... and you want to create all combinations of the <code>letter*</code> files and the <code>number*</code> files, you can do it as follows:</p> <pre><code>package main\n\nimport (\n\"github.com/scipipe/scipipe\"\n\"github.com/scipipe/scipipe/components\"\n)\n\nfunc main() {\nwf := scipipe.NewWorkflow(\"wf\", 4)\n\nletterGlobber := components.NewFileGlobber(wf, \"letter_globber\", \"letterfile_*.txt\")\nnumberGlobber := components.NewFileGlobber(wf, \"number_globber\", \"numberfile_*.txt\")\n\nfileCombiner := components.NewFileCombinator(wf, \"file_combiner\")\nfileCombiner.In(\"letters\").From(letterGlobber.Out())\nfileCombiner.In(\"numbers\").From(numberGlobber.Out())\n\ncatenator := wf.NewProc(\"catenator\", \"cat {i:letters} {i:numbers} &gt; {o:combined}\")\ncatenator.In(\"letters\").From(fileCombiner.Out(\"letters\"))\ncatenator.In(\"numbers\").From(fileCombiner.Out(\"numbers\"))\ncatenator.SetOut(\"combined\", \"{i:letters|basename|%.txt}.{i:numbers|basename|%.txt}.combined.txt\")\n\nwf.Run()\n}\n</code></pre> <p>Note that when accessing an in-port on the FileCombinator with the <code>In(PORTNAME)</code> method, this port will be created automatically, together with a corresponding out-port which can be accessed with the same name, <code>Out(PORTNAME)</code>, as can be seen when we connect the fileCombinator to the catenator process further down in the code.</p> <p>The program above, if put in a <code>.go</code> file and run with <code>go run file.go</code>, will generate the following files (excluding the accompanying .audit.json files):</p> <pre><code>letterfile_b.txt\nletterfile_a.txt\nnumberfile_3.txt\nnumberfile_2.txt\nnumberfile_1.txt\nletterfile_a.numberfile_2.combined.txt\nletterfile_a.numberfile_1.combined.txt\nletterfile_a.numberfile_3.combined.txt\nletterfile_b.numberfile_2.combined.txt\nletterfile_b.numberfile_1.combined.txt\nletterfile_b.numberfile_3.combined.txt\n</code></pre> <p>As you can see, all the combinations of the </p>"},{"location":"howtos/globbing/","title":"Globbing files","text":"<p>There is a component for that, FileGlobber (click link for GoDoc API documentation).</p> <p>A sketchy example, showing how the FileGlobber component can be used, is shown below:</p> <pre><code>package main\n\nimport (\n\"github.com/scipipe/scipipe\"\n\"github.com/scipipe/scipipe/components\"\n)\n\nfunc main() {\nwf := scipipe.NewWorkflow(\"wf\", 4)\n\n// Initiate a new globber component. Since it is not created from the\n// workflow object, it needs to take the workflow as its first argument,\n// in order to connect itself properly to it.\nglobber := components.NewFileGlobber(wf, \"globber\", \"./somedirectory/*\")\n\n// Initiate a command that does some processing on all the globbed files.\n// Extend the command below to do some meaningful processing on all the\n// globbed files\notherProc := wf.NewProc(\"otherproc\", \"cat {i:in} &gt; {o:out}\")\notherProc.In(\"in\").From(globber.Out())\n\n// Run the workflow\nwf.Run()\n}\n</code></pre> <p>Then, given that you have a number of files in a subdirectory called <code>somedirectory</code>, these should now be captured by the globbing component, and sent to <code>otherProc</code> for processing.</p>"},{"location":"howtos/golang_components/","title":"Creating components in Go","text":"<p>Beware: Technical topic, best suited for power-users!</p> <p>If you want to write a component with Go code, but would like to have it work seamlessly with other workflow processes in SciPipe, without reimplementing the whole Process functionality yourself, there is a way to do it: By using the <code>CustomExecute</code> field of Process.</p> <p>In short, it can be done like this:</p> <pre><code>// Initiate task from a \"shell like\" pattern, though here we\n// just specify the out-port, and nothing else. We have to\n// specify the out-port (and any other ports we plan to use later),\n// so that they are correctly initialized.\nfooWriter := sci.NewProc(\"fooer\", \"{o:foo}\")\n\n// Set the output formatter to a static string\nfooWriter.SetOut(\"foo\", \"foo.txt\")\n\n// Create the custom execute function, with pure Go code and\n// add it to the CustomExecute field of the fooWriter process\nfooWriter.CustomExecute = func(task *sci.Task) {\ntask.OutIP(\"foo\").Write([]byte(\"foo\\n\"))\n}\n</code></pre> <p>For a more detailed example, see this example (Have a look at the NewFooer() and NewFoo2Barer() factory functions in particular!)</p>"},{"location":"howtos/hpc/","title":"Interacting with an HPC resource manager","text":"<p>This is being worked on right now (issue #38).</p> <p>What you can do right now, is to use the <code>Prepend</code> field in processes, to add a salloc command string (in the case of SLURM), or any analogous blocking command to other resource managers.</p> <p>So, something like this (See on the third line how the salloc-line is added to the process):</p> <pre><code>wf := scipipe.NewWorkflow(\"Hello_World_Workflow\", 4)\nmyProc := wf.NewProc(\"hello_world\", \"echo Hello World; sleep 10;\")\nmyProc.Prepend = \"salloc -A projectABC123 -p core -t 1:00 -J HelloWorld\"\n</code></pre> <p>(Beware: This is not a full code example, and won't compile without some more boilerplate, which you can find in the introductory examples)</p> <p>You can find the updated GoDoc for the process struct here.</p>"},{"location":"howtos/joining/","title":"Joining multiple files","text":"<p>If you want to join multiple files, you can do that by first using the StreamToSubstream component in combination with an in-port command pattern where the <code>join</code> pattern is used.</p> <p>A concrete usage of this can be seen in the DNA Cancer analysis workflow. In this example, we see that the <code>bams</code> inport is defined like so: <code>{i:bams|join: }</code>. This means that it will receive IPs on the <code>bams</code> inport, and join their filenames, with a space between each. On the next row, we connect this in-port to the OutSubStream of the StreamToSubstream component (in this example, the StreamToSubstream component was stored in a map, but that is specific to the example code, and not for using it in general).</p>"},{"location":"howtos/joining/#more-info","title":"More info","text":"<p>See the Concatenator component.</p> <p>Also see the page about Scatter/Gather.</p>"},{"location":"howtos/parameters/","title":"Using parameters in commands and file names","text":"<p>Parameters are arguments sent to commands as flags, or unnamed values, or sometimes just the occurance of flags.</p> <p>SciPipe does not provide one unified way to handle parameters, but instead suggest a few different strategies, dependent on the usage pattern. This is because it turns out that there is a very large variety in how parameters can be used with shell commands.</p> <p>To keep SciPipe a small and flexible tool, we instead mostly leave the choice up to the workflow author to create a solution for each case, using a few helper tools provided with SciPipe, but also all the programming facilities built in to the Go programming language.</p> <p>Below we will discuss how to handle the most common uses for for parameters in SciPipe. For any more complicated use cases not covered here, please refer to the mailing list or the chat, to ask your question.</p>"},{"location":"howtos/parameters/#static-parameters","title":"Static parameters","text":"<p>If parameters in your shell command is always, the same, you can just add them \"manually\" to the shell command pattern used to create your process.</p> <p>For example, if you always want to write the string \"hello\" to output files, you could create your processes with this string added manually:</p> <pre><code>helloWriter := scipipe.NewProc(\"helloWriter\", \"echo hello &gt; {o:outfile}\")\n</code></pre>"},{"location":"howtos/parameters/#see-also","title":"See also","text":"<ul> <li>Static parameters example</li> </ul>"},{"location":"howtos/parameters/#receive-parameters-dynamically","title":"Receive parameters dynamically","text":"<p>Receiving parameters dynamically is a much more technically demandning solution than using static parameters.</p> <p>The idea is that by using placeholders for parameter values in a command, each parameter for a particular process, will automatically get a channel of type string, on which it can receive values. When the process is ready to execute another shell command, it receives one item on each parameter ports, in addition to receiving one file on each (file-)in-port, and merges the values into the shell command, before executing it.</p> <p>An example of this would be a little too complicated to cover briefly on this page, so please instead see the dynamic parameters example. In the Run method of the Combinatorics task you will find the code used to send values (all combinations of values in three arrays of lenght 3, in this case).</p>"},{"location":"howtos/parameters/#see-also_1","title":"See also","text":"<ul> <li>Dynamic parameters example</li> </ul>"},{"location":"howtos/parameters/#handle-boolean-flags","title":"Handle boolean flags","text":"<p>Topic coming soon. Please add it as a support request in the issue tracker if you need this information fast, and we can prioritize writing it asap.</p>"},{"location":"howtos/parameters/#handling-parameters-in-re-usable-components","title":"Handling parameters in re-usable components","text":"<p>Topic coming soon. Please add it as a support request in the issue tracker if you need this information fast, and we can prioritize writing it asap.</p>"},{"location":"howtos/parameters/#relevant-examples","title":"Relevant examples","text":"<ul> <li>Parameter channels</li> </ul>"},{"location":"howtos/partial_workflows/","title":"Running parts of workflows","text":"<p>SciPipe allows you to, on-demand, run only specific parts of a workflow. This can be useful especially if you are doing modifications far up in an already developed workflow, and want to run only up to a specific process, rather than also running all downstream processes, which might be unnecessary heavy.</p> <p>This can be done by using the workflow.RunTo() method. By using this instead of the normal <code>workflow.Run()</code> method, scipipe will only run this process and all upstream processes of that one.</p> <p>See also a simple\u00a0example of where this is used.</p> <p>There are a few other variants for specifying parts of workflows (and more might be added in the future), such as specifying individual process names, or providing the process structs themselves. Please refer to the relevant parts of the workflow\u00a0documentation for more about that.</p>"},{"location":"howtos/plot_workflow_graph/","title":"Plotting workflow graphs","text":"<p>SciPipe 0.8.0 introduced a feature to plot a directed graph of workflows in SciPipe [1]. This can be done in two ways:</p> <ol> <li>Just producing a DOT text file, with the graph definition</li> <li>Also converting this DOT file to PDF.</li> </ol> <p>Number 1. above can be done without any external dependencies, while number 2 requires that graphviz, with the <code>dot</code> command is installed on the system (On Ubuntu it can be installed with the command: <code>sudo apt-get install graphviz</code>).</p>"},{"location":"howtos/plot_workflow_graph/#how-to-plot-graphs","title":"How to plot graphs","text":"<p>To write a .dot file in SciPipe, include a line like follows, in your workflow definition, provided that you have initiated the variable <code>wf</code> with a workflow struct:</p> <pre><code>func main() {\nwf := scipipe.NewWorkflow(\"my workflow\", 4)\n// Workflow code here\nwf.PlotGraph(\"my_workflow_graph.dot\") // &lt;-- SEE THIS LINE!\nwf.Run()\n}\n</code></pre> <p>If you want to also convert the dot file to PDF in one go, instead change the next last line to:</p> <pre><code>    wf.PlotGraphPDF(\"my_workflow_graph.dot\")\n</code></pre>"},{"location":"howtos/plot_workflow_graph/#how-to-plot-graphs-conditionally-based-on-a-flag","title":"How to plot graphs conditionally based on a flag","text":"<p>Now, you might not want to generate a new plot every time you run your workflow (although, perhaps you would? ... checking in a .dot version of your workflow could in fact be a great way to keep a more readable version of your workflow at hand ... but anyhow), you could make the plotting optional, based on a flag. This is something we've found ourselves doing quite often at pharmb.io. This could be done as follows (more complete code example):</p> <pre><code>package main\n\nimport (\n\"flag\"\n\"github.com/scipipe/scipipe\"\n)\n\nvar (\nplotGraph = flag.Bool(\"plotgraph\", false, \"Plot a directed graph of the workflow to PDF\")\n)\n\nfunc main() {\nflag.Parse()\n\nwf := scipipe.NewWorkflow(\"testwf\", 4)\nwf.NewProc(\"foo\", \"echo foo &gt; {o:out}\")\n\nif *plotGraph {\nwf.PlotGraphPDF(\"wfgraph.dot\")\n}\nwf.Run()\n}\n</code></pre> <p>Now, the graph will only plotted if you run your workflow with the <code>-plotgraph</code> flag, e.g:</p> <pre><code>go run myworkflow.go -plotgraph\n</code></pre>"},{"location":"howtos/plot_workflow_graph/#links","title":"Links","text":"<ul> <li>GoDoc for Workflow.PlotGraph()</li> <li>GoDoc for Workflow.PlotGraphPDF()</li> </ul>"},{"location":"howtos/plot_workflow_graph/#footnotes","title":"Footnotes","text":"<p>[1] these are often called \"DAG\" for \"Directed Acyclic Graph\", but SciPipe does not have a guarantee or requirement on acyclicness of the graph, thus just \"directed graph\".</p>"},{"location":"howtos/reusable_components/","title":"Creating re-usable components","text":""},{"location":"howtos/reusable_components/#what-are-re-usable-components","title":"What are re-usable components","text":"<p>With re-usable components, we mean components that can be stored in a Go package and imported and used later.</p> <p>In order for components in such a library to be easy to use, the ports need to be static methods bound to the process struct, rather than just stored by a string ID in a generic port map, like the <code>In()</code> and <code>Out()</code> methods on <code>Process</code> processes. This is so that the methods can show up in the auto-completion / intellisense function in code editors removing the need to look up the name of the ports manually in the library code all the time.</p>"},{"location":"howtos/reusable_components/#how-to-create-re-usable-components-in-scipipe","title":"How to create re-usable components in SciPipe","text":"<p>Process processes created with the <code>scipipe.NewProc()</code> command, can be turned into such \"re-usable\" component by using a wrapping strategy, that is demonstrated in an example on GitHub.</p> <p>The idea is to create a new struct type for the re-usable component, and then, in the factory method for the process, create an \"inner\" process of type Process, using <code>NewProc()</code> as in the normal case, embedding that in the outer struct and then adding statically defined accessor methods for each of the ports in the inner process, with a similar name. So, if the inner process has an outport named \"foo\", you would define an accessor method named <code>myproc.OutFoo()</code> that returns this port from the inner process.</p> <p>Let's look at a code example of how this works, by creating a process that just writes \"hi\" to a file:</p> <pre><code>type HiWriter struct {\n// Embedd a Process struct\n*sci.Process\n}\n\nfunc NewHiWriter() *HiWriter {\n// Initialize a normal \"Process\" to use as an \"inner\" process\ninnerHiWriter := sci.NewProc(\"hiwriter\", \"echo hi &gt; {o:hifile}\")\ninnerHiWriter.SetOut(\"hifile\", \"hi.txt\")\n\n// Create a new HiWriter process with the inner process embedded into it\nreturn &amp;HiWriter{innerHiWriter}\n}\n\n// OutHiFile provides a static version of the \"hifile\" port in the inner\n// (embedded) process\nfunc (p *HiWriter) OutHiFile() *sci.OutPort {\n// Return the inner process' port named \"hifile\"\nreturn p.Out(\"hifile\")\n}\n</code></pre>"},{"location":"howtos/reusable_components/#see-also","title":"See also","text":"<ul> <li>A full, working, workflow example using this trategy</li> </ul>"},{"location":"howtos/scatter_gather/","title":"Implement scatter/gather workflows","text":"<p>There is work going on to add better scatter/gather support in SciPipe (Issue #20). In the meanwhile, have a look at this example on GitHub which demonstrates one way of doing a scatter gather operation, using the Splitter component (see line 24) and two concatenator components (see lines 38-39) to do the scatter, and gather, operations respectively.</p>"},{"location":"howtos/splitting/","title":"Splitting files","text":"<p>At the time of writing this, the recommended way to split files is to use the <code>FileSplitter</code> component in the component library shipped with SciPipe.</p> <p>Also see the page about Scatter/Gather.</p>"},{"location":"howtos/streaming/","title":"Enable streaming between components","text":"<p>SciPipe can stream the output via UNIX named pipes (or \"FIFO files\").</p> <p>Streaming can be turned on, on out-ports when creating processes with <code>NewProc()</code>, by using <code>{os:outport_name}</code> as placeholder, instead of the normal <code>{o:outport_name}</code> (note the addisional \"s\")</p> <p>You can see how this is used in this example on GitHub.</p> <p>Note that when streaming, you will not get an output file for the output in question.</p> <p>Note also that you still have to provide a path formatting strategy (via some of the <code>Process.SetOut...()</code> functions, or by manually adding one to <code>Process.PathFuncs</code>. This is because a uniqe file name is needed in order to create any audit files, as well as to give a unique name for the named pipe.</p>"},{"location":"howtos/streaming/#see-also","title":"See also","text":"<ul> <li>Streaming example on GitHub.</li> </ul>"},{"location":"howtos/subworkflows/","title":"Creating sub-workflows","text":"<p>It is possible in SciPipe to wrap a whole workflow in a process, so that it can be used as any other process, in larger workflows.</p> <p>This is demonstrated in this example on GitHub.</p>"}]}